
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../ml-tutorial/09-%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A5%E5%85%B7/">
      
      
        <link rel="next" href="../evaluate/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>第一章：特征工程 - 📚面试秘籍</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../styles/css/extra.css">
    
      <link rel="stylesheet" href="../../../styles/css/custom-toc.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="📚面试秘籍" class="md-header__button md-logo" aria-label="📚面试秘籍" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            📚面试秘籍
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              第一章：特征工程
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 0-7 7c0 2.38 1.19 4.47 3 5.74V17a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-2.26c1.81-1.27 3-3.36 3-5.74a7 7 0 0 0-7-7M9 21a1 1 0 0 0 1 1h4a1 1 0 0 0 1-1v-1H9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 1 7 7c0 2.38-1.19 4.47-3 5.74V17a1 1 0 0 1-1 1H9a1 1 0 0 1-1-1v-2.26C6.19 13.47 5 11.38 5 9a7 7 0 0 1 7-7M9 21v-1h6v1a1 1 0 0 1-1 1h-4a1 1 0 0 1-1-1m3-17a5 5 0 0 0-5 5c0 2.05 1.23 3.81 3 4.58V16h4v-2.42c1.77-.77 3-2.53 3-4.58a5 5 0 0 0-5-5"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  🏠Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../maths/math/" class="md-tabs__link">
        
  
  
    
  
  🐵程序员的数学

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../langs/python/basic/" class="md-tabs__link">
          
  
  
    
  
  🦏语言大本营

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../algorithm/basic/" class="md-tabs__link">
          
  
  
    
  
  🐹算法大本营

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../cloud/docker/docker/" class="md-tabs__link">
          
  
  
    
  
  🐺云原生

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../analysis/numpy/" class="md-tabs__link">
          
  
  
    
  
  🦧数据分析大营

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../scikit-learn/basic/" class="md-tabs__link">
          
  
  
    
  
  🐴机器学习

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../deeplearning/pytorch/tensor/" class="md-tabs__link">
          
  
  
    
  
  🐣深度学习

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../llm/huggingface/pipeline/" class="md-tabs__link">
          
  
  
    
  
  🐙大模型

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../llmapp/langchain/basic/" class="md-tabs__link">
          
  
  
    
  
  🐠大模型应用

        </a>
      </li>
    
  

    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="📚面试秘籍" class="md-nav__button md-logo" aria-label="📚面试秘籍" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    📚面试秘籍
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    🏠Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../maths/math/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    🐵程序员的数学
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../../langs/python/basic/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    🦏语言大本营
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../algorithm/basic/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    🐹算法大本营
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../cloud/docker/docker/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    🐺云原生
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../analysis/numpy/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    🦧数据分析大营
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    🐴机器学习
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            🐴机器学习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../scikit-learn/basic/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    scikit-learn手册
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../ml-tutorial/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    机器学习教程
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3" checked>
        
          
          <label class="md-nav__link" for="__nav_7_3" id="__nav_7_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    机器学习面试
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7_3">
            <span class="md-nav__icon md-icon"></span>
            机器学习面试
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    第一章：特征工程
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    第一章：特征工程
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一、特征归一化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、特征归一化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 什么是特征归一化？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 为什么需要归一化？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 归一化的常见方法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-vs-standardization" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 归一化 vs 标准化（Standardization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 实战
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      二、类别型特征
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、类别型特征">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 类别型特征的类型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 常见的编码方式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 实战
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      三、文本表示模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、文本表示模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 概述：为什么需要文本表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 传统表示方法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 分布式词向量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 上下文相关的表示方法
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word2vec_1" class="md-nav__link">
    <span class="md-ellipsis">
      四、Word2Vec
    </span>
  </a>
  
    <nav class="md-nav" aria-label="四、Word2Vec">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 为什么需要 Word2Vec？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Word2Vec 的两种模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 原理公式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-gensim-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 使用 Gensim 训练 Word2Vec（实战）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 Word2Vec 的应用场景
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#46" class="md-nav__link">
    <span class="md-ellipsis">
      4.6 向量推理的神奇之处
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#47" class="md-nav__link">
    <span class="md-ellipsis">
      4.7 常见问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#48" class="md-nav__link">
    <span class="md-ellipsis">
      4.8 总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第二章：模型评估
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../supervised/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第三章：监督学习模型
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dimension/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第四章：降维
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unsupervised/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第五章：非监督学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pgm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第六章：概率图模型
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optiom/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第七章：优化算法
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sample/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第八章：采样
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../forward/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第九章：前向神经网络
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rnn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第十章：循环神经网络
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第十一章：强化学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ensemble/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第十二章：集成学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    第十三章：生成式对抗网络
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../ml-action/knn/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    机器学习实战
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../../deeplearning/pytorch/tensor/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    🐣深度学习
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../../llm/huggingface/pipeline/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    🐙大模型
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    
  
  
  
    <a href="../../../llmapp/langchain/basic/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    🐠大模型应用
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一、特征归一化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、特征归一化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 什么是特征归一化？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 为什么需要归一化？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 归一化的常见方法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-vs-standardization" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 归一化 vs 标准化（Standardization）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 实战
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      二、类别型特征
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、类别型特征">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 类别型特征的类型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 常见的编码方式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 实战
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      三、文本表示模型
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、文本表示模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 概述：为什么需要文本表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 传统表示方法
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 分布式词向量
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 上下文相关的表示方法
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#word2vec_1" class="md-nav__link">
    <span class="md-ellipsis">
      四、Word2Vec
    </span>
  </a>
  
    <nav class="md-nav" aria-label="四、Word2Vec">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 为什么需要 Word2Vec？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Word2Vec 的两种模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 原理公式
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-gensim-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 使用 Gensim 训练 Word2Vec（实战）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 Word2Vec 的应用场景
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#46" class="md-nav__link">
    <span class="md-ellipsis">
      4.6 向量推理的神奇之处
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#47" class="md-nav__link">
    <span class="md-ellipsis">
      4.7 常见问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#48" class="md-nav__link">
    <span class="md-ellipsis">
      4.8 总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="_1">第一章：特征工程<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<p>特征工程（Feature Engineering）是指将原始数据转化为更适合机器学习模型使用的特征集的过程。它在机器学习中非常重要，因为好的特征往往比复杂的模型更能提升效果。</p>
<p>换句话说：特征工程就是“<strong>从数据中提炼出最能描述问题的特征</strong>”。</p>
<p><strong>特征工程</strong>，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，特征工程旨在去除原始数据中的杂志和冗余，
设计更高效的特征以刻画求解的问题与预测模型之间的关系。</p>
<hr />
<h2 id="_2">一、特征归一化<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="11">1.1 什么是特征归一化？<a class="headerlink" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>特征归一化（Normalization）</strong> 是将数值型特征压缩到相同的尺度，通常是 [0, 1] 区间，以避免不同量纲和数值范围对模型造成影响。</p>
<p>它是特征工程中非常重要的预处理步骤，尤其适用于基于“距离计算”或“梯度优化”的模型。</p>
<h3 id="12">1.2 为什么需要归一化？<a class="headerlink" href="#12" title="Permanent link">&para;</a></h3>
<h4 id="1">场景 1：不同特征的数值范围差异大<a class="headerlink" href="#1" title="Permanent link">&para;</a></h4>
<p>比如：</p>
<ul>
<li>身高（150~200cm）</li>
<li>体重（50~100kg）</li>
<li>收入（1w~100w）</li>
</ul>
<p>这些不同量纲的数值直接输入模型，会让“量纲大的特征”主导模型学习。</p>
<h4 id="2">场景 2：使用以下模型时必须归一化<a class="headerlink" href="#2" title="Permanent link">&para;</a></h4>
<ul>
<li>KNN / K-means（基于距离）</li>
<li>SVM（核函数计算）</li>
<li>Logistic / 线性回归（收敛速度受梯度影响）</li>
<li>神经网络（梯度稳定性）</li>
</ul>
<h3 id="13">1.3 归一化的常见方法<a class="headerlink" href="#13" title="Permanent link">&para;</a></h3>
<h4 id="1-min-max-01">1. Min-Max 归一化（缩放到 0~1）<a class="headerlink" href="#1-min-max-01" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
x’ = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
\]</div>
<p><strong>特点：</strong></p>
<ul>
<li>适合数值分布稳定，边界已知的情况；</li>
<li>对异常值敏感（容易压缩其他值）。</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="2-maxabs-1-1">2. MaxAbs 归一化（缩放到 [-1, 1]）<a class="headerlink" href="#2-maxabs-1-1" title="Permanent link">&para;</a></h4>
<p>适合稀疏数据，如文本向量、TF-IDF 等。</p>
<div class="arithmatex">\[
x’ = \frac{x}{|x_{\max}|}
\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaxAbsScaler</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">scaler</span> <span class="o">=</span> <span class="n">MaxAbsScaler</span><span class="p">()</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="3-l2">3. L2 归一化（单位向量归一化）<a class="headerlink" href="#3-l2" title="Permanent link">&para;</a></h4>
<p>让每一行（样本）模长为 1，常用于文本/向量表示。</p>
<p><strong>公式（L2 范数）：</strong></p>
<div class="arithmatex">\[
    x’ = \frac{x}{\|x\|_2}
\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Normalizer</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="n">scaler</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">)</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="14-vs-standardization">1.4 归一化 vs 标准化（Standardization）<a class="headerlink" href="#14-vs-standardization" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>比较项</th>
<th>标准化（Standardization）</th>
<th>归一化（Normalization）</th>
</tr>
</thead>
<tbody>
<tr>
<td>适用场景</td>
<td>大多数模型（如 SVM、逻辑回归、KNN、PCA）</td>
<td>特征有明确边界的情况（如图像像素）</td>
</tr>
<tr>
<td>输出范围</td>
<td>均值 0，标准差 1</td>
<td>一般在 [0, 1] 或 [-1, 1]</td>
</tr>
<tr>
<td>是否受异常值影响</td>
<td>是</td>
<td>是（更敏感）</td>
</tr>
<tr>
<td>是否依赖分布</td>
<td>适合正态分布</td>
<td>无要求</td>
</tr>
<tr>
<td>代码实现（sklearn）</td>
<td><code>StandardScaler</code></td>
<td><code>MinMaxScaler</code></td>
</tr>
</tbody>
</table>
<h3 id="15">1.5 实战<a class="headerlink" href="#15" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="c1"># 示例数据</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>    <span class="s2">&quot;height&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">160</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">180</span><span class="p">],</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">90</span><span class="p">]</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="p">})</span>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
</span><span id="__span-3-11"><a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a><span class="n">df_scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</span><span id="__span-3-12"><a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>
</span><span id="__span-3-13"><a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a><span class="nb">print</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">)</span>
</span></code></pre></div>
<div class="admonition example">
<p class="admonition-title">输出结果</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>      height  weight
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>  0     0.0     0.0
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>  1     0.5     0.5
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>  2     1.0     1.0
</span></code></pre></div>
</div>
<hr />
<h2 id="_3">二、类别型特征<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p><strong>类别型特征（Categorical Features）指的是取值为离散类别</strong>而非数值大小有意义的特征。比如颜色（红、绿、蓝）、城市（北京、上海）、性别（男、女）等。这些特征无法直接输入大多数机器学习算法，需先进行编码和处理。</p>
<h3 id="21">2.1 类别型特征的类型<a class="headerlink" href="#21" title="Permanent link">&para;</a></h3>
<p>类别型特征主要分为两类：</p>
<p><strong>1. 名义型（Nominal）</strong></p>
<ul>
<li>没有顺序或大小的概念。</li>
<li>示例：颜色（红、绿、蓝）、职业、城市。</li>
</ul>
<p><strong>2. 序数型（Ordinal）</strong></p>
<ul>
<li>具有明确的顺序关系，但没有明确的间距或量化含义。</li>
<li>示例：教育程度（小学 &lt; 初中 &lt; 高中 &lt; 大学 &lt; 硕士 &lt; 博士）。</li>
</ul>
<h3 id="22">2.2 常见的编码方式<a class="headerlink" href="#22" title="Permanent link">&para;</a></h3>
<h4 id="1-label-encoding">1. 标签编码（Label Encoding）<a class="headerlink" href="#1-label-encoding" title="Permanent link">&para;</a></h4>
<p>使用整数代替类别。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span class="n">data</span><span class="p">[</span><span class="s1">&#39;color_encoded&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">])</span>
</span></code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>⚠️ 适合有序类别（Ordinal），不适合无序类别（Nominal），否则可能引入虚假顺序信息。</p>
</div>
<h4 id="2-one-hot-encoding">2. 独热编码（One-Hot Encoding）<a class="headerlink" href="#2-one-hot-encoding" title="Permanent link">&para;</a></h4>
<p>将每个类别转化为一个独立的二进制列。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;color&#39;</span><span class="p">)</span>
</span></code></pre></div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>⚠️ 维度可能会爆炸（高基数特征），适合树模型（如 XGBoost、LightGBM）不敏感于稀疏特征。</p>
</div>
<h4 id="3-binary-encoding">3. 二进制编码（Binary Encoding）<a class="headerlink" href="#3-binary-encoding" title="Permanent link">&para;</a></h4>
<p>将类别先转换为整数，再转换为二进制编码。</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>类别A → 1 → 001
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>类别B → 2 → 010
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>类别C → 3 → 011
</span></code></pre></div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>优点：比 One-Hot 更节省空间，适合中高基数类别。</p>
</div>
<h4 id="4-frequency-encoding">4. 频率编码（Frequency Encoding）<a class="headerlink" href="#4-frequency-encoding" title="Permanent link">&para;</a></h4>
<p>用某个类别出现的频率进行编码。</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">freq</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;city&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">data</span><span class="p">[</span><span class="s1">&#39;city_freq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;city&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="5-target-encoding-mean-encoding">5. 目标编码（Target Encoding / Mean Encoding）<a class="headerlink" href="#5-target-encoding-mean-encoding" title="Permanent link">&para;</a></h4>
<p>用某个类别对应的目标变量的均值进行编码。</p>
<p><strong>示例（分类问题）：</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">means</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;city&#39;</span><span class="p">)[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="n">data</span><span class="p">[</span><span class="s1">&#39;city_encoded&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;city&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>
</span></code></pre></div></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>⚠️ 有泄露风险，需使用交叉验证或训练集均值。</p>
</div>
<h3 id="23">2.3 实战<a class="headerlink" href="#23" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>    <span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">],</span>
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>    <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a><span class="p">})</span>
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a><span class="c1"># Label Encoding</span>
</span><span id="__span-10-8"><a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
</span><span id="__span-10-9"><a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
</span><span id="__span-10-10"><a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a><span class="n">df</span><span class="p">[</span><span class="s1">&#39;label_enc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">])</span>
</span><span id="__span-10-11"><a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a>
</span><span id="__span-10-12"><a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a><span class="c1"># One-Hot</span>
</span><span id="__span-10-13"><a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a><span class="n">df_onehot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;color&#39;</span><span class="p">)</span>
</span><span id="__span-10-14"><a id="__codelineno-10-14" name="__codelineno-10-14" href="#__codelineno-10-14"></a>
</span><span id="__span-10-15"><a id="__codelineno-10-15" name="__codelineno-10-15" href="#__codelineno-10-15"></a><span class="c1"># Target Encoding</span>
</span><span id="__span-10-16"><a id="__codelineno-10-16" name="__codelineno-10-16" href="#__codelineno-10-16"></a><span class="n">mean_map</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;color&#39;</span><span class="p">)[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="__span-10-17"><a id="__codelineno-10-17" name="__codelineno-10-17" href="#__codelineno-10-17"></a><span class="n">df</span><span class="p">[</span><span class="s1">&#39;target_enc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">mean_map</span><span class="p">)</span>
</span><span id="__span-10-18"><a id="__codelineno-10-18" name="__codelineno-10-18" href="#__codelineno-10-18"></a>
</span><span id="__span-10-19"><a id="__codelineno-10-19" name="__codelineno-10-19" href="#__codelineno-10-19"></a><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</span><span id="__span-10-20"><a id="__codelineno-10-20" name="__codelineno-10-20" href="#__codelineno-10-20"></a><span class="nb">print</span><span class="p">(</span><span class="n">df_onehot</span><span class="p">)</span>
</span></code></pre></div>
<hr />
<h2 id="_4">三、文本表示模型<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p>文本表示模型（Text Representation Models）是自然语言处理（NLP）中的核心技术，目标是将文本（词、句子、文档）转换为模型可处理的数值向量，以便进行分类、聚类、搜索、问答、生成等任务。</p>
<h3 id="31">3.1 概述：为什么需要文本表示<a class="headerlink" href="#31" title="Permanent link">&para;</a></h3>
<p>机器无法直接理解人类语言，需要将其转化为向量。好的表示方法应该满足：</p>
<ul>
<li>语义保留性：相似文本向量应接近</li>
<li>上下文敏感性：一个词在不同语境中应有不同表示</li>
<li>压缩性：高效表示，避免维度灾难</li>
<li>可泛化性：适用于下游任务，如分类、匹配、检索等</li>
</ul>
<h3 id="32">3.2 传统表示方法<a class="headerlink" href="#32" title="Permanent link">&para;</a></h3>
<h4 id="1-one-hot-encoding">1. One-Hot Encoding<a class="headerlink" href="#1-one-hot-encoding" title="Permanent link">&para;</a></h4>
<ul>
<li>每个词对应一个高维向量（如词表中第 123 个词为 <code>[0,...,1,...,0]</code>）  </li>
<li>缺点：<ul>
<li>稀疏  </li>
<li>不含语义  </li>
<li>向量之间无距离含义（如“猫”和“狗”距离和“猫”和“自行车”一样）  </li>
</ul>
</li>
</ul>
<h4 id="2-bag-of-words-bow">2. Bag of Words (BoW)<a class="headerlink" href="#2-bag-of-words-bow" title="Permanent link">&para;</a></h4>
<ul>
<li>统计词频：<code>[the: 4, cat: 1, sat: 1, on: 1, mat: 1]</code>  </li>
<li>缺点：忽略词序、无法捕捉上下文信息</li>
</ul>
<h4 id="3-tf-idf">3. TF-IDF<a class="headerlink" href="#3-tf-idf" title="Permanent link">&para;</a></h4>
<ul>
<li>TF：Term Frequency，词频  </li>
<li>IDF：Inverse Document Frequency，文档反频率  </li>
<li>优点：考虑了词语的重要性  </li>
<li>缺点：向量仍然稀疏、维度高、不含语义关系  </li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">TfidfVectorizer</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;我 爱 自然语言处理&quot;</span><span class="p">,</span> <span class="s2">&quot;自然语言 很 有趣&quot;</span><span class="p">]</span>
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
</span><span id="__span-11-5"><a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a><span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</span><span id="__span-11-6"><a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</span></code></pre></div>
<h3 id="33">3.3 分布式词向量<a class="headerlink" href="#33" title="Permanent link">&para;</a></h3>
<h4 id="1-word2vec">1. Word2Vec<a class="headerlink" href="#1-word2vec" title="Permanent link">&para;</a></h4>
<ul>
<li>提出者：Google（2013）</li>
<li>两种模型结构：<ul>
<li><strong>CBOW（Continuous Bag-of-Words）</strong>：根据上下文词预测中心词。</li>
<li><strong>Skip-Gram</strong>：根据中心词预测上下文词。</li>
</ul>
</li>
<li>优点：<ul>
<li>能捕捉词与词之间的语义/句法关系。</li>
<li>支持线性推理关系：如 <code>vector("王") - vector("男人") + vector("女人") ≈ vector("女王")</code>。</li>
</ul>
</li>
</ul>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">gensim.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Word2Vec</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="c1"># 训练语料（分词后的句子）</span>
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;我&quot;</span><span class="p">,</span> <span class="s2">&quot;爱&quot;</span><span class="p">,</span> <span class="s2">&quot;自然语言处理&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;自然语言&quot;</span><span class="p">,</span> <span class="s2">&quot;处理&quot;</span><span class="p">,</span> <span class="s2">&quot;很&quot;</span><span class="p">,</span> <span class="s2">&quot;有趣&quot;</span><span class="p">]]</span>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="c1"># 训练模型</span>
</span><span id="__span-12-7"><a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># sg=1 表示使用 Skip-Gram</span>
</span><span id="__span-12-8"><a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a>
</span><span id="__span-12-9"><a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a><span class="c1"># 获取词向量</span>
</span><span id="__span-12-10"><a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a><span class="n">vector</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;自然语言&#39;</span><span class="p">]</span>
</span><span id="__span-12-11"><a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a><span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 输出: (100,)</span>
</span></code></pre></div>
<h4 id="2-gloveglobal-vectors">2. GloVe（Global Vectors）<a class="headerlink" href="#2-gloveglobal-vectors" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>提出者</strong>：斯坦福大学 NLP 团队（2014 年）</li>
<li><strong>论文标题</strong>：<a href="https://aclanthology.org/D14-1162/">GloVe: Global Vectors for Word Representation</a></li>
<li><strong>核心思想</strong>：基于<strong>词与词之间的共现频率矩阵</strong>，通过矩阵因式分解获得词向量表示。</li>
</ul>
<h5 id="_5">基本原理<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h5>
<p>GloVe 不是预测模型（如 Word2Vec），而是<strong>基于计数的模型</strong>。它将一个大型语料库中所有词对的共现频率统计为矩阵 <span class="arithmatex">\( X_{ij} \)</span>，其中：</p>
<ul>
<li><span class="arithmatex">\( X_{ij} \)</span>：词 <span class="arithmatex">\( j \)</span> 在词 <span class="arithmatex">\( i \)</span> 的上下文中出现的次数</li>
</ul>
<p>然后通过构造损失函数，拟合以下关系：</p>
<div class="arithmatex">\[
w_i^T \cdot \tilde{w}_j + b_i + \tilde{b}_j = \log(X_{ij})
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\( w_i \)</span>：词 <span class="arithmatex">\( i \)</span> 的向量  </li>
<li><span class="arithmatex">\( \tilde{w}_j \)</span>：上下文词 <span class="arithmatex">\( j \)</span> 的向量  </li>
<li><span class="arithmatex">\( b_i, \tilde{b}_j \)</span>：偏置项  </li>
<li><span class="arithmatex">\( X_{ij} \)</span>：共现次数  </li>
</ul>
<p>通过优化这个目标函数，学习每个词和上下文词的低维稠密向量。</p>
<h5 id="word2vec">相比 Word2Vec 的优势<a class="headerlink" href="#word2vec" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>特征</th>
<th>GloVe</th>
<th>Word2Vec</th>
</tr>
</thead>
<tbody>
<tr>
<td>建模方式</td>
<td>基于全局共现矩阵</td>
<td>基于局部上下文窗口</td>
</tr>
<tr>
<td>训练机制</td>
<td>矩阵因式分解（线性回归）</td>
<td>神经网络上下文预测</td>
</tr>
<tr>
<td>上下文理解</td>
<td>全局语料统计，语义更稳定</td>
<td>局部窗口，语义动态，适配更好</td>
</tr>
<tr>
<td>能否在线训练</td>
<td>否（需预计算共现矩阵）</td>
<td>是</td>
</tr>
</tbody>
</table>
<h5 id="glove">使用 GloVe 的词向量<a class="headerlink" href="#glove" title="Permanent link">&para;</a></h5>
<p>GloVe 官方提供了多种预训练模型：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>语料</th>
<th>下载地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>50d</td>
<td>Wikipedia 6B</td>
<td><a href="https://nlp.stanford.edu/data/glove.6B.zip">glove.6B.zip</a></td>
</tr>
<tr>
<td>100d</td>
<td>Wikipedia 6B</td>
<td>同上</td>
</tr>
<tr>
<td>300d</td>
<td>Common Crawl 840B</td>
<td><a href="https://nlp.stanford.edu/data/glove.840B.300d.zip">glove.840B.300d.zip</a></td>
</tr>
</tbody>
</table>
<h5 id="glove-python">加载预训练 GloVe 向量（Python 示例）<a class="headerlink" href="#glove-python" title="Permanent link">&para;</a></h5>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="c1"># 加载 GloVe 文件</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="k">def</span><span class="w"> </span><span class="nf">load_glove_embeddings</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
</span><span id="__span-13-5"><a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-13-6"><a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="__span-13-7"><a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a>        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
</span><span id="__span-13-8"><a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a>            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
</span><span id="__span-13-9"><a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a>            <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-13-10"><a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a>            <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</span><span id="__span-13-11"><a id="__codelineno-13-11" name="__codelineno-13-11" href="#__codelineno-13-11"></a>            <span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
</span><span id="__span-13-12"><a id="__codelineno-13-12" name="__codelineno-13-12" href="#__codelineno-13-12"></a>    <span class="k">return</span> <span class="n">embeddings</span>
</span><span id="__span-13-13"><a id="__codelineno-13-13" name="__codelineno-13-13" href="#__codelineno-13-13"></a>
</span><span id="__span-13-14"><a id="__codelineno-13-14" name="__codelineno-13-14" href="#__codelineno-13-14"></a><span class="c1"># 使用示例</span>
</span><span id="__span-13-15"><a id="__codelineno-13-15" name="__codelineno-13-15" href="#__codelineno-13-15"></a><span class="n">glove_path</span> <span class="o">=</span> <span class="s2">&quot;glove.6B.100d.txt&quot;</span>
</span><span id="__span-13-16"><a id="__codelineno-13-16" name="__codelineno-13-16" href="#__codelineno-13-16"></a><span class="n">glove_vectors</span> <span class="o">=</span> <span class="n">load_glove_embeddings</span><span class="p">(</span><span class="n">glove_path</span><span class="p">)</span>
</span><span id="__span-13-17"><a id="__codelineno-13-17" name="__codelineno-13-17" href="#__codelineno-13-17"></a><span class="nb">print</span><span class="p">(</span><span class="n">glove_vectors</span><span class="p">[</span><span class="s2">&quot;king&quot;</span><span class="p">])</span>  <span class="c1"># 打印 &quot;king&quot; 的词向量</span>
</span></code></pre></div>
<h5 id="glove_1">GloVe 的意义<a class="headerlink" href="#glove_1" title="Permanent link">&para;</a></h5>
<ul>
<li>更好地捕捉语义相似性：如 “ice” 和 “snow” 会靠近，而 “ice” 和 “steam” 会被区分。</li>
<li>适用于需要静态词表示的 NLP 任务，如聚类、KMeans 可视化、信息检索等。</li>
</ul>
<h4 id="3-fasttext">3. FastText<a class="headerlink" href="#3-fasttext" title="Permanent link">&para;</a></h4>
<ul>
<li>提出者：Facebook AI（2016）</li>
<li>创新点：将词拆解为子词（n-gram）进行训练</li>
<li>解决 OOV 问题：未见过的词也可以通过其子词组合推导向量。</li>
<li>更适合中文和形态复杂语言。</li>
</ul>
<p><strong>特点:</strong></p>
<ul>
<li>"apple" 会被分成：&lt; ap, app, ppl, ple, le &gt; 等字符 n-gram。</li>
<li>每个词的向量 = 其子词向量平均或加权和。</li>
</ul>
<h5 id="fasttext">训练示例（使用 fasttext）<a class="headerlink" href="#fasttext" title="Permanent link">&para;</a></h5>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="c1"># 安装 fasttext（如果使用 Python，建议用 pip 安装 fasttext-wheel）</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>pip<span class="w"> </span>install<span class="w"> </span>fasttext-wheel
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">fasttext</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="c1"># 训练模型（文本文件每行为一句话，已进行分词）</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">fasttext</span><span class="o">.</span><span class="n">train_unsupervised</span><span class="p">(</span><span class="s1">&#39;corpus.txt&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;skipgram&#39;</span><span class="p">)</span>
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>
</span><span id="__span-15-6"><a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a><span class="c1"># 获取词向量</span>
</span><span id="__span-15-7"><a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a><span class="n">vec</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="s2">&quot;自然语言处理&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="34">3.4 上下文相关的表示方法<a class="headerlink" href="#34" title="Permanent link">&para;</a></h3>
<p>传统的词向量（如 Word2Vec、GloVe、FastText）是<strong>静态的</strong>：一个词在所有上下文中始终对应同一个向量。</p>
<p>然而，在实际语言中，一个词在不同上下文中含义可能不同。例如：</p>
<ul>
<li>“银行”（bank）在“金融银行”和“河岸”中语义完全不同。</li>
</ul>
<p><strong>上下文相关词向量的核心思想</strong></p>
<blockquote>
<p>为每个词在不同上下文中生成<strong>动态的表示向量</strong>。</p>
</blockquote>
<h4 id="1-elmoembeddings-from-language-models">1. ELMo（Embeddings from Language Models）<a class="headerlink" href="#1-elmoembeddings-from-language-models" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>提出者</strong>：AllenNLP（2018）</li>
<li><strong>本质</strong>：利用双向 LSTM（BiLSTM）语言模型，结合上下文生成词的动态表示。</li>
<li><strong>关键特性</strong>：<ul>
<li>每个词的向量由其上下文共同决定。</li>
<li>对词义歧义问题有较强建模能力。</li>
</ul>
</li>
</ul>
<h5 id="elmo">ELMo 表示方式<a class="headerlink" href="#elmo" title="Permanent link">&para;</a></h5>
<p>一个词的表示由多个 LSTM 层的输出加权求和得到：</p>
<div class="arithmatex">\[
\text{ELMo}_t = \gamma \sum_{k=0}^{L} s_k h_{t,k}
\]</div>
<ul>
<li><span class="arithmatex">\( h_{t,k} \)</span>：第 <span class="arithmatex">\( k \)</span> 层 BiLSTM 对第 <span class="arithmatex">\( t \)</span> 个词的输出</li>
<li><span class="arithmatex">\( s_k \)</span>：权重参数（通过训练学习）</li>
<li><span class="arithmatex">\( \gamma \)</span>：缩放因子</li>
</ul>
<h5 id="_6">优点<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h5>
<ul>
<li>跨句建模，语言理解更深入。</li>
<li>可用于下游任务微调。</li>
</ul>
<h4 id="2-gpt-generative-pre-trained-transformer">2. GPT 系列（Generative Pre-trained Transformer）<a class="headerlink" href="#2-gpt-generative-pre-trained-transformer" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>GPT</strong>：使用 Transformer 的 decoder 架构，左到右地建模文本（单向）。</li>
<li><strong>预训练任务</strong>：语言建模（预测下一个词）</li>
<li><strong>适合任务</strong>：生成类（如续写、对话生成）</li>
</ul>
<h5 id="gpt">GPT 特点：<a class="headerlink" href="#gpt" title="Permanent link">&para;</a></h5>
<ul>
<li>上下文建模能力强，适合开放式生成任务。</li>
<li>靠右侧上下文预测下一个词。</li>
</ul>
<h4 id="3-bertbidirectional-encoder-representations-from-transformers">3. BERT（Bidirectional Encoder Representations from Transformers）<a class="headerlink" href="#3-bertbidirectional-encoder-representations-from-transformers" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>提出者</strong>：Google AI（2018）</li>
<li><strong>模型结构</strong>：Transformer Encoder（双向）</li>
<li><strong>预训练任务</strong>：<ul>
<li><strong>Masked Language Modeling (MLM)</strong>：随机遮盖一些词并预测它们。</li>
<li><strong>Next Sentence Prediction (NSP)</strong>：判断两个句子是否为相邻句。</li>
</ul>
</li>
</ul>
<h5 id="bert">BERT 特点<a class="headerlink" href="#bert" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>双向建模</strong>：能同时看到左右上下文。</li>
<li><strong>上下文感知表示</strong>：词向量依赖所在句子语义。</li>
<li><strong>可微调性强</strong>：几乎适用于所有下游 NLP 任务（分类、问答、NER 等）。</li>
</ul>
<h5 id="bert-transformers">BERT 应用示例（使用 Transformers 库）<a class="headerlink" href="#bert-transformers" title="Permanent link">&para;</a></h5>
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="c1"># 加载预训练模型和分词器</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a><span class="c1"># 编码输入文本</span>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Natural Language Processing is amazing!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
</span><span id="__span-16-10"><a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</span><span id="__span-16-11"><a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a>
</span><span id="__span-16-12"><a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a><span class="c1"># 获取最后一层的词向量</span>
</span><span id="__span-16-13"><a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a><span class="n">last_hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
</span><span id="__span-16-14"><a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a><span class="nb">print</span><span class="p">(</span><span class="n">last_hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (batch_size, seq_len, hidden_size)</span>
</span></code></pre></div>
<h2 id="word2vec_1">四、Word2Vec<a class="headerlink" href="#word2vec_1" title="Permanent link">&para;</a></h2>
<p>Word2Vec 是最经典的词嵌入（word embedding）方法之一，由 Google 于 2013 年提出，用神经网络学习词的稠密向量表示。</p>
<h3 id="41-word2vec">4.1 为什么需要 Word2Vec？<a class="headerlink" href="#41-word2vec" title="Permanent link">&para;</a></h3>
<p>传统的词表示方法（如 one-hot 编码）有以下缺点：</p>
<ul>
<li>维度高：词表大时，向量维度也非常大。</li>
<li>稀疏：绝大多数维度都是 0。</li>
<li>不含语义关系：如 “king” 与 “queen” 之间没有任何向量关系。</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>Word2Vec 通过训练学习出具有语义信息的低维稠密向量（Dense Vector），能让“语义相近”的词在向量空间中“靠得更近”。</p>
</div>
<h3 id="42-word2vec">4.2 Word2Vec 的两种模型<a class="headerlink" href="#42-word2vec" title="Permanent link">&para;</a></h3>
<h4 id="1-cbowcontinuous-bag-of-words">1. CBOW（Continuous Bag of Words）<a class="headerlink" href="#1-cbowcontinuous-bag-of-words" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>输入</strong>：上下文词（context）</li>
<li><strong>输出</strong>：目标词（target word）</li>
<li><strong>目标</strong>：从上下文词预测中心词。</li>
</ul>
<h5 id="_7">示例：<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h5>
<ul>
<li>给定句子：“I love natural language processing”</li>
<li>中心词为 <code>"natural"</code>，上下文词为 <code>[I, love, language, processing]</code></li>
<li>CBOW 的任务是：根据上下文 <code>[I, love, language, processing]</code> 来预测 <code>"natural"</code></li>
</ul>
<h5 id="_8">特点：<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h5>
<ul>
<li>更适合小语料</li>
<li>训练速度快，效果稳定</li>
</ul>
<h4 id="2-skip-gram">2. Skip-Gram<a class="headerlink" href="#2-skip-gram" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>输入</strong>：中心词（target）</li>
<li><strong>输出</strong>：上下文词（context）</li>
<li><strong>目标</strong>：从中心词预测其上下文词。</li>
</ul>
<h5 id="_9">示例：<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h5>
<ul>
<li>给定中心词 <code>"natural"</code></li>
<li>预测其上下文词 <code>[I, love, language, processing]</code></li>
</ul>
<h5 id="_10">特点：<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h5>
<ul>
<li>更适合大语料</li>
<li>对低频词效果更好</li>
<li>训练时间相对较长，但语义表达能力强</li>
</ul>
<h5 id="cbow-vs-skip-gram">CBOW vs Skip-Gram 对比<a class="headerlink" href="#cbow-vs-skip-gram" title="Permanent link">&para;</a></h5>
<table>
<thead>
<tr>
<th>模型</th>
<th>输入</th>
<th>输出</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>CBOW</td>
<td>上下文词</td>
<td>中心词</td>
<td>快速，稳定</td>
<td>低频词效果一般</td>
</tr>
<tr>
<td>Skip-Gram</td>
<td>中心词</td>
<td>上下文词</td>
<td>低频词学习能力强</td>
<td>训练较慢</td>
</tr>
</tbody>
</table>
<h3 id="43">4.3 原理公式<a class="headerlink" href="#43" title="Permanent link">&para;</a></h3>
<p>以 Skip-Gram 模型为例，其核心目标是：</p>
<blockquote>
<p>给定一个中心词，预测其上下文中出现的词语。</p>
</blockquote>
<h4 id="skip-gram">Skip-Gram 的目标函数：<a class="headerlink" href="#skip-gram" title="Permanent link">&para;</a></h4>
<p>最大化语料中每个词的上下文词的条件概率乘积：</p>
<div class="arithmatex">\[
\mathcal{L} = \prod_{t=1}^{T} \prod_{\substack{-c \leq j \leq c \\ j \ne 0}} P(w_{t+j} \mid w_t)
\]</div>
<p>其中：</p>
<ul>
<li><span class="arithmatex">\( T \)</span>：语料中的词总数  </li>
<li><span class="arithmatex">\( c \)</span>：上下文窗口大小  </li>
<li><span class="arithmatex">\( w_t \)</span>：当前位置的词（中心词）  </li>
<li><span class="arithmatex">\( w_{t+j} \)</span>：中心词周围的上下文词</li>
</ul>
<h4 id="softmax">条件概率的建模（Softmax）：<a class="headerlink" href="#softmax" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
P(w_O \mid w_I) = \frac{\exp\left(v_{w_O}^\top \cdot v_{w_I}\right)}{\sum_{w=1}^{V} \exp\left(v_{w}^\top \cdot v_{w_I}\right)}
\]</div>
<ul>
<li><span class="arithmatex">\( v_{w_I} \)</span>：输入词 <span class="arithmatex">\( w_I \)</span> 的词向量（中心词）</li>
<li><span class="arithmatex">\( v_{w_O} \)</span>：输出词 <span class="arithmatex">\( w_O \)</span> 的词向量（上下文词）</li>
<li><span class="arithmatex">\( V \)</span>：词表大小（vocabulary size）</li>
</ul>
<h4 id="softmax_1">问题：Softmax 计算代价高<a class="headerlink" href="#softmax_1" title="Permanent link">&para;</a></h4>
<ul>
<li>每次更新都需要计算整个词表中所有词的得分和归一化</li>
<li>词表通常很大（上万甚至百万级），导致计算效率低</li>
</ul>
<h5 id="_11">解决方案（加速技巧）：<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h5>
<h6 id="1-hierarchical-softmax-softmax">1. Hierarchical Softmax（层次化 Softmax）<a class="headerlink" href="#1-hierarchical-softmax-softmax" title="Permanent link">&para;</a></h6>
<ul>
<li>构建一棵霍夫曼树（Huffman Tree）</li>
<li>每个词是树上的一个叶子节点</li>
<li>只需沿路径更新节点，复杂度降为 <span class="arithmatex">\( O(\log V) \)</span></li>
</ul>
<h6 id="2-negative-sampling">2. Negative Sampling（负采样）<a class="headerlink" href="#2-negative-sampling" title="Permanent link">&para;</a></h6>
<ul>
<li>每次只优化一个正样本 + 少量负样本（如 5-20 个）</li>
<li>训练速度显著提升，且效果稳定</li>
<li>本质上是用 logistic 回归训练一个二分类器：</li>
</ul>
<div class="arithmatex">\[
\log \sigma(v_{w_O}^\top v_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)} \left[ \log \sigma(-v_{w_i}^\top v_{w_I}) \right]
\]</div>
<ul>
<li><span class="arithmatex">\( \sigma(x) \)</span>：sigmoid 函数</li>
<li><span class="arithmatex">\( P_n(w) \)</span>：负样本分布</li>
<li><span class="arithmatex">\( k \)</span>：负样本数量</li>
</ul>
<h3 id="44-gensim-word2vec">4.4 使用 Gensim 训练 Word2Vec（实战）<a class="headerlink" href="#44-gensim-word2vec" title="Permanent link">&para;</a></h3>
<h4 id="word2vec_2">训练 Word2Vec 模型<a class="headerlink" href="#word2vec_2" title="Permanent link">&para;</a></h4>
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">gensim.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Word2Vec</span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>
</span><span id="__span-17-3"><a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a><span class="c1"># 训练语料：每句话是一个分词后的列表</span>
</span><span id="__span-17-4"><a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;我&quot;</span><span class="p">,</span> <span class="s2">&quot;爱&quot;</span><span class="p">,</span> <span class="s2">&quot;自然&quot;</span><span class="p">,</span> <span class="s2">&quot;语言&quot;</span><span class="p">,</span> <span class="s2">&quot;处理&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;语言&quot;</span><span class="p">,</span> <span class="s2">&quot;模型&quot;</span><span class="p">,</span> <span class="s2">&quot;很&quot;</span><span class="p">,</span> <span class="s2">&quot;有趣&quot;</span><span class="p">]]</span>
</span><span id="__span-17-5"><a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>
</span><span id="__span-17-6"><a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a><span class="c1"># 训练模型</span>
</span><span id="__span-17-7"><a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span>
</span><span id="__span-17-8"><a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a>    <span class="n">sentences</span><span class="p">,</span> 
</span><span id="__span-17-9"><a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a>    <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># 词向量维度</span>
</span><span id="__span-17-10"><a id="__codelineno-17-10" name="__codelineno-17-10" href="#__codelineno-17-10"></a>    <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>         <span class="c1"># 上下文窗口大小</span>
</span><span id="__span-17-11"><a id="__codelineno-17-11" name="__codelineno-17-11" href="#__codelineno-17-11"></a>    <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>      <span class="c1"># 最小词频</span>
</span><span id="__span-17-12"><a id="__codelineno-17-12" name="__codelineno-17-12" href="#__codelineno-17-12"></a>    <span class="n">sg</span><span class="o">=</span><span class="mi">1</span>              <span class="c1"># sg=1 表示 Skip-Gram，sg=0 表示 CBOW</span>
</span><span id="__span-17-13"><a id="__codelineno-17-13" name="__codelineno-17-13" href="#__codelineno-17-13"></a><span class="p">)</span>
</span><span id="__span-17-14"><a id="__codelineno-17-14" name="__codelineno-17-14" href="#__codelineno-17-14"></a>
</span><span id="__span-17-15"><a id="__codelineno-17-15" name="__codelineno-17-15" href="#__codelineno-17-15"></a><span class="c1"># 查看某个词的词向量</span>
</span><span id="__span-17-16"><a id="__codelineno-17-16" name="__codelineno-17-16" href="#__codelineno-17-16"></a><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;语言&quot;</span><span class="p">])</span>
</span><span id="__span-17-17"><a id="__codelineno-17-17" name="__codelineno-17-17" href="#__codelineno-17-17"></a>
</span><span id="__span-17-18"><a id="__codelineno-17-18" name="__codelineno-17-18" href="#__codelineno-17-18"></a><span class="c1"># 找出与“语言”最相似的词</span>
</span><span id="__span-17-19"><a id="__codelineno-17-19" name="__codelineno-17-19" href="#__codelineno-17-19"></a><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;语言&quot;</span><span class="p">))</span>
</span></code></pre></div>
<h4 id="_12">保存和加载模型<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h4>
<div class="language-python highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="c1"># 保存模型</span>
</span><span id="__span-18-2"><a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;word2vec.model&quot;</span><span class="p">)</span>
</span><span id="__span-18-3"><a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>
</span><span id="__span-18-4"><a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="c1"># 加载模型</span>
</span><span id="__span-18-5"><a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a><span class="kn">from</span><span class="w"> </span><span class="nn">gensim.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Word2Vec</span>
</span><span id="__span-18-6"><a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;word2vec.model&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="45-word2vec">4.5 Word2Vec 的应用场景<a class="headerlink" href="#45-word2vec" title="Permanent link">&para;</a></h3>
<ul>
<li>文本分类（词向量作为输入）</li>
<li>命名实体识别（NER）</li>
<li>情感分析</li>
<li>相似词推荐 / 搜索引擎</li>
<li>可视化（如 t-SNE 降维）</li>
</ul>
<h3 id="46">4.6 向量推理的神奇之处<a class="headerlink" href="#46" title="Permanent link">&para;</a></h3>
<p>Word2Vec 向量空间可以执行语义类比：</p>
<div class="arithmatex">\[
    \text{vector}(“国王”) - \text{vector}(“男人”) + \text{vector}(“女人”) \approx \text{vector}(“女王”)
\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;女&quot;</span><span class="p">,</span> <span class="s2">&quot;国王&quot;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;男&quot;</span><span class="p">])</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>  <span class="c1"># 可能得到 &quot;女王&quot;</span>
</span></code></pre></div>
<h3 id="47">4.7 常见问题<a class="headerlink" href="#47" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>问题</th>
<th>解答说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Word2Vec 能处理未登录词（OOV）吗？</td>
<td>❌ 不能，必须出现在训练语料中，未见过的词没有向量。</td>
</tr>
<tr>
<td>Word2Vec 可以用于中文吗？</td>
<td>✅ 可以，但必须先对中文进行分词处理（如使用jieba）。</td>
</tr>
<tr>
<td>如何选择 vector_size 参数？</td>
<td>常用为 100、200、300，推荐根据任务复杂度和语料量做调优。</td>
</tr>
<tr>
<td>CBOW 和 Skip-Gram 哪个更好？</td>
<td>CBOW 快速且适用于高频词，Skip-Gram 更适合表示低频词语义。</td>
</tr>
<tr>
<td>训练语料越多越好吗？</td>
<td>✅ 是的，语料越大，词向量越能捕捉复杂语义关系。</td>
</tr>
<tr>
<td>Word2Vec 能表示多义词吗？</td>
<td>❌ 不能，每个词只有一个静态向量，无法区分语境。</td>
</tr>
<tr>
<td>是否需要标准化词向量？</td>
<td>训练后通常会进行归一化，以便进行向量相似度计算（如余弦相似度）。</td>
</tr>
<tr>
<td>Word2Vec 和 TF-IDF 有什么区别？</td>
<td>TF-IDF 是稀疏表示，Word2Vec 是低维稠密表示，且包含语义信息。</td>
</tr>
</tbody>
</table>
<p><strong>提示：</strong></p>
<ul>
<li>若任务对词语的上下文语义区分度要求很高（如情感分析、问答系统），建议使用上下文感知模型（如 ELMo 或 BERT）。</li>
<li>Word2Vec 是轻量级、计算成本低的首选向量表示方式，适合很多嵌入需求。</li>
</ul>
<h3 id="48">4.8 总结<a class="headerlink" href="#48" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>项目</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型名称</td>
<td>Word2Vec</td>
</tr>
<tr>
<td>核心结构</td>
<td>CBOW（上下文预测中心词）<br>Skip-Gram（中心词预测上下文）</td>
</tr>
<tr>
<td>输入</td>
<td>单词 ID 或上下文词</td>
</tr>
<tr>
<td>输出</td>
<td>对应词向量</td>
</tr>
<tr>
<td>优点</td>
<td>语义表达能力强、训练速度快、结构简单、适用于多种下游任务</td>
</tr>
<tr>
<td>缺点</td>
<td>静态表示（无上下文变化）<br>无法处理 OOV 和多义词</td>
</tr>
<tr>
<td>训练方法</td>
<td>使用浅层神经网络 + Softmax 或负采样（Negative Sampling）</td>
</tr>
<tr>
<td>加速技巧</td>
<td>Negative Sampling、Hierarchical Softmax</td>
</tr>
<tr>
<td>应用场景</td>
<td>文本分类、聚类、相似度计算、情感分析、搜索排序、推荐系统等</td>
</tr>
<tr>
<td>替代方案</td>
<td>GloVe、FastText、ELMo、BERT 等上下文相关模型</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.tabs.link", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.indexes", "toc.follow", "navigation.top", "search.suggest", "navigation.path", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="../../../styles/js/tablesort.min.js"></script>
      
        <script src="../../../styles/js/tex-mml-chtml.js"></script>
      
    
  </body>
</html>