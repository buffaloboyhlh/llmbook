# 第二章：数据预处理与特征工程

## 标准化与归一化（StandardScaler / MinMaxScaler）

### 🔹 一、标准化（Standardization）

将特征转换为均值为 0，标准差为 1的分布。

$$
x’ = \frac{x - \mu}{\sigma}
$$

+ $\mu$：特征的均值
+ $\sigma$：特征的标准差

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # X 为二维数组
```

**📌 适用场景：**

+ 假设数据服从高斯分布（如线性回归、SVM、PCA）
+ 对异常值不敏感的模型

### 🔹 二、归一化（Normalization）

将数据缩放到一个**固定范围**（通常是 [0, 1]）内。

$$
x’ = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
$$

+ $x_{\min}$：特征的最小值
+ $x_{\max}$：特征的最大值

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```

**📌 适用场景：**

+ 特征没有明显的分布规律
+ 需要将特征限制在统一区间（如图像像素值）
+ 对异常值较敏感

###  三、标准化 vs 归一化 对比表

| 项目         | 标准化（StandardScaler）          | 归一化（MinMaxScaler）           |
|--------------|------------------------------------|----------------------------------|
| 数学定义     | 转换为均值为 0、标准差为 1 的分布 | 缩放到指定范围（如 0~1）         |
| 公式         | \( x' = \frac{x - \mu}{\sigma} \) | \( x' = \frac{x - x_{min}}{x_{max} - x_{min}} \) |
| 是否受异常值影响 | 否                             | 是                               |
| 应用场景     | SVM、PCA、线性回归                | 神经网络、图像处理、距离算法     |
| 是否考虑分布 | 是                                 | 否                               |

### 四、实际示例（含异常值）

```python
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# 示例数据（包含异常值）
X = np.array([[1], [2], [3], [100]])

# 标准化
std_scaler = StandardScaler()
X_std = std_scaler.fit_transform(X)

# 归一化
minmax_scaler = MinMaxScaler()
X_minmax = minmax_scaler.fit_transform(X)

print("原始数据:\n", X.flatten())
print("标准化:\n", X_std.flatten())
print("归一化:\n", X_minmax.flatten())
```

## 缺失值处理、异常值检测

### 🧩 一、缺失值处理（Missing Values）

#### ✅ 1. 什么是缺失值？

缺失值是指数据集中某些样本的特征值为空或不可用，常见表示为：

+ NaN
+ 空字符串 ""
+ 特定值 999, -1（需要自行识别）


#### 🔧 2. 缺失值处理方法

| 方法               | 描述                                | 使用建议                   |
|--------------------|-------------------------------------|----------------------------|
| 删除样本或特征      | 删除含缺失值的行或列                 | 缺失值极少时                |
| 均值/中位数填充     | 用该特征的平均值/中位数替换缺失值   | 数值型特征、缺失比例较小     |
| 众数填充           | 用出现频率最高的值替换               | 类别型特征                  |
| 插值法             | 用前后数据的趋势填充                 | 时间序列、连续数据          |
| 模型预测填充       | 用其他特征建模预测缺失值             | 高级处理方式，适用于复杂情况 |

#### 🛠️ 3. Python 示例

```python
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer

# 构造带缺失值数据
data = pd.DataFrame({
    'A': [1, 2, np.nan, 4],
    'B': ['x', np.nan, 'y', 'z']
})

# 数值型列用均值填充
imputer_num = SimpleImputer(strategy='mean')
data['A'] = imputer_num.fit_transform(data[['A']])

# 类别型列用众数填充
imputer_cat = SimpleImputer(strategy='most_frequent')
data['B'] = imputer_cat.fit_transform(data[['B']])

print(data)
```

### 🚨 二、异常值检测（Outlier Detection）

#### ✅ 1. 什么是异常值？

异常值是指与样本整体分布显著不同的数据点。常见表现有：

+ 特征值极端大/小
+ 不符合大多数数据趋势

#### 🔍 2. 常用检测方法

| 方法                 | 描述                               | 优点                         | 缺点                         |
|----------------------|------------------------------------|------------------------------|------------------------------|
| **箱型图法（IQR）**   | 基于四分位数检测离群点             | 简单直观                     | 仅适合单变量                 |
| **Z-Score 法**        | 看标准差距离                       | 易于实现                     | 需服从正态分布               |
| **基于模型的检测**    | 使用 Isolation Forest / One-Class SVM | 可处理高维数据               | 训练开销较大                 |
| **聚类法**            | 如 DBSCAN 检测稀疏点               | 直观灵活                     | 需调参，可能误判             |

### 📦 3. Python 示例

##### ✅ 方法一：Z-Score 检测异常值

```python
from scipy.stats import zscore
import numpy as np

X = np.array([10, 12, 13, 11, 9, 12, 300])  # 最后一个为异常值
z = np.abs(zscore(X))
print("Z-score:", z)

# 判断阈值
outliers = X[z > 3]
print("异常值:", outliers)
```

##### ✅ 方法二：箱型图法（IQR）

```python
import pandas as pd

data = pd.Series([10, 12, 13, 11, 9, 12, 300])
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1

# 判定异常值
outliers = data[(data < Q1 - 1.5 * IQR) | (data > Q3 + 1.5 * IQR)]
print("异常值:\n", outliers)
```

##### ✅ 方法三：使用 Sklearn 的 Isolation Forest

```python
from sklearn.ensemble import IsolationForest

X = np.array([[10], [12], [13], [11], [9], [12], [300]])
iso = IsolationForest(contamination=0.1)
outliers = iso.fit_predict(X)

print("标记（-1为异常）:", outliers)
```

### 🧾 三、总结表：缺失值与异常值处理对比

| 项目       | 缺失值处理                      | 异常值处理                          |
|------------|----------------------------------|--------------------------------------|
| 表现形式   | NaN、空字符串、特殊值等          | 极大/极小值、不符合分布的点         |
| 是否常见   | 非常常见                         | 相对较少，但影响大                   |
| 是否需要处理 | 是                               | 是                                   |
| 常用方法   | 均值/中位数/众数填充、插值、模型 | Z-score、IQR、Isolation Forest、SVM  |


## 特征编码：One-Hot、Label Encoding

### 1️⃣ 为什么需要特征编码？

机器学习模型无法直接处理类别型特征（如颜色、城市名、性别等），因此我们需要将其转换为数字形式（即编码），以便模型能够理解和处理。


### 2️⃣ One-Hot Encoding（独热编码）

将每个类别表示为一个长度为类别数的向量，只有当前类别位置为 1，其他为 0。

#### 📊 示例：

假设特征“颜色”有三个取值：['红', '绿', '蓝']
One-Hot 编码后：

| 颜色 | 红 | 绿 | 蓝 |
|------|----|----|----|
| 红   | 1  | 0  | 0  |
| 绿   | 0  | 1  | 0  |
| 蓝   | 0  | 0  | 1  |

#### 🧠 特点：

+ 不存在类别间的顺序关系
+ 增加维度（高维稀疏）
+ 不适合类别特别多的情况（如国家、用户 ID）

#### 🛠️ Python 实现：

```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np

data = np.array([['红'], ['绿'], ['蓝']])
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(data)
print(encoded)
```

### 3️⃣ Label Encoding（标签编码）

将每个类别映射为一个整数，例如：

| 颜色 | 编码 |
|------|------|
| 红   | 0    |
| 绿   | 1    |
| 蓝   | 2    |

#### 🧠 特点：

+ 简单快速，不增加维度
+ 默认引入顺序关系（对无序特征不合理）
+ 常用于树模型（如决策树、随机森林、XGBoost）

#### 🛠️ Python 实现：

```python
from sklearn.preprocessing import LabelEncoder

data = ['红', '绿', '蓝']
encoder = LabelEncoder()
encoded = encoder.fit_transform(data)
print(encoded)
```

#### 📊 One-Hot vs Label Encoding 对比表

| 项目           | One-Hot Encoding               | Label Encoding                |
|----------------|--------------------------------|-------------------------------|
| 原理           | 每个类别一个独立的二进制位     | 每个类别映射为一个整数       |
| 维度变化       | 增加维度（高维稀疏）            | 不增加维度                   |
| 是否引入顺序   | ❌ 否                          | ✅ 是（可能引入错误顺序）     |
| 是否适用于树模型 | ⭕ 可选（但不必要）             | ✅ 非常适合                   |
| 是否适用于线性模型 | ✅ 推荐（避免顺序误导）         | ❌ 不推荐（会误导模型）       |
| 类别数量很大时 | ❌ 不推荐（维度灾难）            | ✅ 推荐                       |



## 特征选择与降维：PCA、LDA、SelectKBest

### 1️⃣ 特征选择 vs 降维：概念区分

| 项目             | 特征选择（Feature Selection）       | 降维（Dimensionality Reduction） |
|------------------|--------------------------------------|-----------------------------------|
| 本质             | 保留原始特征的一部分                 | 通过变换生成新的特征（压缩表示） |
| 是否保留原始特征 | ✅ 是                                 | ❌ 否                             |
| 举例             | SelectKBest、Lasso                   | PCA、LDA                         |
| 应用目的         | 删除冗余、无关特征，提升模型效果     | 减少维度、去噪、可视化           |


### 2️⃣ PCA（主成分分析）

+ 通过线性变换，将高维特征映射到低维空间，使方差最大化
+ 不考虑标签（无监督）

#### 📊 作用：

+ 用更少的维度表达尽可能多的信息（方差）
+ 去除冗余、相关性强的特征

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print("原始维度:", X.shape)
print("降维后:", X_pca.shape)
```

### 3️⃣ LDA（线性判别分析）

+ 同样是降维方法，但考虑类别标签（监督式）
+ 最大化类间距离，最小化类内距离 → 提高分类可分性

#### 📊 作用：

+ 降维同时提高分类性能
+ 类似 PCA，但适用于分类问题

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA(n_components=2)
X_lda = lda.fit_transform(X, y)

print("降维后:", X_lda.shape)
```

### 4️⃣ SelectKBest（单变量特征选择）

+ 对每个特征与目标变量计算一个统计量（如 F 分数、卡方、互信息）
+ 按分数选择前 K 个最相关特征

#### 📊 常用评分函数：

+ f_classif：用于分类任务（ANOVA F-test）
+ chi2：卡方检验（非负特征）
+ mutual_info_classif：互信息（非线性相关）

```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=2)
X_selected = selector.fit_transform(X, y)

print("特征选择后:", X_selected.shape)
```

### 📊 方法对比表

| 方法        | 类型       | 是否监督 | 是否保留原始特征 | 特点                                  | 适用场景       |
|-------------|------------|-----------|-------------------|---------------------------------------|----------------|
| PCA         | 降维       | ❌ 无监督 | ❌ 否              | 最大化方差，消除多重共线性            | 无监督学习、压缩 |
| LDA         | 降维       | ✅ 有监督 | ❌ 否              | 最大化类间差异，提高分类效果          | 分类任务       |
| SelectKBest | 特征选择   | ✅ 有监督 | ✅ 是              | 单变量选择，快速有效                   | 快速筛特征     |





