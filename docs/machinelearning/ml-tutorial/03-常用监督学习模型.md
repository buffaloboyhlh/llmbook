# 第三章：常用监督学习模型

##  回归模型

### 线性回归（Linear Regression）

#### 🌟 1. 什么是线性回归？

线性回归是最简单、最经典的回归分析方法，用于建模因变量（目标）与一个或多个自变量（特征）之间的线性关系。
通俗地说：
> 给定一些点，用一条直线把它们尽量“拟合”好。

根据输入变量的数量，分为：

+ 简单线性回归：只有一个特征变量。
+ 多元线性回归：有多个特征变量。

#### ✨ 2. 数学表达

##### （1）简单线性回归


模型形式：

\[
y = w x + b
\]

其中：

- \(y\)：目标变量
- \(x\)：输入特征
- \(w\)：斜率（回归系数）
- \(b\)：截距（偏置）


##### （2）多元线性回归

模型形式：

\[
y = w_1 x_1 + w_2 x_2 + \cdots + w_p x_p + b
\]

矩阵形式：

\[
y = \mathbf{X}\mathbf{w} + b
\]

或（将偏置合并进权重）：

\[
y = \mathbf{X}\mathbf{w}
\]

其中：

- \(\mathbf{X}\)：\(n \times p\) 矩阵（n 个样本，p 个特征）
- \(\mathbf{w}\)：权重向量

#### 🧮 3. 损失函数与最小二乘法


**最小二乘法（OLS, Ordinary Least Squares）**思想：
> 寻找参数 \(\mathbf{w}, b\)，使预测值和真实值之间的**平方误差最小**。

目标函数：

\[
J(\mathbf{w}, b) = \sum_{i=1}^n \left( y_i - (\mathbf{x}_i \cdot \mathbf{w} + b) \right)^2
\]

最优解（闭式解）：

\[
\hat{\mathbf{w}} = \left(\mathbf{X}^\top \mathbf{X}\right)^{-1} \mathbf{X}^\top \mathbf{y}
\]

> ⚠️ 当特征高度相关（共线性）或特征数超过样本数时，\(\mathbf{X}^\top \mathbf{X}\) 不可逆，需要正则化（如岭回归）。

#### 🛠️ 4. Python 实现示例

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# 样本数据
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# 创建模型
model = LinearRegression()

# 拟合
model.fit(X, y)

# 输出系数和截距
print("斜率（w）：", model.coef_)
print("截距（b）：", model.intercept_)

# 预测
y_pred = model.predict(X)
print("预测结果：", y_pred)
```
!!! Example "输出结果"
  
    ```text
        斜率（w）： [2.]
        截距（b）： 0.0
        预测结果： [ 2.  4.  6.  8. 10.]
    ```

##### 多元线性回归

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# 样本数据：5 个样本，2 个特征
# X: (n_samples, n_features)
X = np.array([
    [1, 2],
    [2, 0],
    [3, 1],
    [4, 3],
    [5, 5]
])

# 目标变量
y = np.array([5, 6, 9, 13, 17])

# 创建模型
model = LinearRegression()

# 拟合
model.fit(X, y)

# 输出系数和截距
print("回归系数（w）：", model.coef_)
print("截距（b）：", model.intercept_)

# 预测
y_pred = model.predict(X)
print("预测结果：", y_pred)
```
!!! Example "输出结果"

        回归系数（w）： [1.63636364 2.27272727]
        截距（b）： 1.090909090909092
        预测结果： [ 7.27272727  4.36363636  8.27272727 13.45454545 18.63636364]

这表示：

$y \approx 1.636 \cdot x_1 + 2.273 \cdot x_2 + 1.091$

#### 💡 5. 应用场景

+ 销售预测（根据广告费用预测销量）
+ 房价预测（根据面积、地段等特征）
+ 风险建模（根据客户属性预测违约概率）
+ 时间序列分析（简单趋势建模）

#### ⚖️ 6. 优缺点

##### ✅ **优点**

- **简单直观**  
  算法易于理解和解释。

- **训练快速**  
  计算复杂度低，适合小到中等规模数据。

- **解释性强**  
  模型系数可以清楚反映各个特征对目标的影响。

- **基础性强**  
  是许多复杂模型（如岭回归、Lasso、广义线性模型）的基础。

##### ❌ **缺点**

- **线性假设**  
  假设输入与输出之间是线性关系，如果实际是非线性关系，模型拟合效果差。

- **对异常值敏感**  
  极端值会显著影响模型系数。

- **多重共线性问题**  
  特征之间高度相关时，系数不稳定，模型解释性下降。

- **无法捕捉复杂模式**  
  无法自动建模变量之间的交互或高阶关系。


### 决策树回归（DecisionTreeRegressor）

#### 🌟 1. 概念与原理

决策树回归是一种基于树结构的回归模型，通过不断地划分特征空间，将数据分割成一系列更小的区域，然后在每个区域内预测一个固定的值（通常是区域样本的均值）。

**通俗理解：**
就像用“如果…那么…”的条件分割数据。例如：
> 如果面积 < 100 平方米，则房价 ≈ 80万；否则 ≈ 150万。

#### 🧠 2. 算法流程

1. **选择最佳特征与切分点**
    - 在所有特征和切分点中找到能最小化MSE（均方误差）的划分。

2. **分裂数据**
    - 根据最优划分将数据分为两个子区域。

3. **递归划分**
    - 对每个子区域继续分裂。

4. **停止条件**
    - 达到最大深度
    - 子集样本数低于最小样本数
    - 分裂不再降低误差

5. **叶节点预测**
    - 每个叶子节点的预测值 = 所含样本目标变量的平均值。

#### ✨ 3. Python 示例

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

# 样本数据
X = np.arange(0, 6, 0.5).reshape(-1, 1)
y = np.sin(X).ravel() + np.random.randn(len(X)) * 0.1

# 创建决策树回归器
tree = DecisionTreeRegressor(max_depth=3)

# 拟合
tree.fit(X, y)

# 预测
X_test = np.linspace(0, 6, 100).reshape(-1, 1)
y_pred = tree.predict(X_test)

# 绘制回归曲线
plt.scatter(X, y, color="black", label="Training Data")
plt.plot(X_test, y_pred, color="red", label="Prediction")
plt.legend()
plt.title("Decision Tree Regression")
plt.xlabel("X")
plt.ylabel("y")
plt.show()
```
#### 🎨 4. 绘制决策树结构

可以通过 plot_tree() 可视化树结构：
```python
from sklearn.tree import plot_tree

plt.figure(figsize=(12, 6))
plot_tree(
    tree,
    feature_names=["X"],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Decision Tree Structure")
plt.show()
```

#### 📊 使用场景

✅ **适合以下情境：**

- **非线性关系建模**
    - 特征与目标变量之间的关系复杂或有明显分段规律。

- **小型或中型数据集**
    - 样本量不大时，模型易于训练和解释。

- **需要可视化和解释**
    - 决策树可以清晰显示分裂条件和预测逻辑。

- **无需特征缩放**
    - 对数据的尺度不敏感，预处理简单。

- **混合类型特征**
    - 可以同时处理数值型和类别型变量。

**示例应用：**

- 房价预测（面积、地段、楼龄等）
- 客户价值评分
- 产品需求预测
- 能耗建模
- 简单的时间序列分段趋势建模

#### ⚖️ 优缺点

#####  ✅ **优点**
- **直观易解释**
    - 可以生成可视化的树结构。
- **支持非线性**
    - 不同分支可以学习不同的局部模式。
- **无需特征缩放**
    - 不依赖标准化或归一化。
- **处理混合数据**
    - 同时支持数值型和类别型特征。

##### ❌ **缺点**
- **容易过拟合**
    - 如果树太深，会在训练数据上表现很好，但泛化差。
- **不稳定**
    - 对小的输入波动敏感，树结构可能完全不同。
- **预测不连续**
    - 输出是分段常数，可能不够平滑。
- **不支持外推**
    - 在训练数据范围之外预测时不可靠。


### 支持向量回归（SVR）

#### 🌟 1. 概念与原理


**支持向量回归（SVR）**是支持向量机（SVM）在回归问题上的扩展。  
它的核心思想是：  
> 在误差不超过 ε 的情况下，找到一个最平滑的函数（即具有最大间隔的回归超平面）。

与普通回归不同，SVR在ε范围内的预测误差不计惩罚，只惩罚超出ε的误差。

**通俗理解：**

- 想象一条平滑的带有“容忍带(ε-tube)”的直线。
- 只对超出带子的点进行惩罚。

#### 🧠 2. 算法要点

- **目标函数**
    - 尽量让模型平滑（即权重小）
    - 同时尽量让多数点落在ε-tube内
- **核函数**
    - 通过核函数处理非线性关系（如RBF核）
- **ε-insensitive loss**
    - 在ε范围内误差视为0
- **支持向量**
    - 位于ε-tube外的点称为支持向量

#### ✨ 3. Python 示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR

# 样本数据
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel()

# 加一些噪声
y[::5] += 0.5 * (0.5 - np.random.rand(8))

# 创建 SVR 模型
svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)

# 拟合
svr_rbf.fit(X, y)

# 预测
X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = svr_rbf.predict(X_test)

# 绘图
plt.scatter(X, y, color='darkorange', label='data')
plt.plot(X_test, y_pred, color='navy', lw=2, label='SVR model')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Support Vector Regression')
plt.legend()
plt.show()
```

#### 📊 4. 使用场景

✅ 非线性回归问题
✅ 需要对异常值有一定鲁棒性
✅ 样本量中小型（SVR在大规模数据上较慢）

示例应用：

+ 股票价格走势预测
+ 非线性时间序列建模
+ 工业过程变量预测
+ 生物医学信号回归

#### ⚖️ 5. 优缺点

##### ✅ 优点

+ 支持非线性回归（通过核函数）
+ 对异常值鲁棒（ε不敏感区）
+ 能在高维特征空间建模复杂模式

##### ❌ 缺点

+ 大样本数据集训练速度慢
+ 超参数（C、ε、gamma）敏感，需要调参
+ 难以直接解释模型

### KNN 回归

#### 🌟 1. 概念与原理

**KNN回归**是一种基于实例的非参数回归方法，不构建显式的模型。  
它的核心思想是：  
> 对一个样本进行预测时，找到其最近的K个邻居，取这些邻居的目标变量的平均值作为预测结果。

与KNN分类不同：

- 分类：投票确定类别
- 回归：平均邻居的值

#### 🧠 2. 算法流程

1. **选择距离度量**
    - 常用欧几里得距离，也可以用曼哈顿距离等

2. **选取K值**
    - 例如K=3或K=5

3. **对新样本进行预测**
    - 计算其与所有训练样本的距离
    - 选择最近的K个样本
    - 取这些邻居的目标变量的平均值（或加权平均）

#### ✨ 3. Python示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor

# 样本数据
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel()

# 创建 KNN 回归器
knn = KNeighborsRegressor(n_neighbors=3)

# 拟合
knn.fit(X, y)

# 预测
X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = knn.predict(X_test)

# 绘图
plt.scatter(X, y, color='darkorange', label='data')
plt.plot(X_test, y_pred, color='navy', label='KNN prediction')
plt.xlabel('X')
plt.ylabel('y')
plt.title('KNN Regression (k=3)')
plt.legend()
plt.show()
```

#### 📊 4. 使用场景

✅ 特征与目标变量之间关系复杂
✅ 无需模型假设（非参数方法）
✅ 小数据集场景效果好

示例应用：

+ 房价预测（基于邻近房屋）
+ 个性化推荐（基于相似用户/商品）
+ 时间序列短期预测
+ 医疗数据局部模式建模

####  ⚖️ 5. 优缺点

#####  ✅ 优点

+ 简单直观，易理解
+ 无需训练过程（惰性学习）
+ 能捕捉任意复杂关系
+ 对异常值相对鲁棒

##### ❌ 缺点

+ 大数据集预测速度慢（需计算所有距离）
+ 对维度灾难敏感（高维数据效果差）
+ K值选择敏感（需交叉验证）
+ 不支持外推（只能在已有数据分布附近预测）



### 集成方法：随机森林回归、XGBoost、LightGBM

#### 随机森林回归

##### 🌟 概念

**随机森林回归**是基于集成学习中**Bagging（Bootstrap Aggregating）**思想的一种回归方法。  
它通过构建多棵决策树，并对它们的预测结果进行平均，来提升模型的稳定性和精度。

**通俗理解：**
> “让很多不同的树一起投票，大家平均意见。”

##### 🧠 算法原理

1. **Bootstrap采样：**
    - 从训练集有放回地采样，生成多份样本子集。

2. **训练决策树：**
    - 每棵树在一个子集上训练。
    - 每次分裂节点时，随机选择部分特征进行划分。

3. **集成：**
    - 所有树预测结果取平均值，作为最终回归输出。

##### ✨ Python示例

```python
from sklearn.ensemble import RandomForestRegressor
import numpy as np
import matplotlib.pyplot as plt

# 创建数据
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 0.5 * (0.5 - np.random.rand(16))  # 添加噪声

# 创建模型
rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    random_state=42
)

# 拟合
rf.fit(X, y)

# 预测
X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = rf.predict(X_test)

# 绘图
plt.scatter(X, y, color="darkorange", label="data")
plt.plot(X_test, y_pred, color="navy", label="Random Forest Prediction")
plt.xlabel("X")
plt.ylabel("y")
plt.title("Random Forest Regression")
plt.legend()
plt.show()
```

##### 📊 使用场景

✅ 非线性关系建模
✅ 需要高精度预测
✅ 小到中型数据集（大数据也可扩展）

示例：

+ 房价预测
+ 商品需求预测
+ 风险评分
+ 复杂时间序列建模

##### ⚖️ 优缺点

###### ✅ 优点

+ 易用，少量调参即可
+ 鲁棒性强，不易过拟合
+ 可以处理高维数据
+ 可估计特征重要性

###### ❌ 缺点

+ 训练和预测速度较慢（相比单棵树）
+ 模型较大，存储占用高
+ 不易解释（每棵树的规则复杂）

###### 📝 常用参数

+ n_estimators: 树的数量（默认100）
+ max_depth: 树的最大深度
+ min_samples_split: 内部分裂所需的最小样本数
+ max_features: 分裂时考虑的特征数量

#### XGBoost

##### 🌟 概念

**XGBoost**（Extreme Gradient Boosting）是一种高效的**梯度提升树（Gradient Boosting Tree）**库。  
它的核心思想：  
> 通过迭代构建多棵树，每棵树都拟合上一次预测的残差，不断提高模型性能。

相比传统GBDT，XGBoost在：

✅ 训练速度
✅ 精度
✅ 过拟合控制
✅ 并行化
上都有显著提升。


##### 🧠 算法要点

- **Boosting**
    - 多棵树按顺序建立，后一棵树纠正前一棵树的误差。
- **二阶梯度优化**
    - 利用损失函数一阶、二阶导数提高优化精度。
- **Shrinkage**
    - 学习率缩小每棵树贡献。
- **列采样**
    - 随机抽取部分特征增强多样性。
- **正则化**
    - 树复杂度惩罚防止过拟合。

##### ✨ Python示例

```python
import xgboost as xgb
from sklearn.metrics import mean_squared_error
import numpy as np

X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 0.5 * (0.5 - np.random.rand(16))

model = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

model.fit(X, y)

X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = model.predict(X_test)

mse = mean_squared_error(np.sin(X_test).ravel(), y_pred)
print("MSE:", mse)
```

##### 📊 使用场景

✅ 特征多、非线性关系复杂
✅ 大规模数据集
✅ 高精度预测需求

示例：

+ 房价预测
+ 销售预测
+ 金融风险评估
+ 用户评分预测
+ 时间序列建模

##### ⚖️ 优缺点

###### ✅ 优点

+ 高精度
+ 抗过拟合
+ 支持并行计算
+ 灵活性强

###### ❌ 缺点

+ 参数多，需要调优
+ 内存占用高
+ 解释性差

###### 📝 常用参数

+ n_estimators: 树数量
+ learning_rate: 学习率
+ max_depth: 树最大深度
+ subsample: 样本采样比
+ colsample_bytree: 特征采样比
+ reg_alpha: L1正则
+ reg_lambda: L2正则

#### LightGBM

**LightGBM（Light Gradient Boosting Machine）**是一个由微软开发的高性能梯度提升框架，专为大规模数据和高维稀疏数据而设计。

与传统GBDT相比：

✅ 更快的训练速度
✅ 更少的内存占用
✅ 更好的分布式支持
✅ 高准确性

##### 🧠 算法亮点

1️⃣ **Histogram-based分箱**

- 将连续特征离散化成有限个bin（直方图），减少内存和计算量。

2️⃣ **Leaf-wise生长策略**

- 每次选择增益最大的叶子分裂，拟合能力强。

3️⃣ **支持类别特征**

- 可直接处理分类变量。

4️⃣ **高效并行**

- 特征并行、数据并行、Voting并行。

##### ✨ Python示例

```python
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import numpy as np

X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 0.5 * (0.5 - np.random.rand(16))

model = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    num_leaves=31,
    random_state=42
)

model.fit(X, y)

X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = model.predict(X_test)

mse = mean_squared_error(np.sin(X_test).ravel(), y_pred)
print("MSE:", mse)
```

##### 📊 使用场景

✅ 大规模数据集
✅ 高维稀疏特征
✅ 高性能生产环境
✅ 复杂非线性关系建模

示例：

+ 销售预测
+ 广告点击率预测
+ 风险评估
+ Kaggle竞赛

##### ⚖️ 优缺点

######  ✅ 优点

+ 训练速度快
+ 内存占用低
+ 自动支持类别特征
+ 高精度
+ 分布式训练

######  ❌ 缺点

+ 更容易过拟合
+ 参数理解成本较高
+ 小数据集优势不明显

----

## 分类模型

----

### 逻辑回归（Logistic Regression）

#### 🌟 概念

逻辑回归是一种**分类模型**（不是回归模型）。  
通过学习线性函数，将样本映射到概率区间 [0,1]，再根据阈值进行分类。

通俗理解：  
> 用一条线把不同类别分开，同时把结果映射成概率。

逻辑回归常用于：  
✅ 二分类（最经典）  
✅ 多分类（Softmax扩展）

#### ✏️ 数学表达式

对于输入特征 \(x\) 和参数 \(\beta\)：  

1. **线性部分：**
   \[
   z = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n
   \]

2. **Sigmoid函数：**
   \[
   \sigma(z) = \frac{1}{1 + e^{-z}}
   \]
   输出概率 \(\in (0,1)\)

3. **预测分类：**
   \[
   \hat{y} =
   \begin{cases}
   1 & \text{if } \sigma(z) \geq 0.5 \\
   0 & \text{if } \sigma(z) < 0.5
   \end{cases}
   \]

#### 🧠 算法流程

1. 初始化权重
2. 最大似然估计（MLE）优化交叉熵损失：
   \[
   Loss = - \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i) \right]
   \]
3. 迭代更新权重（如梯度下降）

#### ✨ Python示例

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# 生成样本
X, y = make_classification(
    n_samples=200,
    n_features=4,
    n_informative=2,
    n_redundant=0,
    random_state=42
)

# 划分数据
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 创建模型
model = LogisticRegression()

# 拟合
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 输出结果
print(classification_report(y_test, y_pred))
```

####  📊 使用场景

✅ 二分类问题
✅ 特征与目标变量近似线性关系
✅ 概率输出需求

示例应用：

+ 信用风险判断
+ 广告点击预测
+ 疾病预测

####  ⚖️ 优缺点

##### ✅ 优点

+ 理论成熟、可解释性强
+ 收敛速度快
+ 概率输出直观
+ 小数据集表现良好

#####  ❌ 缺点

+ 只能建模线性边界
+ 对异常值敏感
+ 需要特征缩放和预处理

### 决策树分类（DecisionTreeClassifier）


### KNN 分类


### 支持向量机（SVM）


### 朴素贝叶斯（Naive Bayes）


### 集成方法：随机森林、梯度提升树、CatBoost


