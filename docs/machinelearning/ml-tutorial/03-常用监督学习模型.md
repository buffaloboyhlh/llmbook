# ç¬¬ä¸‰ç« ï¼šå¸¸ç”¨ç›‘ç£å­¦ä¹ æ¨¡å‹

##  å›å½’æ¨¡å‹

### çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰

#### ğŸŒŸ 1. ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’ï¼Ÿ

çº¿æ€§å›å½’æ˜¯æœ€ç®€å•ã€æœ€ç»å…¸çš„å›å½’åˆ†ææ–¹æ³•ï¼Œç”¨äºå»ºæ¨¡å› å˜é‡ï¼ˆç›®æ ‡ï¼‰ä¸ä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ï¼ˆç‰¹å¾ï¼‰ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚
é€šä¿—åœ°è¯´ï¼š
> ç»™å®šä¸€äº›ç‚¹ï¼Œç”¨ä¸€æ¡ç›´çº¿æŠŠå®ƒä»¬å°½é‡â€œæ‹Ÿåˆâ€å¥½ã€‚

æ ¹æ®è¾“å…¥å˜é‡çš„æ•°é‡ï¼Œåˆ†ä¸ºï¼š

+ ç®€å•çº¿æ€§å›å½’ï¼šåªæœ‰ä¸€ä¸ªç‰¹å¾å˜é‡ã€‚
+ å¤šå…ƒçº¿æ€§å›å½’ï¼šæœ‰å¤šä¸ªç‰¹å¾å˜é‡ã€‚

#### âœ¨ 2. æ•°å­¦è¡¨è¾¾

##### ï¼ˆ1ï¼‰ç®€å•çº¿æ€§å›å½’


æ¨¡å‹å½¢å¼ï¼š

\[
y = w x + b
\]

å…¶ä¸­ï¼š

- \(y\)ï¼šç›®æ ‡å˜é‡
- \(x\)ï¼šè¾“å…¥ç‰¹å¾
- \(w\)ï¼šæ–œç‡ï¼ˆå›å½’ç³»æ•°ï¼‰
- \(b\)ï¼šæˆªè·ï¼ˆåç½®ï¼‰


##### ï¼ˆ2ï¼‰å¤šå…ƒçº¿æ€§å›å½’

æ¨¡å‹å½¢å¼ï¼š

\[
y = w_1 x_1 + w_2 x_2 + \cdots + w_p x_p + b
\]

çŸ©é˜µå½¢å¼ï¼š

\[
y = \mathbf{X}\mathbf{w} + b
\]

æˆ–ï¼ˆå°†åç½®åˆå¹¶è¿›æƒé‡ï¼‰ï¼š

\[
y = \mathbf{X}\mathbf{w}
\]

å…¶ä¸­ï¼š

- \(\mathbf{X}\)ï¼š\(n \times p\) çŸ©é˜µï¼ˆn ä¸ªæ ·æœ¬ï¼Œp ä¸ªç‰¹å¾ï¼‰
- \(\mathbf{w}\)ï¼šæƒé‡å‘é‡

#### ğŸ§® 3. æŸå¤±å‡½æ•°ä¸æœ€å°äºŒä¹˜æ³•


**æœ€å°äºŒä¹˜æ³•ï¼ˆOLS, Ordinary Least Squaresï¼‰**æ€æƒ³ï¼š
> å¯»æ‰¾å‚æ•° \(\mathbf{w}, b\)ï¼Œä½¿é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„**å¹³æ–¹è¯¯å·®æœ€å°**ã€‚

ç›®æ ‡å‡½æ•°ï¼š

\[
J(\mathbf{w}, b) = \sum_{i=1}^n \left( y_i - (\mathbf{x}_i \cdot \mathbf{w} + b) \right)^2
\]

æœ€ä¼˜è§£ï¼ˆé—­å¼è§£ï¼‰ï¼š

\[
\hat{\mathbf{w}} = \left(\mathbf{X}^\top \mathbf{X}\right)^{-1} \mathbf{X}^\top \mathbf{y}
\]

> âš ï¸ å½“ç‰¹å¾é«˜åº¦ç›¸å…³ï¼ˆå…±çº¿æ€§ï¼‰æˆ–ç‰¹å¾æ•°è¶…è¿‡æ ·æœ¬æ•°æ—¶ï¼Œ\(\mathbf{X}^\top \mathbf{X}\) ä¸å¯é€†ï¼Œéœ€è¦æ­£åˆ™åŒ–ï¼ˆå¦‚å²­å›å½’ï¼‰ã€‚

#### ğŸ› ï¸ 4. Python å®ç°ç¤ºä¾‹

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# æ ·æœ¬æ•°æ®
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

# åˆ›å»ºæ¨¡å‹
model = LinearRegression()

# æ‹Ÿåˆ
model.fit(X, y)

# è¾“å‡ºç³»æ•°å’Œæˆªè·
print("æ–œç‡ï¼ˆwï¼‰ï¼š", model.coef_)
print("æˆªè·ï¼ˆbï¼‰ï¼š", model.intercept_)

# é¢„æµ‹
y_pred = model.predict(X)
print("é¢„æµ‹ç»“æœï¼š", y_pred)
```
!!! Example "è¾“å‡ºç»“æœ"
  
    ```text
        æ–œç‡ï¼ˆwï¼‰ï¼š [2.]
        æˆªè·ï¼ˆbï¼‰ï¼š 0.0
        é¢„æµ‹ç»“æœï¼š [ 2.  4.  6.  8. 10.]
    ```

##### å¤šå…ƒçº¿æ€§å›å½’

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# æ ·æœ¬æ•°æ®ï¼š5 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ªç‰¹å¾
# X: (n_samples, n_features)
X = np.array([
    [1, 2],
    [2, 0],
    [3, 1],
    [4, 3],
    [5, 5]
])

# ç›®æ ‡å˜é‡
y = np.array([5, 6, 9, 13, 17])

# åˆ›å»ºæ¨¡å‹
model = LinearRegression()

# æ‹Ÿåˆ
model.fit(X, y)

# è¾“å‡ºç³»æ•°å’Œæˆªè·
print("å›å½’ç³»æ•°ï¼ˆwï¼‰ï¼š", model.coef_)
print("æˆªè·ï¼ˆbï¼‰ï¼š", model.intercept_)

# é¢„æµ‹
y_pred = model.predict(X)
print("é¢„æµ‹ç»“æœï¼š", y_pred)
```
!!! Example "è¾“å‡ºç»“æœ"

        å›å½’ç³»æ•°ï¼ˆwï¼‰ï¼š [1.63636364 2.27272727]
        æˆªè·ï¼ˆbï¼‰ï¼š 1.090909090909092
        é¢„æµ‹ç»“æœï¼š [ 7.27272727  4.36363636  8.27272727 13.45454545 18.63636364]

è¿™è¡¨ç¤ºï¼š

$y \approx 1.636 \cdot x_1 + 2.273 \cdot x_2 + 1.091$

#### ğŸ’¡ 5. åº”ç”¨åœºæ™¯

+ é”€å”®é¢„æµ‹ï¼ˆæ ¹æ®å¹¿å‘Šè´¹ç”¨é¢„æµ‹é”€é‡ï¼‰
+ æˆ¿ä»·é¢„æµ‹ï¼ˆæ ¹æ®é¢ç§¯ã€åœ°æ®µç­‰ç‰¹å¾ï¼‰
+ é£é™©å»ºæ¨¡ï¼ˆæ ¹æ®å®¢æˆ·å±æ€§é¢„æµ‹è¿çº¦æ¦‚ç‡ï¼‰
+ æ—¶é—´åºåˆ—åˆ†æï¼ˆç®€å•è¶‹åŠ¿å»ºæ¨¡ï¼‰

#### âš–ï¸ 6. ä¼˜ç¼ºç‚¹

##### âœ… **ä¼˜ç‚¹**

- **ç®€å•ç›´è§‚**  
  ç®—æ³•æ˜“äºç†è§£å’Œè§£é‡Šã€‚

- **è®­ç»ƒå¿«é€Ÿ**  
  è®¡ç®—å¤æ‚åº¦ä½ï¼Œé€‚åˆå°åˆ°ä¸­ç­‰è§„æ¨¡æ•°æ®ã€‚

- **è§£é‡Šæ€§å¼º**  
  æ¨¡å‹ç³»æ•°å¯ä»¥æ¸…æ¥šåæ˜ å„ä¸ªç‰¹å¾å¯¹ç›®æ ‡çš„å½±å“ã€‚

- **åŸºç¡€æ€§å¼º**  
  æ˜¯è®¸å¤šå¤æ‚æ¨¡å‹ï¼ˆå¦‚å²­å›å½’ã€Lassoã€å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼‰çš„åŸºç¡€ã€‚

##### âŒ **ç¼ºç‚¹**

- **çº¿æ€§å‡è®¾**  
  å‡è®¾è¾“å…¥ä¸è¾“å‡ºä¹‹é—´æ˜¯çº¿æ€§å…³ç³»ï¼Œå¦‚æœå®é™…æ˜¯éçº¿æ€§å…³ç³»ï¼Œæ¨¡å‹æ‹Ÿåˆæ•ˆæœå·®ã€‚

- **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ**  
  æç«¯å€¼ä¼šæ˜¾è‘—å½±å“æ¨¡å‹ç³»æ•°ã€‚

- **å¤šé‡å…±çº¿æ€§é—®é¢˜**  
  ç‰¹å¾ä¹‹é—´é«˜åº¦ç›¸å…³æ—¶ï¼Œç³»æ•°ä¸ç¨³å®šï¼Œæ¨¡å‹è§£é‡Šæ€§ä¸‹é™ã€‚

- **æ— æ³•æ•æ‰å¤æ‚æ¨¡å¼**  
  æ— æ³•è‡ªåŠ¨å»ºæ¨¡å˜é‡ä¹‹é—´çš„äº¤äº’æˆ–é«˜é˜¶å…³ç³»ã€‚


### å†³ç­–æ ‘å›å½’ï¼ˆDecisionTreeRegressorï¼‰

#### ğŸŒŸ 1. æ¦‚å¿µä¸åŸç†

å†³ç­–æ ‘å›å½’æ˜¯ä¸€ç§åŸºäºæ ‘ç»“æ„çš„å›å½’æ¨¡å‹ï¼Œé€šè¿‡ä¸æ–­åœ°åˆ’åˆ†ç‰¹å¾ç©ºé—´ï¼Œå°†æ•°æ®åˆ†å‰²æˆä¸€ç³»åˆ—æ›´å°çš„åŒºåŸŸï¼Œç„¶ååœ¨æ¯ä¸ªåŒºåŸŸå†…é¢„æµ‹ä¸€ä¸ªå›ºå®šçš„å€¼ï¼ˆé€šå¸¸æ˜¯åŒºåŸŸæ ·æœ¬çš„å‡å€¼ï¼‰ã€‚

**é€šä¿—ç†è§£ï¼š**
å°±åƒç”¨â€œå¦‚æœâ€¦é‚£ä¹ˆâ€¦â€çš„æ¡ä»¶åˆ†å‰²æ•°æ®ã€‚ä¾‹å¦‚ï¼š
> å¦‚æœé¢ç§¯ < 100 å¹³æ–¹ç±³ï¼Œåˆ™æˆ¿ä»· â‰ˆ 80ä¸‡ï¼›å¦åˆ™ â‰ˆ 150ä¸‡ã€‚

#### ğŸ§  2. ç®—æ³•æµç¨‹

1. **é€‰æ‹©æœ€ä½³ç‰¹å¾ä¸åˆ‡åˆ†ç‚¹**
    - åœ¨æ‰€æœ‰ç‰¹å¾å’Œåˆ‡åˆ†ç‚¹ä¸­æ‰¾åˆ°èƒ½æœ€å°åŒ–MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰çš„åˆ’åˆ†ã€‚

2. **åˆ†è£‚æ•°æ®**
    - æ ¹æ®æœ€ä¼˜åˆ’åˆ†å°†æ•°æ®åˆ†ä¸ºä¸¤ä¸ªå­åŒºåŸŸã€‚

3. **é€’å½’åˆ’åˆ†**
    - å¯¹æ¯ä¸ªå­åŒºåŸŸç»§ç»­åˆ†è£‚ã€‚

4. **åœæ­¢æ¡ä»¶**
    - è¾¾åˆ°æœ€å¤§æ·±åº¦
    - å­é›†æ ·æœ¬æ•°ä½äºæœ€å°æ ·æœ¬æ•°
    - åˆ†è£‚ä¸å†é™ä½è¯¯å·®

5. **å¶èŠ‚ç‚¹é¢„æµ‹**
    - æ¯ä¸ªå¶å­èŠ‚ç‚¹çš„é¢„æµ‹å€¼ = æ‰€å«æ ·æœ¬ç›®æ ‡å˜é‡çš„å¹³å‡å€¼ã€‚

#### âœ¨ 3. Python ç¤ºä¾‹

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

# æ ·æœ¬æ•°æ®
X = np.arange(0, 6, 0.5).reshape(-1, 1)
y = np.sin(X).ravel() + np.random.randn(len(X)) * 0.1

# åˆ›å»ºå†³ç­–æ ‘å›å½’å™¨
tree = DecisionTreeRegressor(max_depth=3)

# æ‹Ÿåˆ
tree.fit(X, y)

# é¢„æµ‹
X_test = np.linspace(0, 6, 100).reshape(-1, 1)
y_pred = tree.predict(X_test)

# ç»˜åˆ¶å›å½’æ›²çº¿
plt.scatter(X, y, color="black", label="Training Data")
plt.plot(X_test, y_pred, color="red", label="Prediction")
plt.legend()
plt.title("Decision Tree Regression")
plt.xlabel("X")
plt.ylabel("y")
plt.show()
```
#### ğŸ¨ 4. ç»˜åˆ¶å†³ç­–æ ‘ç»“æ„

å¯ä»¥é€šè¿‡ plot_tree() å¯è§†åŒ–æ ‘ç»“æ„ï¼š
```python
from sklearn.tree import plot_tree

plt.figure(figsize=(12, 6))
plot_tree(
    tree,
    feature_names=["X"],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("Decision Tree Structure")
plt.show()
```

#### ğŸ“Š ä½¿ç”¨åœºæ™¯

âœ… **é€‚åˆä»¥ä¸‹æƒ…å¢ƒï¼š**

- **éçº¿æ€§å…³ç³»å»ºæ¨¡**
    - ç‰¹å¾ä¸ç›®æ ‡å˜é‡ä¹‹é—´çš„å…³ç³»å¤æ‚æˆ–æœ‰æ˜æ˜¾åˆ†æ®µè§„å¾‹ã€‚

- **å°å‹æˆ–ä¸­å‹æ•°æ®é›†**
    - æ ·æœ¬é‡ä¸å¤§æ—¶ï¼Œæ¨¡å‹æ˜“äºè®­ç»ƒå’Œè§£é‡Šã€‚

- **éœ€è¦å¯è§†åŒ–å’Œè§£é‡Š**
    - å†³ç­–æ ‘å¯ä»¥æ¸…æ™°æ˜¾ç¤ºåˆ†è£‚æ¡ä»¶å’Œé¢„æµ‹é€»è¾‘ã€‚

- **æ— éœ€ç‰¹å¾ç¼©æ”¾**
    - å¯¹æ•°æ®çš„å°ºåº¦ä¸æ•æ„Ÿï¼Œé¢„å¤„ç†ç®€å•ã€‚

- **æ··åˆç±»å‹ç‰¹å¾**
    - å¯ä»¥åŒæ—¶å¤„ç†æ•°å€¼å‹å’Œç±»åˆ«å‹å˜é‡ã€‚

**ç¤ºä¾‹åº”ç”¨ï¼š**

- æˆ¿ä»·é¢„æµ‹ï¼ˆé¢ç§¯ã€åœ°æ®µã€æ¥¼é¾„ç­‰ï¼‰
- å®¢æˆ·ä»·å€¼è¯„åˆ†
- äº§å“éœ€æ±‚é¢„æµ‹
- èƒ½è€—å»ºæ¨¡
- ç®€å•çš„æ—¶é—´åºåˆ—åˆ†æ®µè¶‹åŠ¿å»ºæ¨¡

#### âš–ï¸ ä¼˜ç¼ºç‚¹

#####  âœ… **ä¼˜ç‚¹**
- **ç›´è§‚æ˜“è§£é‡Š**
    - å¯ä»¥ç”Ÿæˆå¯è§†åŒ–çš„æ ‘ç»“æ„ã€‚
- **æ”¯æŒéçº¿æ€§**
    - ä¸åŒåˆ†æ”¯å¯ä»¥å­¦ä¹ ä¸åŒçš„å±€éƒ¨æ¨¡å¼ã€‚
- **æ— éœ€ç‰¹å¾ç¼©æ”¾**
    - ä¸ä¾èµ–æ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–ã€‚
- **å¤„ç†æ··åˆæ•°æ®**
    - åŒæ—¶æ”¯æŒæ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾ã€‚

##### âŒ **ç¼ºç‚¹**
- **å®¹æ˜“è¿‡æ‹Ÿåˆ**
    - å¦‚æœæ ‘å¤ªæ·±ï¼Œä¼šåœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†æ³›åŒ–å·®ã€‚
- **ä¸ç¨³å®š**
    - å¯¹å°çš„è¾“å…¥æ³¢åŠ¨æ•æ„Ÿï¼Œæ ‘ç»“æ„å¯èƒ½å®Œå…¨ä¸åŒã€‚
- **é¢„æµ‹ä¸è¿ç»­**
    - è¾“å‡ºæ˜¯åˆ†æ®µå¸¸æ•°ï¼Œå¯èƒ½ä¸å¤Ÿå¹³æ»‘ã€‚
- **ä¸æ”¯æŒå¤–æ¨**
    - åœ¨è®­ç»ƒæ•°æ®èŒƒå›´ä¹‹å¤–é¢„æµ‹æ—¶ä¸å¯é ã€‚


### æ”¯æŒå‘é‡å›å½’ï¼ˆSVRï¼‰

#### ğŸŒŸ 1. æ¦‚å¿µä¸åŸç†


**æ”¯æŒå‘é‡å›å½’ï¼ˆSVRï¼‰**æ˜¯æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰åœ¨å›å½’é—®é¢˜ä¸Šçš„æ‰©å±•ã€‚  
å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  
> åœ¨è¯¯å·®ä¸è¶…è¿‡ Îµ çš„æƒ…å†µä¸‹ï¼Œæ‰¾åˆ°ä¸€ä¸ªæœ€å¹³æ»‘çš„å‡½æ•°ï¼ˆå³å…·æœ‰æœ€å¤§é—´éš”çš„å›å½’è¶…å¹³é¢ï¼‰ã€‚

ä¸æ™®é€šå›å½’ä¸åŒï¼ŒSVRåœ¨ÎµèŒƒå›´å†…çš„é¢„æµ‹è¯¯å·®ä¸è®¡æƒ©ç½šï¼Œåªæƒ©ç½šè¶…å‡ºÎµçš„è¯¯å·®ã€‚

**é€šä¿—ç†è§£ï¼š**

- æƒ³è±¡ä¸€æ¡å¹³æ»‘çš„å¸¦æœ‰â€œå®¹å¿å¸¦(Îµ-tube)â€çš„ç›´çº¿ã€‚
- åªå¯¹è¶…å‡ºå¸¦å­çš„ç‚¹è¿›è¡Œæƒ©ç½šã€‚

#### ğŸ§  2. ç®—æ³•è¦ç‚¹

- **ç›®æ ‡å‡½æ•°**
    - å°½é‡è®©æ¨¡å‹å¹³æ»‘ï¼ˆå³æƒé‡å°ï¼‰
    - åŒæ—¶å°½é‡è®©å¤šæ•°ç‚¹è½åœ¨Îµ-tubeå†…
- **æ ¸å‡½æ•°**
    - é€šè¿‡æ ¸å‡½æ•°å¤„ç†éçº¿æ€§å…³ç³»ï¼ˆå¦‚RBFæ ¸ï¼‰
- **Îµ-insensitive loss**
    - åœ¨ÎµèŒƒå›´å†…è¯¯å·®è§†ä¸º0
- **æ”¯æŒå‘é‡**
    - ä½äºÎµ-tubeå¤–çš„ç‚¹ç§°ä¸ºæ”¯æŒå‘é‡

#### âœ¨ 3. Python ç¤ºä¾‹

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR

# æ ·æœ¬æ•°æ®
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel()

# åŠ ä¸€äº›å™ªå£°
y[::5] += 0.5 * (0.5 - np.random.rand(8))

# åˆ›å»º SVR æ¨¡å‹
svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)

# æ‹Ÿåˆ
svr_rbf.fit(X, y)

# é¢„æµ‹
X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = svr_rbf.predict(X_test)

# ç»˜å›¾
plt.scatter(X, y, color='darkorange', label='data')
plt.plot(X_test, y_pred, color='navy', lw=2, label='SVR model')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Support Vector Regression')
plt.legend()
plt.show()
```

#### ğŸ“Š 4. ä½¿ç”¨åœºæ™¯

âœ… éçº¿æ€§å›å½’é—®é¢˜
âœ… éœ€è¦å¯¹å¼‚å¸¸å€¼æœ‰ä¸€å®šé²æ£’æ€§
âœ… æ ·æœ¬é‡ä¸­å°å‹ï¼ˆSVRåœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè¾ƒæ…¢ï¼‰

ç¤ºä¾‹åº”ç”¨ï¼š

+ è‚¡ç¥¨ä»·æ ¼èµ°åŠ¿é¢„æµ‹
+ éçº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡
+ å·¥ä¸šè¿‡ç¨‹å˜é‡é¢„æµ‹
+ ç”Ÿç‰©åŒ»å­¦ä¿¡å·å›å½’

#### âš–ï¸ 5. ä¼˜ç¼ºç‚¹

##### âœ… ä¼˜ç‚¹

+ æ”¯æŒéçº¿æ€§å›å½’ï¼ˆé€šè¿‡æ ¸å‡½æ•°ï¼‰
+ å¯¹å¼‚å¸¸å€¼é²æ£’ï¼ˆÎµä¸æ•æ„ŸåŒºï¼‰
+ èƒ½åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´å»ºæ¨¡å¤æ‚æ¨¡å¼

##### âŒ ç¼ºç‚¹

+ å¤§æ ·æœ¬æ•°æ®é›†è®­ç»ƒé€Ÿåº¦æ…¢
+ è¶…å‚æ•°ï¼ˆCã€Îµã€gammaï¼‰æ•æ„Ÿï¼Œéœ€è¦è°ƒå‚
+ éš¾ä»¥ç›´æ¥è§£é‡Šæ¨¡å‹

### KNN å›å½’

#### ğŸŒŸ 1. æ¦‚å¿µä¸åŸç†

**KNNå›å½’**æ˜¯ä¸€ç§åŸºäºå®ä¾‹çš„éå‚æ•°å›å½’æ–¹æ³•ï¼Œä¸æ„å»ºæ˜¾å¼çš„æ¨¡å‹ã€‚  
å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  
> å¯¹ä¸€ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹æ—¶ï¼Œæ‰¾åˆ°å…¶æœ€è¿‘çš„Kä¸ªé‚»å±…ï¼Œå–è¿™äº›é‚»å±…çš„ç›®æ ‡å˜é‡çš„å¹³å‡å€¼ä½œä¸ºé¢„æµ‹ç»“æœã€‚

ä¸KNNåˆ†ç±»ä¸åŒï¼š

- åˆ†ç±»ï¼šæŠ•ç¥¨ç¡®å®šç±»åˆ«
- å›å½’ï¼šå¹³å‡é‚»å±…çš„å€¼

#### ğŸ§  2. ç®—æ³•æµç¨‹

1. **é€‰æ‹©è·ç¦»åº¦é‡**
    - å¸¸ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œä¹Ÿå¯ä»¥ç”¨æ›¼å“ˆé¡¿è·ç¦»ç­‰

2. **é€‰å–Kå€¼**
    - ä¾‹å¦‚K=3æˆ–K=5

3. **å¯¹æ–°æ ·æœ¬è¿›è¡Œé¢„æµ‹**
    - è®¡ç®—å…¶ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„è·ç¦»
    - é€‰æ‹©æœ€è¿‘çš„Kä¸ªæ ·æœ¬
    - å–è¿™äº›é‚»å±…çš„ç›®æ ‡å˜é‡çš„å¹³å‡å€¼ï¼ˆæˆ–åŠ æƒå¹³å‡ï¼‰

#### âœ¨ 3. Pythonç¤ºä¾‹

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor

# æ ·æœ¬æ•°æ®
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel()

# åˆ›å»º KNN å›å½’å™¨
knn = KNeighborsRegressor(n_neighbors=3)

# æ‹Ÿåˆ
knn.fit(X, y)

# é¢„æµ‹
X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = knn.predict(X_test)

# ç»˜å›¾
plt.scatter(X, y, color='darkorange', label='data')
plt.plot(X_test, y_pred, color='navy', label='KNN prediction')
plt.xlabel('X')
plt.ylabel('y')
plt.title('KNN Regression (k=3)')
plt.legend()
plt.show()
```

#### ğŸ“Š 4. ä½¿ç”¨åœºæ™¯

âœ… ç‰¹å¾ä¸ç›®æ ‡å˜é‡ä¹‹é—´å…³ç³»å¤æ‚
âœ… æ— éœ€æ¨¡å‹å‡è®¾ï¼ˆéå‚æ•°æ–¹æ³•ï¼‰
âœ… å°æ•°æ®é›†åœºæ™¯æ•ˆæœå¥½

ç¤ºä¾‹åº”ç”¨ï¼š

+ æˆ¿ä»·é¢„æµ‹ï¼ˆåŸºäºé‚»è¿‘æˆ¿å±‹ï¼‰
+ ä¸ªæ€§åŒ–æ¨èï¼ˆåŸºäºç›¸ä¼¼ç”¨æˆ·/å•†å“ï¼‰
+ æ—¶é—´åºåˆ—çŸ­æœŸé¢„æµ‹
+ åŒ»ç–—æ•°æ®å±€éƒ¨æ¨¡å¼å»ºæ¨¡

####  âš–ï¸ 5. ä¼˜ç¼ºç‚¹

#####  âœ… ä¼˜ç‚¹

+ ç®€å•ç›´è§‚ï¼Œæ˜“ç†è§£
+ æ— éœ€è®­ç»ƒè¿‡ç¨‹ï¼ˆæƒ°æ€§å­¦ä¹ ï¼‰
+ èƒ½æ•æ‰ä»»æ„å¤æ‚å…³ç³»
+ å¯¹å¼‚å¸¸å€¼ç›¸å¯¹é²æ£’

##### âŒ ç¼ºç‚¹

+ å¤§æ•°æ®é›†é¢„æµ‹é€Ÿåº¦æ…¢ï¼ˆéœ€è®¡ç®—æ‰€æœ‰è·ç¦»ï¼‰
+ å¯¹ç»´åº¦ç¾éš¾æ•æ„Ÿï¼ˆé«˜ç»´æ•°æ®æ•ˆæœå·®ï¼‰
+ Kå€¼é€‰æ‹©æ•æ„Ÿï¼ˆéœ€äº¤å‰éªŒè¯ï¼‰
+ ä¸æ”¯æŒå¤–æ¨ï¼ˆåªèƒ½åœ¨å·²æœ‰æ•°æ®åˆ†å¸ƒé™„è¿‘é¢„æµ‹ï¼‰



### é›†æˆæ–¹æ³•ï¼šéšæœºæ£®æ—å›å½’ã€XGBoostã€LightGBM

#### éšæœºæ£®æ—å›å½’

##### ğŸŒŸ æ¦‚å¿µ

**éšæœºæ£®æ—å›å½’**æ˜¯åŸºäºé›†æˆå­¦ä¹ ä¸­**Baggingï¼ˆBootstrap Aggregatingï¼‰**æ€æƒ³çš„ä¸€ç§å›å½’æ–¹æ³•ã€‚  
å®ƒé€šè¿‡æ„å»ºå¤šæ£µå†³ç­–æ ‘ï¼Œå¹¶å¯¹å®ƒä»¬çš„é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡ï¼Œæ¥æå‡æ¨¡å‹çš„ç¨³å®šæ€§å’Œç²¾åº¦ã€‚

**é€šä¿—ç†è§£ï¼š**
> â€œè®©å¾ˆå¤šä¸åŒçš„æ ‘ä¸€èµ·æŠ•ç¥¨ï¼Œå¤§å®¶å¹³å‡æ„è§ã€‚â€

##### ğŸ§  ç®—æ³•åŸç†

1. **Bootstrapé‡‡æ ·ï¼š**
    - ä»è®­ç»ƒé›†æœ‰æ”¾å›åœ°é‡‡æ ·ï¼Œç”Ÿæˆå¤šä»½æ ·æœ¬å­é›†ã€‚

2. **è®­ç»ƒå†³ç­–æ ‘ï¼š**
    - æ¯æ£µæ ‘åœ¨ä¸€ä¸ªå­é›†ä¸Šè®­ç»ƒã€‚
    - æ¯æ¬¡åˆ†è£‚èŠ‚ç‚¹æ—¶ï¼Œéšæœºé€‰æ‹©éƒ¨åˆ†ç‰¹å¾è¿›è¡Œåˆ’åˆ†ã€‚

3. **é›†æˆï¼š**
    - æ‰€æœ‰æ ‘é¢„æµ‹ç»“æœå–å¹³å‡å€¼ï¼Œä½œä¸ºæœ€ç»ˆå›å½’è¾“å‡ºã€‚

##### âœ¨ Pythonç¤ºä¾‹

```python
from sklearn.ensemble import RandomForestRegressor
import numpy as np
import matplotlib.pyplot as plt

# åˆ›å»ºæ•°æ®
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 0.5 * (0.5 - np.random.rand(16))  # æ·»åŠ å™ªå£°

# åˆ›å»ºæ¨¡å‹
rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    random_state=42
)

# æ‹Ÿåˆ
rf.fit(X, y)

# é¢„æµ‹
X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = rf.predict(X_test)

# ç»˜å›¾
plt.scatter(X, y, color="darkorange", label="data")
plt.plot(X_test, y_pred, color="navy", label="Random Forest Prediction")
plt.xlabel("X")
plt.ylabel("y")
plt.title("Random Forest Regression")
plt.legend()
plt.show()
```

##### ğŸ“Š ä½¿ç”¨åœºæ™¯

âœ… éçº¿æ€§å…³ç³»å»ºæ¨¡
âœ… éœ€è¦é«˜ç²¾åº¦é¢„æµ‹
âœ… å°åˆ°ä¸­å‹æ•°æ®é›†ï¼ˆå¤§æ•°æ®ä¹Ÿå¯æ‰©å±•ï¼‰

ç¤ºä¾‹ï¼š

+ æˆ¿ä»·é¢„æµ‹
+ å•†å“éœ€æ±‚é¢„æµ‹
+ é£é™©è¯„åˆ†
+ å¤æ‚æ—¶é—´åºåˆ—å»ºæ¨¡

##### âš–ï¸ ä¼˜ç¼ºç‚¹

###### âœ… ä¼˜ç‚¹

+ æ˜“ç”¨ï¼Œå°‘é‡è°ƒå‚å³å¯
+ é²æ£’æ€§å¼ºï¼Œä¸æ˜“è¿‡æ‹Ÿåˆ
+ å¯ä»¥å¤„ç†é«˜ç»´æ•°æ®
+ å¯ä¼°è®¡ç‰¹å¾é‡è¦æ€§

###### âŒ ç¼ºç‚¹

+ è®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦è¾ƒæ…¢ï¼ˆç›¸æ¯”å•æ£µæ ‘ï¼‰
+ æ¨¡å‹è¾ƒå¤§ï¼Œå­˜å‚¨å ç”¨é«˜
+ ä¸æ˜“è§£é‡Šï¼ˆæ¯æ£µæ ‘çš„è§„åˆ™å¤æ‚ï¼‰

###### ğŸ“ å¸¸ç”¨å‚æ•°

+ n_estimators: æ ‘çš„æ•°é‡ï¼ˆé»˜è®¤100ï¼‰
+ max_depth: æ ‘çš„æœ€å¤§æ·±åº¦
+ min_samples_split: å†…éƒ¨åˆ†è£‚æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
+ max_features: åˆ†è£‚æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°é‡

#### XGBoost

##### ğŸŒŸ æ¦‚å¿µ

**XGBoost**ï¼ˆExtreme Gradient Boostingï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„**æ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boosting Treeï¼‰**åº“ã€‚  
å®ƒçš„æ ¸å¿ƒæ€æƒ³ï¼š  
> é€šè¿‡è¿­ä»£æ„å»ºå¤šæ£µæ ‘ï¼Œæ¯æ£µæ ‘éƒ½æ‹Ÿåˆä¸Šä¸€æ¬¡é¢„æµ‹çš„æ®‹å·®ï¼Œä¸æ–­æé«˜æ¨¡å‹æ€§èƒ½ã€‚

ç›¸æ¯”ä¼ ç»ŸGBDTï¼ŒXGBooståœ¨ï¼š

âœ… è®­ç»ƒé€Ÿåº¦
âœ… ç²¾åº¦
âœ… è¿‡æ‹Ÿåˆæ§åˆ¶
âœ… å¹¶è¡ŒåŒ–
ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚


##### ğŸ§  ç®—æ³•è¦ç‚¹

- **Boosting**
    - å¤šæ£µæ ‘æŒ‰é¡ºåºå»ºç«‹ï¼Œåä¸€æ£µæ ‘çº æ­£å‰ä¸€æ£µæ ‘çš„è¯¯å·®ã€‚
- **äºŒé˜¶æ¢¯åº¦ä¼˜åŒ–**
    - åˆ©ç”¨æŸå¤±å‡½æ•°ä¸€é˜¶ã€äºŒé˜¶å¯¼æ•°æé«˜ä¼˜åŒ–ç²¾åº¦ã€‚
- **Shrinkage**
    - å­¦ä¹ ç‡ç¼©å°æ¯æ£µæ ‘è´¡çŒ®ã€‚
- **åˆ—é‡‡æ ·**
    - éšæœºæŠ½å–éƒ¨åˆ†ç‰¹å¾å¢å¼ºå¤šæ ·æ€§ã€‚
- **æ­£åˆ™åŒ–**
    - æ ‘å¤æ‚åº¦æƒ©ç½šé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

##### âœ¨ Pythonç¤ºä¾‹

```python
import xgboost as xgb
from sklearn.metrics import mean_squared_error
import numpy as np

X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 0.5 * (0.5 - np.random.rand(16))

model = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

model.fit(X, y)

X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = model.predict(X_test)

mse = mean_squared_error(np.sin(X_test).ravel(), y_pred)
print("MSE:", mse)
```

##### ğŸ“Š ä½¿ç”¨åœºæ™¯

âœ… ç‰¹å¾å¤šã€éçº¿æ€§å…³ç³»å¤æ‚
âœ… å¤§è§„æ¨¡æ•°æ®é›†
âœ… é«˜ç²¾åº¦é¢„æµ‹éœ€æ±‚

ç¤ºä¾‹ï¼š

+ æˆ¿ä»·é¢„æµ‹
+ é”€å”®é¢„æµ‹
+ é‡‘èé£é™©è¯„ä¼°
+ ç”¨æˆ·è¯„åˆ†é¢„æµ‹
+ æ—¶é—´åºåˆ—å»ºæ¨¡

##### âš–ï¸ ä¼˜ç¼ºç‚¹

###### âœ… ä¼˜ç‚¹

+ é«˜ç²¾åº¦
+ æŠ—è¿‡æ‹Ÿåˆ
+ æ”¯æŒå¹¶è¡Œè®¡ç®—
+ çµæ´»æ€§å¼º

###### âŒ ç¼ºç‚¹

+ å‚æ•°å¤šï¼Œéœ€è¦è°ƒä¼˜
+ å†…å­˜å ç”¨é«˜
+ è§£é‡Šæ€§å·®

###### ğŸ“ å¸¸ç”¨å‚æ•°

+ n_estimators: æ ‘æ•°é‡
+ learning_rate: å­¦ä¹ ç‡
+ max_depth: æ ‘æœ€å¤§æ·±åº¦
+ subsample: æ ·æœ¬é‡‡æ ·æ¯”
+ colsample_bytree: ç‰¹å¾é‡‡æ ·æ¯”
+ reg_alpha: L1æ­£åˆ™
+ reg_lambda: L2æ­£åˆ™

#### LightGBM

**LightGBMï¼ˆLight Gradient Boosting Machineï¼‰**æ˜¯ä¸€ä¸ªç”±å¾®è½¯å¼€å‘çš„é«˜æ€§èƒ½æ¢¯åº¦æå‡æ¡†æ¶ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å’Œé«˜ç»´ç¨€ç–æ•°æ®è€Œè®¾è®¡ã€‚

ä¸ä¼ ç»ŸGBDTç›¸æ¯”ï¼š

âœ… æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦
âœ… æ›´å°‘çš„å†…å­˜å ç”¨
âœ… æ›´å¥½çš„åˆ†å¸ƒå¼æ”¯æŒ
âœ… é«˜å‡†ç¡®æ€§

##### ğŸ§  ç®—æ³•äº®ç‚¹

1ï¸âƒ£ **Histogram-basedåˆ†ç®±**

- å°†è¿ç»­ç‰¹å¾ç¦»æ•£åŒ–æˆæœ‰é™ä¸ªbinï¼ˆç›´æ–¹å›¾ï¼‰ï¼Œå‡å°‘å†…å­˜å’Œè®¡ç®—é‡ã€‚

2ï¸âƒ£ **Leaf-wiseç”Ÿé•¿ç­–ç•¥**

- æ¯æ¬¡é€‰æ‹©å¢ç›Šæœ€å¤§çš„å¶å­åˆ†è£‚ï¼Œæ‹Ÿåˆèƒ½åŠ›å¼ºã€‚

3ï¸âƒ£ **æ”¯æŒç±»åˆ«ç‰¹å¾**

- å¯ç›´æ¥å¤„ç†åˆ†ç±»å˜é‡ã€‚

4ï¸âƒ£ **é«˜æ•ˆå¹¶è¡Œ**

- ç‰¹å¾å¹¶è¡Œã€æ•°æ®å¹¶è¡Œã€Votingå¹¶è¡Œã€‚

##### âœ¨ Pythonç¤ºä¾‹

```python
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import numpy as np

X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 0.5 * (0.5 - np.random.rand(16))

model = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    num_leaves=31,
    random_state=42
)

model.fit(X, y)

X_test = np.linspace(0, 5, 100).reshape(-1, 1)
y_pred = model.predict(X_test)

mse = mean_squared_error(np.sin(X_test).ravel(), y_pred)
print("MSE:", mse)
```

##### ğŸ“Š ä½¿ç”¨åœºæ™¯

âœ… å¤§è§„æ¨¡æ•°æ®é›†
âœ… é«˜ç»´ç¨€ç–ç‰¹å¾
âœ… é«˜æ€§èƒ½ç”Ÿäº§ç¯å¢ƒ
âœ… å¤æ‚éçº¿æ€§å…³ç³»å»ºæ¨¡

ç¤ºä¾‹ï¼š

+ é”€å”®é¢„æµ‹
+ å¹¿å‘Šç‚¹å‡»ç‡é¢„æµ‹
+ é£é™©è¯„ä¼°
+ Kaggleç«èµ›

##### âš–ï¸ ä¼˜ç¼ºç‚¹

######  âœ… ä¼˜ç‚¹

+ è®­ç»ƒé€Ÿåº¦å¿«
+ å†…å­˜å ç”¨ä½
+ è‡ªåŠ¨æ”¯æŒç±»åˆ«ç‰¹å¾
+ é«˜ç²¾åº¦
+ åˆ†å¸ƒå¼è®­ç»ƒ

######  âŒ ç¼ºç‚¹

+ æ›´å®¹æ˜“è¿‡æ‹Ÿåˆ
+ å‚æ•°ç†è§£æˆæœ¬è¾ƒé«˜
+ å°æ•°æ®é›†ä¼˜åŠ¿ä¸æ˜æ˜¾

----

## åˆ†ç±»æ¨¡å‹

----

### é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰

#### ğŸŒŸ æ¦‚å¿µ

é€»è¾‘å›å½’æ˜¯ä¸€ç§**åˆ†ç±»æ¨¡å‹**ï¼ˆä¸æ˜¯å›å½’æ¨¡å‹ï¼‰ã€‚  
é€šè¿‡å­¦ä¹ çº¿æ€§å‡½æ•°ï¼Œå°†æ ·æœ¬æ˜ å°„åˆ°æ¦‚ç‡åŒºé—´ [0,1]ï¼Œå†æ ¹æ®é˜ˆå€¼è¿›è¡Œåˆ†ç±»ã€‚

é€šä¿—ç†è§£ï¼š  
> ç”¨ä¸€æ¡çº¿æŠŠä¸åŒç±»åˆ«åˆ†å¼€ï¼ŒåŒæ—¶æŠŠç»“æœæ˜ å°„æˆæ¦‚ç‡ã€‚

é€»è¾‘å›å½’å¸¸ç”¨äºï¼š  
âœ… äºŒåˆ†ç±»ï¼ˆæœ€ç»å…¸ï¼‰  
âœ… å¤šåˆ†ç±»ï¼ˆSoftmaxæ‰©å±•ï¼‰

#### âœï¸ æ•°å­¦è¡¨è¾¾å¼

å¯¹äºè¾“å…¥ç‰¹å¾ \(x\) å’Œå‚æ•° \(\beta\)ï¼š  

1. **çº¿æ€§éƒ¨åˆ†ï¼š**
   \[
   z = \beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n
   \]

2. **Sigmoidå‡½æ•°ï¼š**
   \[
   \sigma(z) = \frac{1}{1 + e^{-z}}
   \]
   è¾“å‡ºæ¦‚ç‡ \(\in (0,1)\)

3. **é¢„æµ‹åˆ†ç±»ï¼š**
   \[
   \hat{y} =
   \begin{cases}
   1 & \text{if } \sigma(z) \geq 0.5 \\
   0 & \text{if } \sigma(z) < 0.5
   \end{cases}
   \]

#### ğŸ§  ç®—æ³•æµç¨‹

1. åˆå§‹åŒ–æƒé‡
2. æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ä¼˜åŒ–äº¤å‰ç†µæŸå¤±ï¼š
   \[
   Loss = - \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i) \right]
   \]
3. è¿­ä»£æ›´æ–°æƒé‡ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™ï¼‰

#### âœ¨ Pythonç¤ºä¾‹

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# ç”Ÿæˆæ ·æœ¬
X, y = make_classification(
    n_samples=200,
    n_features=4,
    n_informative=2,
    n_redundant=0,
    random_state=42
)

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºæ¨¡å‹
model = LogisticRegression()

# æ‹Ÿåˆ
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¾“å‡ºç»“æœ
print(classification_report(y_test, y_pred))
```

####  ğŸ“Š ä½¿ç”¨åœºæ™¯

âœ… äºŒåˆ†ç±»é—®é¢˜
âœ… ç‰¹å¾ä¸ç›®æ ‡å˜é‡è¿‘ä¼¼çº¿æ€§å…³ç³»
âœ… æ¦‚ç‡è¾“å‡ºéœ€æ±‚

ç¤ºä¾‹åº”ç”¨ï¼š

+ ä¿¡ç”¨é£é™©åˆ¤æ–­
+ å¹¿å‘Šç‚¹å‡»é¢„æµ‹
+ ç–¾ç—…é¢„æµ‹

####  âš–ï¸ ä¼˜ç¼ºç‚¹

##### âœ… ä¼˜ç‚¹

+ ç†è®ºæˆç†Ÿã€å¯è§£é‡Šæ€§å¼º
+ æ”¶æ•›é€Ÿåº¦å¿«
+ æ¦‚ç‡è¾“å‡ºç›´è§‚
+ å°æ•°æ®é›†è¡¨ç°è‰¯å¥½

#####  âŒ ç¼ºç‚¹

+ åªèƒ½å»ºæ¨¡çº¿æ€§è¾¹ç•Œ
+ å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
+ éœ€è¦ç‰¹å¾ç¼©æ”¾å’Œé¢„å¤„ç†

### å†³ç­–æ ‘åˆ†ç±»ï¼ˆDecisionTreeClassifierï¼‰


### KNN åˆ†ç±»


### æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰


### æœ´ç´ è´å¶æ–¯ï¼ˆNaive Bayesï¼‰


### é›†æˆæ–¹æ³•ï¼šéšæœºæ£®æ—ã€æ¢¯åº¦æå‡æ ‘ã€CatBoost


