# ç¬¬ä¸‰ç« ï¼šç›‘ç£å­¦ä¹ 

## ä¸€ã€çº¿æ€§æ¨¡å‹

### 1.1 çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰

çº¿æ€§å›å½’æ˜¯ä¸€ç§æœ€åŸºç¡€ä½†éå¸¸é‡è¦çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä¸»è¦ç”¨äºé¢„æµ‹è¿ç»­å€¼ã€‚

#### 1. ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’ï¼Ÿ

çº¿æ€§å›å½’çš„ç›®æ ‡æ˜¯ï¼š
> æ‰¾åˆ°ä¸€æ¡æœ€ä½³çš„ç›´çº¿ï¼Œæ‹Ÿåˆä¸€ç»„ç‚¹çš„è¶‹åŠ¿ï¼Œä»è€Œè¿›è¡Œé¢„æµ‹ã€‚

**æ¯”å¦‚ï¼š**

+ æ ¹æ®æˆ¿å­çš„é¢ç§¯é¢„æµ‹æˆ¿ä»·
+ æ ¹æ®å­¦ç”Ÿçš„å­¦ä¹ æ—¶é—´é¢„æµ‹è€ƒè¯•æˆç»©

è¿™å°±æ˜¯çº¿æ€§å›å½’çš„ç»å…¸åº”ç”¨ã€‚

#### 2. ä¸€å…ƒçº¿æ€§å›å½’å…¬å¼

å‡è®¾ä½ æœ‰ä¸€ä¸ªå˜é‡ xï¼Œæˆ‘ä»¬æƒ³é¢„æµ‹ yï¼Œé‚£ä¹ˆæ¨¡å‹å¦‚ä¸‹ï¼š

$y = wx + b$

+ wï¼šæ–œç‡ï¼ˆweightï¼‰
+ bï¼šæˆªè·ï¼ˆbiasï¼‰
+ è¿™å°±æ˜¯æˆ‘ä»¬è¦å­¦çš„å‚æ•°

####  3. ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°æœ€åˆé€‚çš„ w å’Œ bï¼Œä½¿å¾—ï¼š

+ é¢„æµ‹å€¼ $\hat{y}$ å°½å¯èƒ½æ¥è¿‘çœŸå®å€¼ y
+ æŸå¤±å‡½æ•°ï¼ˆè¯¯å·®ï¼‰æœ€å°

å¸¸ç”¨çš„æŸå¤±å‡½æ•°æ˜¯ **å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰**ï¼š

$\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$

####  4. æ¨¡å‹è®­ç»ƒï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰** æ¥ä¸æ–­è°ƒæ•´ w å’Œ bï¼Œç›´åˆ° MSE æœ€å°ã€‚

#### 5. Python å®æˆ˜æ¼”ç¤ºï¼ˆsklearnï¼‰

```python
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# æ¨¡æ‹Ÿæ•°æ®ï¼šæˆ¿å±‹é¢ç§¯ vs æˆ¿ä»·
X = np.array([[50], [60], [80], [100], [120]])  # é¢ç§¯
y = np.array([150, 180, 240, 280, 310])        # æˆ¿ä»·ï¼ˆå•ä½ï¼šä¸‡ï¼‰

# åˆ›å»ºæ¨¡å‹å¹¶è®­ç»ƒ
model = LinearRegression()
model.fit(X, y)

# é¢„æµ‹
X_test = np.array([[90]])
y_pred = model.predict(X_test)
print("é¢„æµ‹ 90 å¹³ç±³æˆ¿å­çš„ä»·æ ¼ï¼š", y_pred[0], "ä¸‡")

# å¯è§†åŒ–
plt.scatter(X, y, color='blue', label='çœŸå®æ•°æ®')
plt.plot(X, model.predict(X), color='red', label='æ‹Ÿåˆçº¿')
plt.xlabel('é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰')
plt.ylabel('ä»·æ ¼ï¼ˆä¸‡ï¼‰')
plt.title('çº¿æ€§å›å½’ï¼šé¢ç§¯ vs ä»·æ ¼')
plt.legend()
plt.show()
```

####  6. å¤šå…ƒçº¿æ€§å›å½’

å½“æˆ‘ä»¬æœ‰å¤šä¸ªå˜é‡ï¼ˆä¾‹å¦‚é¢ç§¯ã€å§å®¤æ•°é‡ã€æ¥¼å±‚ç­‰ï¼‰ï¼Œæ¨¡å‹å˜æˆï¼š

$y = w_1x_1 + w_2x_2 + \dots + w_nx_n + b$

Python è®­ç»ƒæ–¹å¼å®Œå…¨ç›¸åŒï¼Œåªéœ€æä¾›å¤šä¸ªç‰¹å¾å³å¯ã€‚

####  7. çº¿æ€§å›å½’çš„ä¼˜ç¼ºç‚¹

##### ä¼˜ç‚¹ï¼š

+ ç®€å•ç›´è§‚ï¼Œæ˜“äºç†è§£
+ è®­ç»ƒé€Ÿåº¦å¿«
+ å¯è§£é‡Šæ€§å¼ºï¼ˆæ¯ä¸ªå˜é‡çš„æƒé‡å¯è§£é‡Šï¼‰

##### ç¼ºç‚¹ï¼š

+ åªèƒ½å¤„ç†çº¿æ€§å…³ç³»
+ å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
+ ä¸èƒ½å¤„ç†å¤šå³°æˆ–éçº¿æ€§æ•°æ®

#### 8. å¸¸è§å˜ç§

| æ¨¡å‹                         | ç‰¹ç‚¹                                   |
|------------------------------|----------------------------------------|
| å²­å›å½’ï¼ˆRidgeï¼‰              | åŠ å…¥ L2 æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ             |
| Lasso å›å½’                   | åŠ å…¥ L1 æ­£åˆ™åŒ–ï¼Œå¯ç”¨äºç‰¹å¾é€‰æ‹©         |
| å¤šé¡¹å¼å›å½’                   | å¯æ‹Ÿåˆæ›²çº¿ï¼ˆéçº¿æ€§ï¼‰                   |
| é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰ | è™½å«â€œå›å½’â€ï¼Œå…¶å®æ˜¯åˆ†ç±»ç®—æ³•             |


----

### 1.2 é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰

#### 1. ä»€ä¹ˆæ˜¯é€»è¾‘å›å½’ï¼Ÿ

é€»è¾‘å›å½’æ˜¯ä¸€ç§**ç”¨äºäºŒåˆ†ç±»é—®é¢˜çš„ç›‘ç£å­¦ä¹ ç®—æ³•**ï¼Œå°½ç®¡åå­—ä¸­æœ‰â€œå›å½’â€ï¼Œä½†å®ƒå…¶å®æ˜¯ç”¨äº**åˆ†ç±»ä»»åŠ¡**çš„ã€‚

- è¾“å…¥ï¼šä¸€ç»„ç‰¹å¾  
- è¾“å‡ºï¼šæŸä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚ï¼š0 æˆ– 1ï¼‰  
- ç›®æ ‡ï¼šå­¦ä¹ ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥ç‰¹å¾åé¢„æµ‹æ ·æœ¬å±äºç±»åˆ« 1 çš„æ¦‚ç‡  

#### 2. åº”ç”¨åœºæ™¯

| åº”ç”¨é¢†åŸŸ     | ç¤ºä¾‹                  |
|--------------|-----------------------|
| åŒ»ç–—è¯Šæ–­     | åˆ¤æ–­è‚¿ç˜¤æ˜¯è‰¯æ€§æˆ–æ¶æ€§ |
| é‡‘èé£æ§     | åˆ¤æ–­æ˜¯å¦å¯èƒ½è¿çº¦     |
| å¸‚åœºè¥é”€     | åˆ¤æ–­ç”¨æˆ·æ˜¯å¦ä¼šç‚¹å‡»å¹¿å‘Š |
| ç¤¾ä¼šè°ƒæŸ¥åˆ†æ | åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æ”¯æŒæŸè§‚ç‚¹ |

#### 3. æ¨¡å‹å…¬å¼ä¸åŸç†

##### âœ… çº¿æ€§éƒ¨åˆ†

é€»è¾‘å›å½’é¦–å…ˆä½¿ç”¨ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼š

$$
z = w^\top x + b
$$

å…¶ä¸­ï¼š

- \( x \)ï¼šç‰¹å¾å‘é‡  
- \( w \)ï¼šæƒé‡å‘é‡  
- \( b \)ï¼šåç½®é¡¹  

##### âœ… Sigmoid å‡½æ•°

ä¸ºäº†æŠŠ \( z \) æ˜ å°„åˆ° [0, 1] èŒƒå›´ï¼Œæˆ‘ä»¬ä½¿ç”¨ **Sigmoid** å‡½æ•°ï¼š

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

è¾“å‡ºå°±æ˜¯å±äºç±»åˆ« 1 çš„æ¦‚ç‡ \( P(y=1|x) \)ã€‚

##### âœ… åˆ†ç±»åˆ¤å®šè§„åˆ™

$$
\hat{y} = \begin{cases}
1, & \text{if } \sigma(z) \geq 0.5 \\
0, & \text{otherwise}
\end{cases}
$$

#### 4. æŸå¤±å‡½æ•°ï¼ˆå¯¹æ•°æŸå¤±ï¼‰

é€»è¾‘å›å½’ä½¿ç”¨ **å¯¹æ•°æŸå¤±å‡½æ•°ï¼ˆLog Lossï¼‰**ï¼š

$$
L(y, \hat{y}) = -y \log(\hat{y}) - (1 - y) \log(1 - \hat{y})
$$

ç›®æ ‡æ˜¯**æœ€å°åŒ–æ‰€æœ‰æ ·æœ¬çš„å¹³å‡æŸå¤±**ã€‚

#### 5. æ¨¡å‹è®­ç»ƒè¿‡ç¨‹

1. åˆå§‹åŒ–æƒé‡ \( w \) å’Œåç½® \( b \)
2. å¯¹æ¯ä¸€è½®è®­ç»ƒï¼ˆepochï¼‰ï¼š
   - è®¡ç®—é¢„æµ‹å€¼ \( \hat{y} \)
   - è®¡ç®—æŸå¤±å‡½æ•°
   - ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ï¼š
     $$
     w := w - \eta \cdot \nabla_w L, \quad b := b - \eta \cdot \nabla_b L
     $$
   - ç›´åˆ°æŸå¤±æ”¶æ•›æˆ–è¾¾åˆ°è¿­ä»£ä¸Šé™

#### 6. Python å®ç°ï¼ˆä½¿ç”¨ scikit-learnï¼‰

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# åŠ è½½æ•°æ®ï¼šä½¿ç”¨ Iris æ•°æ®é›†ä¸­ä¸¤ä¸ªç±»åˆ«
iris = load_iris()
X = iris.data[iris.target != 2]
y = iris.target[iris.target != 2]

# åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# æ¨¡å‹è®­ç»ƒ
model = LogisticRegression()
model.fit(X_train, y_train)

# æ¨¡å‹é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°å‡†ç¡®ç‡
from sklearn.metrics import accuracy_score
print("Accuracy:", accuracy_score(y_test, y_pred))
```

#### 7.  ä¼˜ç‚¹ä¸ç¼ºç‚¹

| ä¼˜ç‚¹                         | ç¼ºç‚¹                     |
|------------------------------|--------------------------|
| ç®€å•é«˜æ•ˆï¼Œé€‚ç”¨äºçº¿æ€§é—®é¢˜     | å¯¹éçº¿æ€§é—®é¢˜è¡¨ç°è¾ƒå·®     |
| æ¦‚ç‡è¾“å‡ºï¼Œæ¨¡å‹å¯è§£é‡Šæ€§å¼º     | å®¹æ˜“æ¬ æ‹Ÿåˆ               |
| è®­ç»ƒé€Ÿåº¦å¿«ï¼Œé€‚åˆå¤§æ ·æœ¬       | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ             |

----

### 1.3 å²­å›å½’ï¼ˆRidge Regressionï¼‰å’Œ Lassoå›å½’

#### 1. ä¸ºä»€ä¹ˆéœ€è¦æ­£åˆ™åŒ–ï¼Ÿ

åœ¨ä½¿ç”¨æ™®é€šçº¿æ€§å›å½’æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š

+ ç‰¹å¾æ•°é‡å¤šï¼ˆé«˜ç»´ï¼‰
+ ç‰¹å¾ä¹‹é—´å­˜åœ¨å…±çº¿æ€§ï¼ˆMulticollinearityï¼‰
+ æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†**æ­£åˆ™åŒ–æ–¹æ³•**ï¼š

+ Ridge Regressionï¼ˆå²­å›å½’ï¼‰ï¼šL2 æ­£åˆ™åŒ–
+ Lasso Regressionï¼šL1 æ­£åˆ™åŒ–

#### 2.  å²­å›å½’ï¼ˆRidge Regressionï¼‰

åœ¨æ™®é€šæœ€å°äºŒä¹˜çš„æŸå¤±å‡½æ•°åŸºç¡€ä¸Šï¼ŒåŠ ä¸Šäº† L2 æ­£åˆ™é¡¹ï¼š

$$
\text{Loss}{\text{Ridge}} = \sum{i=1}^n (y_i - \hat{y}i)^2 + \lambda \sum{j=1}^p w_j^2
$$

å…¶ä¸­ï¼š

+ $\lambda$ï¼šæ­£åˆ™åŒ–å¼ºåº¦ï¼ˆè¶…å‚æ•°ï¼‰
+ $w_j$ï¼šå›å½’ç³»æ•°

**ç‰¹ç‚¹**

+ ç¼©å°ç³»æ•°ï¼Œä½†ä¸ä¼šä½¿å…¶å˜ä¸º 0
+ æ›´é€‚ç”¨äºæ‰€æœ‰ç‰¹å¾éƒ½æœ‰è´¡çŒ®çš„æƒ…å†µ

#### 3. Lasso å›å½’ï¼ˆLasso Regressionï¼‰

Lasso åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥äº† L1 æ­£åˆ™é¡¹ï¼š

$$
\text{Loss}{\text{Lasso}} = \sum{i=1}^n (y_i - \hat{y}i)^2 + \lambda \sum{j=1}^p |w_j|
$$

**ç‰¹ç‚¹**

+ å¯ä»¥è®©æŸäº›ç³»æ•°å˜ä¸º 0ï¼ˆå˜é‡é€‰æ‹©ï¼‰
+ æ›´é€‚ç”¨äºç‰¹å¾å¤šã€ä½†éƒ¨åˆ†ç‰¹å¾ä¸é‡è¦çš„åœºæ™¯

#### 4.  Ridge vs Lasso å¯¹æ¯”

| å¯¹æ¯”ç‚¹       | Ridge å›å½’             | Lasso å›å½’              |
|--------------|-------------------------|--------------------------|
| æ­£åˆ™åŒ–ç±»å‹   | L2 æ­£åˆ™åŒ–               | L1 æ­£åˆ™åŒ–                |
| æ˜¯å¦ç¨€ç–     | å¦                      | æ˜¯ï¼ˆå¯è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼‰     |
| ç³»æ•°ä¸º 0     | ä¸å¯èƒ½                  | å¯èƒ½                     |
| åº”ç”¨åœºæ™¯     | æ‰€æœ‰ç‰¹å¾éƒ½ç›¸å…³          | éƒ¨åˆ†ç‰¹å¾ç›¸å…³             |

#### 5. Python å®ç°ï¼ˆscikit-learnï¼‰

###### ğŸ“Œ å²­å›å½’ç¤ºä¾‹

```python
from sklearn.linear_model import Ridge
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# åŠ è½½æ•°æ®
X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y)

# å»ºç«‹æ¨¡å‹
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
print("Ridge score:", ridge.score(X_test, y_test))
```

###### ğŸ“Œ Lasso å›å½’ç¤ºä¾‹

```python
from sklearn.linear_model import Lasso

# å»ºç«‹æ¨¡å‹
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
print("Lasso score:", lasso.score(X_test, y_test))
```

#### 6. æ­£åˆ™åŒ–å‚æ•° $\alpha$ çš„é€‰æ‹©

+ $\alpha$ è¶Šå¤§ â†’ æ­£åˆ™åŒ–è¶Šå¼º â†’ æ¨¡å‹æ›´ç®€å•ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ
+ $\alpha$ è¶Šå° â†’ æ­£åˆ™åŒ–è¶Šå¼± â†’ æ¨¡å‹æ›´å¤æ‚ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ

å¯ä»¥ä½¿ç”¨ GridSearchCV æˆ– RidgeCVã€LassoCV è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°ã€‚


## äºŒã€å†³ç­–æ ‘ä¸é›†æˆæ–¹æ³•

### 2.1 å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰

#### 1.  ä»€ä¹ˆæ˜¯å†³ç­–æ ‘ï¼Ÿ

å†³ç­–æ ‘æ˜¯ä¸€ç§æ ‘çŠ¶ç»“æ„çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¸¸ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒé€šè¿‡ä¸€ç³»åˆ—**é—®é¢˜åˆ¤æ–­**å°†æ ·æœ¬åˆ’åˆ†ä¸ºä¸åŒç±»åˆ«ï¼Œç»“æ„ç±»ä¼¼äºâ€œäºŒåä¸ªé—®é¢˜â€ã€‚

```text
        æ˜¯å¦ä¸‹é›¨ï¼Ÿ
         /    \
       æ˜¯      å¦
      /          \
  æ˜¯å¦å¸¦ä¼ï¼Ÿ     æ˜¯å¦æˆ´å¢¨é•œï¼Ÿ
```

#### 2. å†³ç­–æ ‘çš„ç»„æˆç»“æ„

| å…ƒç´          | è¯´æ˜                            |
|--------------|---------------------------------|
| æ ¹èŠ‚ç‚¹       | æ ‘çš„èµ·ç‚¹ï¼Œå¯¹æ•´ä¸ªæ•°æ®è¿›è¡Œç¬¬ä¸€æ¬¡åˆ’åˆ† |
| å†…éƒ¨èŠ‚ç‚¹     | æ¯ä¸ªåˆ¤æ–­æ¡ä»¶æ‰€åœ¨çš„ä½ç½®            |
| å¶å­èŠ‚ç‚¹     | æœ€ç»ˆå†³ç­–ç»“æœï¼ˆåˆ†ç±»æˆ–æ•°å€¼ï¼‰        |
| åˆ†æ”¯         | ä»ä¸€ä¸ªèŠ‚ç‚¹åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹çš„è·¯å¾„      |


#### 3. å†³ç­–æ ‘çš„æ„å»ºæµç¨‹

1.	é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜ç‰¹å¾è¿›è¡Œåˆ’åˆ†
2. æ ¹æ®è¯¥ç‰¹å¾å°†æ•°æ®é›†åˆ’åˆ†æˆå­é›†
3.	å¯¹æ¯ä¸ªå­é›†é‡å¤æ­¥éª¤ 1 å’Œ 2ï¼Œç›´åˆ°æ»¡è¶³ç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚æ ‘æ·±åº¦æˆ–æ ·æœ¬æ•°ï¼‰

#### 4.  ç‰¹å¾é€‰æ‹©æ ‡å‡†ï¼ˆæ ¸å¿ƒï¼‰

##### ğŸ”¹ ä¿¡æ¯å¢ç›Šï¼ˆID3ï¼‰

+ è¡¡é‡åˆ’åˆ†åä¿¡æ¯çš„ä¸ç¡®å®šæ€§é™ä½ç¨‹åº¦
+ ä½¿ç”¨ç†µï¼ˆEntropyï¼‰è¿›è¡Œè¡¡é‡ï¼š

$$
Gain(D, A) = Entropy(D) - \sum_{v=1}^V \frac{|D_v|}{|D|} Entropy(D_v)
$$

##### ğŸ”¹ åŸºå°¼æŒ‡æ•°ï¼ˆCARTï¼‰

+ è¡¡é‡æ•°æ®çš„çº¯åº¦ï¼Œå€¼è¶Šå°çº¯åº¦è¶Šé«˜ï¼š

$$
Gini(D) = 1 - \sum_{k=1}^K p_k^2
$$

##### ğŸ”¹ ä¿¡æ¯å¢ç›Šç‡ï¼ˆC4.5ï¼‰

+ ä¿¡æ¯å¢ç›Šå½’ä¸€åŒ–åï¼Œæ›´é€‚åˆå¤šå€¼ç‰¹å¾ã€‚

#### 5. å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹

| ä¼˜ç‚¹                           | ç¼ºç‚¹                           |
|--------------------------------|--------------------------------|
| å¯è§†åŒ–ã€è§£é‡Šæ€§å¼º              | å®¹æ˜“è¿‡æ‹Ÿåˆ                    |
| å¯¹éçº¿æ€§å…³ç³»å»ºæ¨¡èƒ½åŠ›å¼º        | å¯¹å™ªå£°å’Œå°æ ·æœ¬æ•æ„Ÿ            |
| å¤„ç†ç¼ºå¤±å€¼å’Œä¸éœ€è¦å½’ä¸€åŒ–      | æ ‘ç»“æ„å¯èƒ½ä¸ç¨³å®š               |
| å¯å¤„ç†æ•°å€¼å’Œç±»åˆ«å‹å˜é‡        | æ€§èƒ½ä¸å¦‚é›†æˆæ–¹æ³•ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰ |

#### 6. Python å®ç°ï¼ˆsklearnï¼‰

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®
X, y = load_iris(return_X_y=True)

# å»ºç«‹æ¨¡å‹
clf = DecisionTreeClassifier(criterion="gini", max_depth=3)
clf.fit(X, y)

# å¯è§†åŒ–
plt.figure(figsize=(12,8))
tree.plot_tree(clf, feature_names=load_iris().feature_names, class_names=load_iris().target_names, filled=True)
plt.show()
```

#### 7. å†³ç­–æ ‘è¶…å‚æ•°


| å‚æ•°                 | ä½œç”¨è¯´æ˜                                   |
|----------------------|--------------------------------------------|
| `criterion`          | ç‰¹å¾é€‰æ‹©æ ‡å‡†ï¼Œå¦‚ `"gini"`ã€`"entropy"`     |
| `max_depth`          | æœ€å¤§æ ‘æ·±åº¦                                 |
| `min_samples_split`  | å†…éƒ¨èŠ‚ç‚¹å†åˆ’åˆ†æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°            |
| `min_samples_leaf`   | å¶å­èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°                  |
| `max_features`       | æ¯æ¬¡åˆ’åˆ†æ—¶è€ƒè™‘çš„æœ€å¤§ç‰¹å¾æ•°                |


### 2.2 éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰

#### 1ï¸âƒ£ ä»€ä¹ˆæ˜¯éšæœºæ£®æ—ï¼Ÿ

éšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼ˆEnsemble Learningï¼‰ï¼Œé€šè¿‡ç»„åˆå¤šä¸ªå†³ç­–æ ‘æ¥è¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚å®ƒçš„æ€æƒ³æ˜¯ **â€œä¼—äººæ‹¾æŸ´ç«ç„°é«˜â€**ï¼šå•æ£µæ ‘å¯èƒ½ä¸å‡†ï¼Œä½†ä¸€ç¾¤æ ‘çš„â€œæŠ•ç¥¨â€æ›´ç¨³å¥ã€‚

#### 2ï¸âƒ£ å·¥ä½œåŸç†ï¼ˆç®€åŒ–æµç¨‹ï¼‰

1. **æ•°æ®é‡‡æ ·ï¼ˆBootstrapï¼‰**ï¼š
    - ä»åŸå§‹æ•°æ®ä¸­æœ‰æ”¾å›åœ°é‡‡æ ·ï¼Œç”Ÿæˆå¤šä¸ªå­æ•°æ®é›†ï¼ˆæ¯æ£µæ ‘ä¸€ä¸ªï¼‰ã€‚
2. **è®­ç»ƒå¤šä¸ªå†³ç­–æ ‘**ï¼š
    - æ¯æ£µæ ‘åœ¨è®­ç»ƒæ—¶ï¼Œåœ¨æ¯ä¸ªèŠ‚ç‚¹éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†ç‰¹å¾è¿›è¡Œåˆ’åˆ†ï¼ˆç‰¹å¾å­é›†ï¼‰ã€‚
3. **é¢„æµ‹é˜¶æ®µ**ï¼š
    - åˆ†ç±»ä»»åŠ¡ï¼šå¤šæ•°æŠ•ç¥¨ã€‚
    - å›å½’ä»»åŠ¡ï¼šå–å¹³å‡å€¼ã€‚

####  3ï¸âƒ£ éšæœºæ£®æ— vs å†³ç­–æ ‘

| é¡¹ç›®             | å†³ç­–æ ‘                   | éšæœºæ£®æ—                        |
|------------------|--------------------------|---------------------------------|
| æ˜¯å¦æ˜“è¿‡æ‹Ÿåˆ     | âœ… æ˜¯                     | âŒ å‡å°‘è¿‡æ‹Ÿåˆ                   |
| æ¨¡å‹ç¨³å®šæ€§       | âŒ å¯¹æ•°æ®å˜åŒ–æ•æ„Ÿ         | âœ… ç¨³å®š                         |
| å¯è§£é‡Šæ€§         | âœ… å¼º                     | âŒ è¾ƒå¼±                         |
| è®­ç»ƒæ—¶é—´         | â±ï¸ å¿«                    | â±ï¸ æ…¢ï¼ˆè®­ç»ƒå¤šæ£µæ ‘ï¼‰            |
| å‡†ç¡®ç‡           | ä¸€èˆ¬                     | è¾ƒé«˜                            |


#### 4ï¸âƒ£ éšæœºæ€§æ¥æº

- **Baggingï¼ˆæ ·æœ¬éšæœºï¼‰**ï¼šè®­ç»ƒæ•°æ®é€šè¿‡éšæœºé‡‡æ ·ç”Ÿæˆå­é›†
- **ç‰¹å¾éšæœºæ€§**ï¼šæ¯æ¬¡åˆ†è£‚èŠ‚ç‚¹æ—¶ï¼Œåªè€ƒè™‘éƒ¨åˆ†ç‰¹å¾è€Œéå…¨éƒ¨

è¿™ä¸¤ç§â€œéšæœºæ€§â€è®©æ¯æ£µæ ‘ä¹‹é—´å·®å¼‚æ›´å¤§ï¼Œä»è€Œå‡å°‘**æ–¹å·®**ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

#### 5ï¸âƒ£ ä¼˜ç‚¹ä¸ç¼ºç‚¹


| ä¼˜ç‚¹                             | ç¼ºç‚¹                              |
|----------------------------------|-----------------------------------|
| æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º                   | æ¨¡å‹è¾ƒå¤§ï¼Œä¸æ˜“éƒ¨ç½²                |
| å¯å¤„ç†é«˜ç»´æ•°æ®å’Œç¼ºå¤±å€¼           | å¯è§£é‡Šæ€§ä¸å¦‚å•æ£µå†³ç­–æ ‘            |
| å¯¹å¼‚å¸¸å€¼é²æ£’                    | è®­ç»ƒæ—¶é—´å’Œå†…å­˜å¼€é”€è¾ƒå¤§            |
| é€‚åˆå¹¶è¡Œè®­ç»ƒ                    | å¯¹å°æ•°æ®é›†å¯èƒ½è¿‡æ‹Ÿåˆ              |

#### 6ï¸âƒ£ Python å®ç°ï¼ˆä½¿ç”¨ sklearnï¼‰

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# æ•°æ®åŠ è½½
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# æ¨¡å‹è®­ç»ƒ
model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
y_pred = model.predict(X_test)
print("å‡†ç¡®ç‡:", accuracy_score(y_test, y_pred))
```

#### 7ï¸âƒ£ å¸¸ç”¨è¶…å‚æ•°

| å‚æ•°                | è¯´æ˜                                          |
|---------------------|-----------------------------------------------|
| `n_estimators`      | æ£®æ—ä¸­æ ‘çš„æ•°é‡ï¼ˆè¶Šå¤šè¶Šç¨³å®šï¼Œè®¡ç®—é‡è¶Šå¤§ï¼‰     |
| `max_depth`         | æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ                 |
| `max_features`      | æ¯æ¬¡åˆ’åˆ†æ—¶è€ƒè™‘çš„æœ€å¤§ç‰¹å¾æ•°                   |
| `min_samples_split` | å†…éƒ¨èŠ‚ç‚¹å†åˆ’åˆ†æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°               |
| `min_samples_leaf`  | å¶å­èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°ï¼Œå¢å¤§å¯é˜²æ­¢è¿‡æ‹Ÿåˆ         |
| `bootstrap`         | æ˜¯å¦è¿›è¡Œæœ‰æ”¾å›é‡‡æ ·ï¼ˆé»˜è®¤ `True`ï¼‰            |
| `random_state`      | éšæœºç§å­ï¼Œç”¨äºç»“æœå¤ç°                       |
| `n_jobs`            | å¹¶è¡Œæ‰§è¡Œçš„çº¿ç¨‹æ•°ï¼ˆå¦‚è®¾ä¸º `-1` åˆ™ä½¿ç”¨æ‰€æœ‰CPUï¼‰ |
| `criterion`         | åˆ†è£‚èŠ‚ç‚¹çš„æ ‡å‡†ï¼ˆå¦‚ `"gini"` æˆ– `"entropy"`ï¼‰  |


#### 8ï¸âƒ£ åº”ç”¨åœºæ™¯

+ åŒ»ç–—è¯Šæ–­ï¼ˆé¢„æµ‹ç–¾ç—…ï¼‰
+ é‡‘èä¿¡ç”¨è¯„åˆ†
+ å®¢æˆ·æµå¤±é¢„æµ‹
+ æ–‡æœ¬åˆ†ç±»ã€åƒåœ¾é‚®ä»¶æ£€æµ‹
+ å›¾åƒè¯†åˆ«ä¸­çš„åˆçº§ç‰¹å¾åˆ†ç±»ä»»åŠ¡

### 2.3 æ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boostingï¼‰

#### 1ï¸âƒ£ ä»€ä¹ˆæ˜¯æ¢¯åº¦æå‡æ ‘ï¼Ÿ

æ¢¯åº¦æå‡æ ‘ï¼ˆGBDTï¼‰æ˜¯ä¸€ç§**é›†æˆå­¦ä¹ **æ–¹æ³•ï¼Œå±äº Boosting ç³»åˆ—ã€‚å®ƒé€šè¿‡ä¸²è¡Œåœ°è®­ç»ƒå¤šä¸ªå†³ç­–æ ‘ï¼Œæ¯æ£µæ ‘éƒ½æ‹Ÿåˆä¸Šä¸€æ¬¡é¢„æµ‹çš„**æ®‹å·®ï¼ˆè¯¯å·®ï¼‰**ï¼Œä»è€Œé€æ­¥ä¼˜åŒ–æ•´ä¸ªæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚

> ğŸ“Œ ä¸€å¥è¯æ€»ç»“ï¼šGBDT = å¤šæ£µå†³ç­–æ ‘ + æ¢¯åº¦ä¸‹é™æ€æƒ³

#### 2ï¸âƒ£ å·¥ä½œåŸç†

GBDT æ˜¯ä¸€ä¸ªé€æ­¥è¿­ä»£çš„è¿‡ç¨‹ï¼š

1. å…ˆè®­ç»ƒä¸€æ£µå†³ç­–æ ‘ `fâ‚(x)`ï¼Œé¢„æµ‹ç›®æ ‡ `y`ã€‚
2. è®¡ç®—æ®‹å·®ï¼š`râ‚ = y - fâ‚(x)`
3. å†è®­ç»ƒç¬¬äºŒæ£µæ ‘ `fâ‚‚(x)` æ¥æ‹Ÿåˆæ®‹å·® `râ‚`
4. å¾—åˆ°æ–°æ¨¡å‹ï¼š`y_pred = fâ‚(x) + fâ‚‚(x)`
5. é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæœ€ç»ˆæ¨¡å‹ä¸ºï¼š

\[
F(x) = \sum_{m=1}^{M} \gamma_m f_m(x)
\]

å…¶ä¸­æ¯ä¸€è½®é€šè¿‡è´Ÿæ¢¯åº¦æ–¹å‘ä¼˜åŒ–æŸå¤±å‡½æ•°ã€‚

#### 3ï¸âƒ£ GBDT ä¸éšæœºæ£®æ—å¯¹æ¯”

| é¡¹ç›®           | éšæœºæ£®æ—ï¼ˆRFï¼‰          | æ¢¯åº¦æå‡æ ‘ï¼ˆGBDTï¼‰             |
|----------------|--------------------------|-------------------------------|
| è®­ç»ƒæ–¹å¼       | å¹¶è¡Œè®­ç»ƒå¤šæ£µæ ‘            | ä¸²è¡Œè®­ç»ƒï¼Œæ¯æ£µæ ‘ä¾èµ–ä¸Šä¸€æ£µ     |
| é›†æˆæ–¹å¼       | å¤šæ•°æŠ•ç¥¨æˆ–å¹³å‡å€¼          | æ®‹å·®æ‹Ÿåˆ + åŠ æƒæ±‚å’Œ            |
| åå·®ä¸æ–¹å·®     | é™ä½æ–¹å·®                 | é™ä½åå·®                       |
| å¯¹å¼‚å¸¸å€¼æ•æ„Ÿæ€§ | è¾ƒä½                     | è¾ƒé«˜ï¼ˆä½¿ç”¨äº†æ®‹å·®æ‹Ÿåˆï¼‰         |
| é€Ÿåº¦           | è¾ƒå¿«                     | è¾ƒæ…¢                           |

#### 4ï¸âƒ£ æŸå¤±å‡½æ•°ä¸æ¢¯åº¦

GBDT å¯ç”¨äºå›å½’å’Œåˆ†ç±»ï¼Œå…¶æ ¸å¿ƒæ˜¯é€šè¿‡æŸå¤±å‡½æ•°çš„**æ¢¯åº¦æ–¹å‘**æ›´æ–°æ¨¡å‹ã€‚

- **å›å½’é—®é¢˜å¸¸ç”¨ï¼š** MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰
- **åˆ†ç±»é—®é¢˜å¸¸ç”¨ï¼š** å¯¹æ•°æŸå¤±ï¼ˆlog lossï¼‰

æ¯ä¸€è½®éƒ½æ˜¯å¯¹æŸå¤±å‡½æ•°å…³äºé¢„æµ‹å€¼çš„è´Ÿæ¢¯åº¦æ–¹å‘è¿›è¡Œæ‹Ÿåˆã€‚

#### 5ï¸âƒ£ Python ç¤ºä¾‹ï¼ˆä½¿ç”¨ `sklearn`ï¼‰

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# è®­ç»ƒ GBDT æ¨¡å‹
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)
model.fit(X_train, y_train)

# é¢„æµ‹ä¸è¯„ä¼°
y_pred = model.predict(X_test)
print("å‡†ç¡®ç‡:", accuracy_score(y_test, y_pred))
```

#### 6ï¸âƒ£ å¸¸ç”¨è¶…å‚æ•°

| å‚æ•°                | å«ä¹‰                                                 |
|---------------------|------------------------------------------------------|
| `n_estimators`      | å¼±å­¦ä¹ å™¨çš„æ•°é‡ï¼Œå³æ ‘çš„æ£µæ•°                           |
| `learning_rate`     | å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ¯æ£µæ ‘å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼Œè¾ƒå°å¯æå‡ç²¾åº¦  |
| `max_depth`         | æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦ï¼Œæ§åˆ¶æ¨¡å‹å¤æ‚åº¦                    |
| `min_samples_split` | å†…éƒ¨èŠ‚ç‚¹å†åˆ’åˆ†æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°                      |
| `min_samples_leaf`  | å¶å­èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°ï¼Œé¿å…è¿‡æ‹Ÿåˆ                      |
| `subsample`         | æ¯æ£µæ ‘è®­ç»ƒæ‰€ç”¨æ•°æ®çš„æ¯”ä¾‹ï¼Œ<1 å¯å¼•å…¥éšæœºæ€§é˜²è¿‡æ‹Ÿåˆ   |
| `loss`              | æŸå¤±å‡½æ•°ç±»å‹ï¼Œå¦‚ `squared_error` æˆ– `log_loss`      |
| `max_features`      | å¯»æ‰¾æœ€ä½³åˆ’åˆ†æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°é‡                         |

#### 7ï¸âƒ£ ä¼˜ç‚¹ä¸ç¼ºç‚¹

| ä¼˜ç‚¹                               | ç¼ºç‚¹                             |
|------------------------------------|----------------------------------|
| ç²¾åº¦é«˜ï¼Œæ•ˆæœé€šå¸¸ä¼˜äºå•æ£µå†³ç­–æ ‘     | è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œä¸èƒ½å¹¶è¡Œè®­ç»ƒ       |
| å¯ç”¨äºå›å½’å’Œåˆ†ç±»ä»»åŠ¡               | å¯¹å¼‚å¸¸å€¼è¾ƒæ•æ„Ÿ                   |
| å¯çµæ´»é€‰æ‹©æŸå¤±å‡½æ•°                 | å¯¹å‚æ•°è°ƒèŠ‚è¾ƒä¸ºæ•æ„Ÿï¼Œè°ƒå‚è¾ƒå¤æ‚   |
| å¯ä»¥æ•æ‰ç‰¹å¾é—´çš„éçº¿æ€§äº¤äº’å…³ç³»     | ä¸é€‚åˆå®æ—¶é¢„æµ‹ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢     |
| å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›                     | å¦‚æœæ ‘çš„æ•°é‡è¿‡å¤šå¯èƒ½è¿‡æ‹Ÿåˆ       |


## ä¸‰ã€æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

#### 1ï¸âƒ£ ä»€ä¹ˆæ˜¯ SVMï¼Ÿ

SVMï¼ˆSupport Vector Machineï¼Œæ”¯æŒå‘é‡æœºï¼‰æ˜¯ä¸€ç§**ç›‘ç£å­¦ä¹ çš„åˆ†ç±»ç®—æ³•**ã€‚

å®ƒçš„ç›®æ ‡æ˜¯ï¼š
> æ‰¾åˆ°ä¸€æ¡æœ€ä½³çš„â€œåˆ†ç•Œçº¿â€ï¼ˆä¹Ÿç§°ä¸º**è¶…å¹³é¢**ï¼‰ï¼ŒæŠŠä¸åŒç±»åˆ«çš„æ•°æ®åˆ†å¼€ï¼Œå¹¶å°½å¯èƒ½ä¿æŒè·ç¦»æœ€å¤§ï¼ˆå®‰å…¨é—´éš”æœ€å¤§ï¼‰ã€‚

#### 2ï¸âƒ£ ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ SVMï¼Ÿ

SVM ç‰¹åˆ«é€‚åˆä»¥ä¸‹åœºæ™¯ï¼š

+ æ ·æœ¬æ•°é‡ä¸å¤šï¼Œä½†ç‰¹å¾å¾ˆå¤šï¼ˆé«˜ç»´æ•°æ®ï¼‰
+ åˆ†ç±»ä»»åŠ¡è¦æ±‚é«˜å‡†ç¡®ç‡
+ æƒ³è¦æ‰¾åˆ°â€œé²æ£’æ€§å¼ºâ€çš„åˆ†ç±»è¾¹ç•Œ

#### 3ï¸âƒ£ SVM çš„å·¥ä½œåŸç†

##### çº¿æ€§å¯åˆ†æƒ…å†µï¼š

SVM æ‰¾ä¸€æ¡â€œæœ€ä½³ç›´çº¿â€ï¼ˆæˆ–è¶…å¹³é¢ï¼‰ï¼ŒæŠŠä¸¤ç±»ç‚¹ï¼ˆä¾‹å¦‚çŒ« vs ç‹—ï¼‰å®Œå…¨åˆ†å¼€ï¼ŒåŒæ—¶ï¼š

âœ… ä¿è¯ä¸¤è¾¹çš„ç‚¹ç¦»åˆ†ç•Œçº¿**å°½å¯èƒ½è¿œ**

âœ… ä¸­é—´çš„â€œå®‰å…¨å¸¦â€ï¼ˆé—´éš”ï¼‰**è¶Šå®½è¶Šå¥½**

![svm.png](../../imgs/ml/svm.png)

##### æ•°å­¦å½¢å¼

å‡è®¾æˆ‘ä»¬è¦æ‰¾çš„è¶…å¹³é¢æ˜¯ï¼š

```text
wÂ·x + b = 0
```
å…¶ä¸­ï¼š

+ w æ˜¯æ³•å‘é‡ï¼Œå†³å®šæ–¹å‘
+ b æ˜¯åç½®ï¼Œå†³å®šè·ç¦»åŸç‚¹å¤šè¿œ

åˆ†ç±»è§„åˆ™ï¼š

+ æ­£ç±»ï¼šwÂ·x + b â‰¥ +1
+ è´Ÿç±»ï¼šwÂ·x + b â‰¤ -1

ç›®æ ‡æ˜¯ï¼š**æœ€å¤§åŒ–é—´éš” = 2 / ||w||**

#### 4ï¸âƒ£ ç¡¬é—´éš” vs è½¯é—´éš”

##### ç¡¬é—´éš”ï¼ˆHard Marginï¼‰ï¼š

+ è¦æ±‚æ•°æ®å®Œå…¨å¯åˆ†
+ ä¸å…è®¸æœ‰ä»»ä½•åˆ†ç±»é”™è¯¯
+ é€‚åˆå¹²å‡€æ•°æ®ï¼Œä½†ç°å®ä¸­å¾ˆå°‘

##### è½¯é—´éš”ï¼ˆSoft Marginï¼‰ï¼š

+ å…è®¸éƒ¨åˆ†é”™è¯¯åˆ†ç±»ï¼ˆå¼•å…¥æ¾å¼›å˜é‡ Î¾ï¼‰
+ å¹³è¡¡â€œé—´éš”æœ€å¤§â€å’Œâ€œåˆ†ç±»é”™è¯¯æœ€å°â€
+ æ›´é€‚åˆçœŸå®æ•°æ®

####  5ï¸âƒ£ æ ¸å‡½æ•°ï¼ˆå¤„ç†éçº¿æ€§é—®é¢˜ï¼‰

æœ‰äº›æ•°æ®ç”¨ç›´çº¿æ ¹æœ¬åˆ†ä¸å¼€æ€ä¹ˆåŠï¼Ÿ

!!! info

    ğŸ‘‰ ç­”æ¡ˆæ˜¯ï¼šç”¨æ ¸å‡½æ•°æŠŠæ•°æ®â€œæ˜ å°„â€åˆ°é«˜ç»´ç©ºé—´ï¼Œåœ¨é«˜ç»´ç©ºé—´é‡Œåˆ†å¼€ï¼

#### 6ï¸âƒ£ Python å®ç° SVM åˆ†ç±»ï¼ˆçº¿æ€§æ ¸ï¼‰

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import matplotlib.pyplot as plt

# ç”Ÿæˆç®€å•æ•°æ®
X, y = datasets.make_classification(n_samples=100, n_features=2, 
                                     n_redundant=0, n_clusters_per_class=1, random_state=42)

# åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºæ¨¡å‹ï¼Œä½¿ç”¨çº¿æ€§æ ¸
clf = SVC(kernel='linear')
clf.fit(X_train, y_train)

# è¾“å‡ºå‡†ç¡®ç‡
print("å‡†ç¡®ç‡:", clf.score(X_test, y_test))

# å¯è§†åŒ–
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.title("æ”¯æŒå‘é‡æœºåˆ†ç±»æ•ˆæœ")
plt.show()
```

#### 7ï¸âƒ£ SVM çš„ä¼˜ç¼ºç‚¹

##### âœ… ä¼˜ç‚¹

+ åˆ†ç±»å‡†ç¡®ç‡é«˜ï¼Œç‰¹åˆ«æ˜¯é«˜ç»´æ•°æ®
+ å¯¹å°æ ·æœ¬æœ‰æ•ˆ
+ å¯ä»¥ä½¿ç”¨æ ¸å‡½æ•°æ‰©å±•åˆ°éçº¿æ€§åˆ†ç±»
+ ç†è®ºåŸºç¡€å¼ºï¼Œæ³›åŒ–èƒ½åŠ›å¥½

##### âŒ ç¼ºç‚¹

+ å¯¹å‚æ•°æ•æ„Ÿï¼ˆå¦‚ Cã€æ ¸å‚æ•°ï¼‰
+ ä¸é€‚åˆå¤§æ•°æ®é›†ï¼ˆè®­ç»ƒæ…¢ï¼‰
+ å¯¹å™ªå£°å’Œé‡å æ•°æ®ä¸å¤ªç¨³å¥

## å››ã€è´å¶æ–¯æ–¹æ³•

### 4.1 æœ´ç´ è´å¶æ–¯ï¼ˆNaive Bayesï¼‰

#### 1ï¸âƒ£ åŸºæœ¬æ¦‚å¿µ

æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§**åŸºäºè´å¶æ–¯å®šç†ï¼ˆBayes Theoremï¼‰**çš„åˆ†ç±»ç®—æ³•ï¼Œé€‚ç”¨äºæ–‡æœ¬åˆ†ç±»ã€åƒåœ¾é‚®ä»¶æ£€æµ‹ã€æƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ã€‚

##### è´å¶æ–¯å®šç†ï¼š

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

**è§£é‡Šï¼š**

- \(P(A|B)\)ï¼šåœ¨å·²çŸ¥ B çš„æ¡ä»¶ä¸‹ A å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆåéªŒæ¦‚ç‡ï¼‰  
- \(P(B|A)\)ï¼šåœ¨ A å‘ç”Ÿçš„å‰æä¸‹ B å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆä¼¼ç„¶ï¼‰  
- \(P(A)\)ï¼šA å‘ç”Ÿçš„å…ˆéªŒæ¦‚ç‡  
- \(P(B)\)ï¼šB å‘ç”Ÿçš„æ€»æ¦‚ç‡  

#### 2ï¸âƒ£ æœ´ç´ è´å¶æ–¯çš„â€œæœ´ç´ â€å‡è®¾

æœ´ç´ è´å¶æ–¯ç®—æ³•å‡è®¾ï¼š

> ç‰¹å¾ä¹‹é—´æ˜¯**æ¡ä»¶ç‹¬ç«‹**çš„ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä¸€ä¸ªæ–‡æœ¬æœ‰å¤šä¸ªè¯ï¼ˆç‰¹å¾ï¼‰ï¼Œæœ´ç´ è´å¶æ–¯ä¼šè®¤ä¸ºè¿™äº›è¯æ˜¯å½¼æ­¤ç‹¬ç«‹å½±å“ç»“æœçš„ã€‚

#### 3ï¸âƒ£ å…¬å¼æ¨å¯¼


è®¾æœ‰ç‰¹å¾å‘é‡ \(X = (x_1, x_2, ..., x_n)\)ï¼Œç±»åˆ«ä¸º \(C_k\)ï¼Œç›®æ ‡æ˜¯è®¡ç®—ï¼š

$$
P(C_k|X) \propto P(C_k) \cdot \prod_{i=1}^{n} P(x_i|C_k)
$$

- \(P(C_k)\)ï¼šç±»åˆ« \(C_k\) çš„å…ˆéªŒæ¦‚ç‡  
- \(P(x_i|C_k)\)ï¼šåœ¨ç±»åˆ«ä¸º \(C_k\) çš„æ¡ä»¶ä¸‹ï¼Œç¬¬ \(i\) ä¸ªç‰¹å¾å‡ºç°çš„æ¦‚ç‡  

#### 4ï¸âƒ£ å¸¸è§çš„ä¸‰ç§æœ´ç´ è´å¶æ–¯æ¨¡å‹


| ç±»å‹              | ç‰¹å¾å˜é‡ç±»å‹ | ä¸¾ä¾‹                        |
|-------------------|---------------|-----------------------------|
| é«˜æ–¯æœ´ç´ è´å¶æ–¯    | è¿ç»­å€¼       | èº«é«˜ã€ä½“é‡ã€æ¸©åº¦ç­‰         |
| å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯  | è¯é¢‘è®¡æ•°     | æ–‡æœ¬åˆ†ç±»ï¼ˆè¯å‡ºç°æ¬¡æ•°ï¼‰     |
| ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯  | äºŒå€¼å˜é‡     | æ–‡æœ¬ä¸­æŸè¯æ˜¯å¦å‡ºç°ï¼ˆ0/1ï¼‰  |

#### 5ï¸âƒ£ é€šä¿—ä¾‹å­ï¼šå¤©æ°”ä¸æ‰“çƒé¢„æµ‹

å‡è®¾ä½ æƒ³æ ¹æ®å¤©æ°”é¢„æµ‹æ˜¯å¦å»æ‰“çƒï¼Œæœ‰å¦‚ä¸‹è®­ç»ƒæ•°æ®ï¼š

| å¤©æ°”   | æ¹¿åº¦ | é£å¤§ | å»æ‰“çƒ |
|--------|------|------|--------|
| æ™´å¤©   | é«˜   | å¦   | å¦     |
| é˜´å¤©   | é«˜   | å¦   | æ˜¯     |
| é›¨å¤©   | é«˜   | å¦   | æ˜¯     |
| é›¨å¤©   | ä½   | æ˜¯   | å¦     |
| æ™´å¤©   | ä½   | æ˜¯   | æ˜¯     |

##### æ­¥éª¤1ï¼šè®¡ç®—å…ˆéªŒæ¦‚ç‡

- å»æ‰“çƒï¼ˆæ˜¯ï¼‰ï¼š3/5  
- ä¸å»æ‰“çƒï¼ˆå¦ï¼‰ï¼š2/5  

##### æ­¥éª¤2ï¼šè®¡ç®—æ¡ä»¶æ¦‚ç‡ï¼ˆä»¥â€œæ™´å¤©, é«˜æ¹¿åº¦, æ— é£â€ä¸ºä¾‹ï¼‰

- \(P(æ™´å¤©|æ˜¯)\)ï¼Œ\(P(é«˜æ¹¿åº¦|æ˜¯)\)ï¼Œ\(P(æ— é£|æ˜¯)\)  
- \(P(æ™´å¤©|å¦)\)ï¼Œ\(P(é«˜æ¹¿åº¦|å¦)\)ï¼Œ\(P(æ— é£|å¦)\)  

##### æ­¥éª¤3ï¼šä»£å…¥è´å¶æ–¯å…¬å¼è®¡ç®—åéªŒæ¦‚ç‡

æ¯”è¾ƒ \(P(æ˜¯|æ™´å¤©, é«˜, æ— é£)\) å’Œ \(P(å¦|æ™´å¤©, é«˜, æ— é£)\)ï¼Œå“ªä¸ªå¤§å°±é¢„æµ‹å“ªä¸ªç±»åˆ«ã€‚

#### 6ï¸âƒ£ ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ sklearnï¼‰

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

# ç¤ºä¾‹æ–‡æœ¬å’Œæ ‡ç­¾
texts = ["æˆ‘ å–œæ¬¢ ç¯®çƒ", "ä½  å–œæ¬¢ è¶³çƒ", "ä»– è®¨åŒ ç¯®çƒ", "æˆ‘ å–œæ¬¢ è¶³çƒ"]
labels = ["æ­£é¢", "æ­£é¢", "è´Ÿé¢", "æ­£é¢"]

# å‘é‡åŒ–æ–‡æœ¬
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

# è®­ç»ƒæ¨¡å‹
model = MultinomialNB()
model.fit(X, labels)

# é¢„æµ‹
test = vectorizer.transform(["æˆ‘ å–œæ¬¢ è¶³çƒ"])
print(model.predict(test))  # è¾“å‡ºï¼š['æ­£é¢']
```

#### 7ï¸âƒ£ ä¼˜ç¼ºç‚¹

##### âœ… ä¼˜ç‚¹

+ è®­ç»ƒé€Ÿåº¦å¿«ï¼šä¸éœ€è¦å¤æ‚è®¡ç®—
+ å¯¹é«˜ç»´æ•°æ®æœ‰æ•ˆï¼šå¦‚æ–‡æœ¬åˆ†ç±»
+ å®ç°ç®€å•ï¼šæ˜“äºç†è§£ä¸å®ç°

##### âŒ ç¼ºç‚¹

+ å‡è®¾ç‰¹å¾ç‹¬ç«‹ï¼šå®é™…ä¸­ç‰¹å¾å¾€å¾€ç›¸å…³
+ å¯¹æ¦‚ç‡ä¸º0æ•æ„Ÿï¼šéœ€è¦å¹³æ»‘å¤„ç†ï¼ˆå¦‚æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰

#### 8ï¸âƒ£ å¹³æ»‘æŠ€æœ¯ï¼ˆæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰

å½“æŸä¸ªæ¡ä»¶æ¦‚ç‡ä¸º 0 æ—¶ï¼Œæ•´ä¸ªä¹˜ç§¯ä¸º 0ï¼Œå¯¼è‡´é¢„æµ‹å¤±è´¥ã€‚

**è§£å†³æ–¹æ³•ï¼š**

$$
P(x_i|C_k) = \frac{count(x_i, C_k) + 1}{count(C_k) + V}
$$

å…¶ä¸­ (V) æ˜¯ç‰¹å¾æ€»æ•°ã€‚

#### 9ï¸âƒ£ åº”ç”¨åœºæ™¯

+ ğŸ“§ åƒåœ¾é‚®ä»¶åˆ†ç±»
+ ğŸ’¬ æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
+ ğŸ§¬ åŒ»ç–—è¯Šæ–­åˆ†ç±»
+ ğŸ§¾ æ–°é—»åˆ†ç±»


### 4.2 è´å¶æ–¯ç½‘ç»œ

#### 1ï¸âƒ£ ä»€ä¹ˆæ˜¯è´å¶æ–¯ç½‘ç»œï¼Ÿ

è´å¶æ–¯ç½‘ç»œï¼ˆBayesian Networkï¼Œç®€ç§° BNï¼‰æ˜¯ä¸€ç§**æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ¨¡å‹**ï¼Œç”¨äºè¡¨ç¤ºå˜é‡ä¹‹é—´çš„**æ¦‚ç‡å…³ç³»**ï¼Œç»“åˆäº†å›¾è®ºä¸æ¦‚ç‡è®ºçš„æ€æƒ³ã€‚

> æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ª**éšæœºå˜é‡**ï¼Œæ¯æ¡è¾¹ä»£è¡¨å˜é‡é—´çš„**æ¡ä»¶ä¾èµ–å…³ç³»**ã€‚

#### 2ï¸âƒ£ åŸºæœ¬ç»„æˆ

1. **èŠ‚ç‚¹ï¼ˆNodeï¼‰**ï¼šè¡¨ç¤ºéšæœºå˜é‡  
2. **æœ‰å‘è¾¹ï¼ˆDirected Edgeï¼‰**ï¼šè¡¨ç¤ºå˜é‡é—´çš„ä¾èµ–  
3. **æ¡ä»¶æ¦‚ç‡è¡¨ï¼ˆConditional Probability Table, CPTï¼‰**ï¼šè¡¨ç¤ºåœ¨ç»™å®šçˆ¶èŠ‚ç‚¹çš„æ¡ä»¶ä¸‹ï¼Œè¯¥å˜é‡çš„æ¦‚ç‡åˆ†å¸ƒ

#### 3ï¸âƒ£ å½¢å¼å®šä¹‰

è´å¶æ–¯ç½‘ç»œæ˜¯ä¸€ä¸ªä¸‰å…ƒç»„ï¼š

\[
G = (X, E, P)
\]

- \(X = \{X_1, X_2, ..., X_n\}\)ï¼šå˜é‡é›†åˆ  
- \(E\)ï¼šè¾¹çš„é›†åˆï¼Œæ„æˆä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰  
- \(P = \{P(X_i | Pa(X_i))\}\)ï¼šæ¯ä¸ªå˜é‡çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒï¼ˆPa è¡¨ç¤ºçˆ¶èŠ‚ç‚¹ï¼‰


#### 4ï¸âƒ£ ä¾‹å­ï¼šæ„Ÿå†’è¯Šæ–­æ¨¡å‹

è€ƒè™‘å¦‚ä¸‹å˜é‡ï¼š

+ Cï¼šæ„Ÿå†’ï¼ˆColdï¼‰
+ Fï¼šå‘çƒ§ï¼ˆFeverï¼‰
+ Sï¼šæ‰“å–·åšï¼ˆSneezeï¼‰

##### å›¾ç»“æ„ï¼š
```text
   C
  / \
 F   S
```
F å’Œ S éƒ½ç”± C å†³å®š

**æ¡ä»¶æ¦‚ç‡è¡¨ç¤ºï¼š**

+ P(C)ï¼šæ„Ÿå†’çš„å…ˆéªŒæ¦‚ç‡
+ P(F|C)ï¼šåœ¨æ„Ÿå†’æƒ…å†µä¸‹å‘çƒ§çš„æ¦‚ç‡
+ P(S|C)ï¼šåœ¨æ„Ÿå†’æƒ…å†µä¸‹æ‰“å–·åšçš„æ¦‚ç‡

#### 5ï¸âƒ£ è”åˆæ¦‚ç‡åˆ†å¸ƒ

é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œæœ‰ï¼š

$P(C, F, S) = P(C) \cdot P(F|C) \cdot P(S|C)$

è´å¶æ–¯ç½‘ç»œçš„ä¼˜ç‚¹æ˜¯æŠŠå¤æ‚çš„è”åˆæ¦‚ç‡æ‹†åˆ†ä¸ºæ›´å®¹æ˜“è®¡ç®—çš„å±€éƒ¨æ¡ä»¶æ¦‚ç‡ã€‚

#### 6ï¸âƒ£ æ¨ç†ï¼ˆInferenceï¼‰

##### é—®é¢˜ç¤ºä¾‹ï¼š

å¦‚æœçŸ¥é“æŸäººå‘çƒ§ï¼ˆ\(F = 1\)ï¼‰ï¼Œæ±‚ä»–æ„Ÿå†’çš„æ¦‚ç‡ï¼ˆ\(P(C=1 \mid F=1)\)ï¼‰

###### ä½¿ç”¨è´å¶æ–¯å…¬å¼ï¼š

\[
P(C=1 \mid F=1) = \frac{P(F=1 \mid C=1) \cdot P(C=1)}{P(F=1)}
\]

å…¶ä¸­ \(P(F=1)\) å¯é€šè¿‡å…¨æ¦‚ç‡å…¬å¼è®¡ç®—ï¼š

\[
P(F=1) = P(F=1 \mid C=1) \cdot P(C=1) + P(F=1 \mid C=0) \cdot P(C=0)
\]

##### æ­¥éª¤æ€»ç»“ï¼š

1. åˆ©ç”¨ **è´å¶æ–¯å…¬å¼** è¿›è¡Œåæ¨ã€‚
2. è‹¥åˆ†æ¯æœªçŸ¥ï¼Œä½¿ç”¨**å…¨æ¦‚ç‡å…¬å¼**å±•å¼€ã€‚
3. å¯ä»¥ç”¨è¡¨æ ¼ã€æ¨ç†å¼•æ“ï¼ˆå¦‚ pgmpyï¼‰æ±‚è§£ã€‚

#### 7ï¸âƒ£ å»ºæ¨¡æµç¨‹

1. **ç¡®å®šå˜é‡**ï¼šå¦‚å¤©æ°”ã€ç–¾ç—…ã€ç—‡çŠ¶ç­‰  
2. **ç¡®å®šä¾èµ–å…³ç³»ï¼ˆå›¾ç»“æ„ï¼‰**  
3. **æ”¶é›†æ•°æ®ï¼Œæ„å»º CPTï¼ˆæ¡ä»¶æ¦‚ç‡è¡¨ï¼‰**  
4. **æ¨ç†è®¡ç®—æˆ–é‡‡æ ·ï¼ˆå¦‚é¢„æµ‹ã€å†³ç­–ç­‰ï¼‰**

#### 8ï¸âƒ£ åº”ç”¨åœºæ™¯

| åœºæ™¯             | æè¿°                         |
|------------------|------------------------------|
| åŒ»ç–—è¯Šæ–­         | ç–¾ç—…ã€ç—‡çŠ¶ä¹‹é—´çš„å› æœå»ºæ¨¡     |
| é£é™©è¯„ä¼°         | é‡‘èæ¬ºè¯ˆã€ä¿¡ç”¨è¿çº¦åˆ†æ       |
| æ™ºèƒ½æ¨èç³»ç»Ÿ     | ç”¨æˆ·åå¥½å»ºæ¨¡                 |
| æœºå™¨æ•…éšœè¯Šæ–­     | å„ç§éƒ¨ä»¶çŠ¶æ€å»ºæ¨¡             |
| NLP è¯­ä¹‰å»ºæ¨¡     | è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»             |

#### 9ï¸âƒ£ ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ Python `pgmpy` åº“ï¼‰

```python
from pgmpy.models import BayesianNetwork
from pgmpy.factors.discrete import TabularCPD
from pgmpy.inference import VariableElimination

# æ„å»ºè´å¶æ–¯ç½‘ç»œç»“æ„
model = BayesianNetwork([('C', 'F'), ('C', 'S')])

# å®šä¹‰æ¡ä»¶æ¦‚ç‡è¡¨
cpd_c = TabularCPD(variable='C', variable_card=2, values=[[0.7], [0.3]])
cpd_f = TabularCPD(variable='F', variable_card=2,
                   values=[[0.9, 0.2], [0.1, 0.8]],
                   evidence=['C'], evidence_card=[2])
cpd_s = TabularCPD(variable='S', variable_card=2,
                   values=[[0.8, 0.3], [0.2, 0.7]],
                   evidence=['C'], evidence_card=[2])

# åŠ å…¥æ¨¡å‹
model.add_cpds(cpd_c, cpd_f, cpd_s)

# éªŒè¯æ¨¡å‹
assert model.check_model()

# æ¨ç†ï¼šå·²çŸ¥ F=1ï¼Œæ±‚ C çš„æ¦‚ç‡
infer = VariableElimination(model)
result = infer.query(variables=['C'], evidence={'F': 1})
print(result)
```

## äº”ã€ æœ€è¿‘é‚»ç®—æ³•

### 5.1 Kè¿‘é‚»ï¼ˆK-Nearest Neighbors, KNNï¼‰

#### 1ï¸âƒ£ æ¦‚è¿°

**K-è¿‘é‚»ç®—æ³•ï¼ˆK-Nearest Neighbors, KNNï¼‰** æ˜¯ä¸€ç§**ç›‘ç£å­¦ä¹ **ç®—æ³•ï¼Œå¯ç”¨äº**åˆ†ç±»**å’Œ**å›å½’**ã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯ï¼š

> ä¸€ä¸ªæ ·æœ¬çš„ç±»åˆ«æˆ–æ•°å€¼ç”±å…¶åœ¨ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘çš„ K ä¸ªæ ·æœ¬æ‰€å†³å®šã€‚

KNN æ˜¯ä¸€ç§**åŸºäºå®ä¾‹çš„å­¦ä¹ ï¼ˆlazy learningï¼‰**æ–¹æ³•ï¼Œå®ƒä¸æ˜¾å¼åœ°æ„é€ æ¨¡å‹ï¼Œè€Œæ˜¯ç­‰åˆ°é¢„æµ‹æ—¶æ‰è¿›è¡Œè®¡ç®—ã€‚

#### 2ï¸âƒ£ æ ¸å¿ƒæ€æƒ³

##### åˆ†ç±»ä»»åŠ¡ï¼š

ç»™å®šä¸€ä¸ªå¾…åˆ†ç±»æ ·æœ¬ï¼Œæ‰¾å‡ºå…¶åœ¨è®­ç»ƒé›†ä¸­æœ€è¿‘çš„ K ä¸ªé‚»å±…ï¼Œæ ¹æ®è¿™ K ä¸ªé‚»å±…çš„å¤šæ•°ç±»åˆ«ï¼Œç¡®å®šè¯¥æ ·æœ¬çš„ç±»åˆ«ã€‚

##### å›å½’ä»»åŠ¡ï¼š

è¾“å‡º K ä¸ªé‚»å±…çš„å¹³å‡å€¼æˆ–åŠ æƒå¹³å‡å€¼ä½œä¸ºé¢„æµ‹å€¼ã€‚

#### 3ï¸âƒ£ åŸºæœ¬æ­¥éª¤

1. é€‰æ‹©ä¸€ä¸ªæ•´æ•° \( K \) å€¼ã€‚
2. è®¡ç®—å¾…é¢„æµ‹æ ·æœ¬ä¸è®­ç»ƒé›†ä¸­æ‰€æœ‰æ ·æœ¬ä¹‹é—´çš„**è·ç¦»**ã€‚
3. é€‰å‡ºè·ç¦»æœ€è¿‘çš„ K ä¸ªæ ·æœ¬ã€‚
4. åˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨**å¤šæ•°æŠ•ç¥¨**æ³•å†³å®šåˆ†ç±»ã€‚  
   å›å½’ä»»åŠ¡ï¼šä½¿ç”¨**å¹³å‡å€¼æˆ–åŠ æƒå¹³å‡å€¼**è¿›è¡Œé¢„æµ‹ã€‚

#### 4ï¸âƒ£ å¸¸è§è·ç¦»åº¦é‡


##### 1ï¸âƒ£ æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆEuclidean Distanceï¼‰

\[
d(x, y) = \sqrt{ \sum_{i=1}^n (x_i - y_i)^2 }
\]

##### 2ï¸âƒ£ æ›¼å“ˆé¡¿è·ç¦»ï¼ˆManhattan Distanceï¼‰

\[
d(x, y) = \sum_{i=1}^n |x_i - y_i|
\]

##### 3ï¸âƒ£ é—µå¯å¤«æ–¯åŸºè·ç¦»ï¼ˆMinkowski Distanceï¼‰

\[
d(x, y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}
\]

#### 5ï¸âƒ£ K çš„é€‰æ‹©

| K å€¼     | ç‰¹å¾                            |
|----------|---------------------------------|
| K è¾ƒå°   | æ¨¡å‹å¯¹å™ªå£°æ•æ„Ÿï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ       |
| K è¾ƒå¤§   | æ¨¡å‹è¿‡äºå¹³æ»‘ï¼Œå®¹æ˜“æ¬ æ‹Ÿåˆ         |
| ä¸€èˆ¬é€‰æ‹© | é€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜çš„ K å€¼      |


#### 6ï¸âƒ£ ä¼˜ç‚¹ä¸ç¼ºç‚¹

| ä¼˜ç‚¹                         | ç¼ºç‚¹                                 |
|------------------------------|--------------------------------------|
| åŸç†ç®€å•ã€æ˜“äºå®ç°           | é¢„æµ‹æ…¢ï¼Œéœ€å­˜å‚¨æ‰€æœ‰æ•°æ®               |
| æ— éœ€è®­ç»ƒè¿‡ç¨‹ï¼Œé€‚åˆå°æ•°æ®é›†   | å¯¹é«˜ç»´æ•°æ®ï¼ˆç»´åº¦ç¾éš¾ï¼‰ä¸é€‚ç”¨         |
| å¯ä»¥å¤„ç†å¤šåˆ†ç±»é—®é¢˜           | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œä¾èµ–è·ç¦»åº¦é‡æ–¹å¼       |


#### 7ï¸âƒ£ ä»£ç å®ç°ï¼ˆä»¥ sklearn ä¸ºä¾‹ï¼‰

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®
iris = load_iris()
X = iris.data
y = iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åˆ›å»º KNN åˆ†ç±»å™¨ï¼ŒK=3
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# é¢„æµ‹
y_pred = knn.predict(X_test)

# å‡†ç¡®ç‡
print("å‡†ç¡®ç‡ï¼š", accuracy_score(y_test, y_pred))
```

#### 8ï¸âƒ£ KNN åº”ç”¨åœºæ™¯

| åº”ç”¨é¢†åŸŸ   | ç¤ºä¾‹                                     |
|------------|------------------------------------------|
| åŒ»ç–—è¯Šæ–­   | æ ¹æ®ç—‡çŠ¶é¢„æµ‹ç–¾ç—…                         |
| æ¨èç³»ç»Ÿ   | åŸºäºç”¨æˆ·ç›¸ä¼¼æ€§è¿›è¡Œæ¨è                   |
| æ–‡æœ¬åˆ†ç±»   | æ–°é—»ã€é‚®ä»¶ã€æƒ…æ„Ÿåˆ†æç­‰æ–‡æœ¬åˆ†ç±»ä»»åŠ¡       |
| å›¾åƒè¯†åˆ«   | é€šè¿‡åƒç´ ç‰¹å¾è¯†åˆ«æ•°å­—ã€ç‰©ä½“               |


## å…­ã€ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ 

### 6.1 å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

#### 1ï¸âƒ£ æ¦‚è¿°

å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ˜¯ä¸€ç§å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”±å¤šä¸ªç¥ç»å…ƒå±‚æ„æˆï¼š

- è¾“å…¥å±‚ï¼ˆInput Layerï¼‰
- ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚ï¼ˆHidden Layersï¼‰
- è¾“å‡ºå±‚ï¼ˆOutput Layerï¼‰

ç‰¹ç‚¹ï¼š

- å…¨è¿æ¥ç»“æ„ï¼ˆFully Connectedï¼‰
- å¯ç”¨äºåˆ†ç±»ä¸å›å½’ä»»åŠ¡
- æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„åŸºç¡€ç»„æˆæ¨¡å—

#### 2ï¸âƒ£ ç½‘ç»œç»“æ„å›¾

```
è¾“å…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ â€¦ â†’ è¾“å‡ºå±‚
```

è¯´æ˜ï¼š

- è¾“å…¥å±‚ï¼šèŠ‚ç‚¹æ•° = ç‰¹å¾ç»´åº¦  
- éšè—å±‚ï¼šå¯ä»¥æœ‰å¤šä¸ªï¼Œæ¯å±‚èŠ‚ç‚¹æ•°å¯è°ƒ  
- è¾“å‡ºå±‚ï¼š
    - åˆ†ç±»ä»»åŠ¡ï¼šèŠ‚ç‚¹æ•° = ç±»åˆ«æ•°
    - å›å½’ä»»åŠ¡ï¼šèŠ‚ç‚¹æ•° = 1


####  3ï¸âƒ£ å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰


æ¯å±‚çš„è®¡ç®—å¦‚ä¸‹ï¼š

$$
z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}
$$

$$
a^{(l)} = \sigma(z^{(l)})
$$

å…¶ä¸­ï¼š

- \( a^{(l-1)} \)ï¼šä¸Šä¸€å±‚çš„è¾“å‡º
- \( W^{(l)} \)ã€\( b^{(l)} \)ï¼šå½“å‰å±‚çš„æƒé‡ä¸åç½®
- \( \sigma \)ï¼šæ¿€æ´»å‡½æ•°ï¼Œå¦‚ ReLUã€Sigmoid ç­‰

#### 4ï¸âƒ£ å¸¸è§æ¿€æ´»å‡½æ•°

| æ¿€æ´»å‡½æ•° | æ•°å­¦è¡¨è¾¾å¼ | ç‰¹ç‚¹ |
|----------|------------|------|
| Sigmoid  | \( \sigma(x) = \frac{1}{1 + e^{-x}} \) | è¾“å‡ºèŒƒå›´åœ¨ (0, 1)ï¼Œå®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤± |
| Tanh     | \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \) | è¾“å‡ºèŒƒå›´ (-1, 1)ï¼Œé›¶ä¸­å¿ƒï¼Œæœ‰åŠ©äºæ”¶æ•› |
| ReLU     | \( \mathrm{ReLU}(x) = \max(0, x) \) | è®¡ç®—ç®€å•ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œè®­ç»ƒæ•ˆæœå¥½ |

#### 5ï¸âƒ£ åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰

åå‘ä¼ æ’­ç®—æ³•ç”¨äºè®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°æƒé‡å’Œåç½®ã€‚

è®¡ç®—è¾“å‡ºå±‚è¯¯å·®ï¼š

$$
\delta^{(L)} = \nabla_a L \circ \sigma'(z^{(L)})
$$

è¯¯å·®é€å±‚åä¼ ï¼š

$$
\delta^{(l)} = \left(W^{(l+1)}\right)^{T} \delta^{(l+1)} \circ \sigma'(z^{(l)})
$$

å‚æ•°æ¢¯åº¦è®¡ç®—ï¼š

$$
\nabla_{W^{(l)}} L = \delta^{(l)} (a^{(l-1)})^{T}
$$

$$
\nabla_{b^{(l)}} L = \delta^{(l)}
$$

å‚æ•°æ›´æ–°ï¼ˆå­¦ä¹ ç‡ä¸º \( \eta \)ï¼‰ï¼š

$$
W^{(l)} \leftarrow W^{(l)} - \eta \nabla_{W^{(l)}} L
$$

$$
b^{(l)} \leftarrow b^{(l)} - \eta \nabla_{b^{(l)}} L
$$

#### 6ï¸âƒ£ æŸå¤±å‡½æ•°

| ä»»åŠ¡ç±»å‹ | å¸¸ç”¨æŸå¤±å‡½æ•° | æ•°å­¦è¡¨è¾¾å¼ |
|----------|--------------|------------|
| å›å½’     | å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ | \( \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 \) |
| åˆ†ç±»     | äº¤å‰ç†µæŸå¤±ï¼ˆCross Entropyï¼‰ | \( \mathrm{CE} = - \sum_{i} y_i \log(\hat{y}_i) \) |


#### 7ï¸âƒ£ PyTorch å®ç°ç¤ºä¾‹

```python
import torch
import torch.nn as nn
import torch.optim as optim

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(4, 16),
            nn.ReLU(),
            nn.Linear(16, 3)
        )
    def forward(self, x):
        return self.net(x)

model = MLP()
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# ç¤ºä¾‹è®­ç»ƒè¿‡ç¨‹
for epoch in range(100):
    inputs = torch.randn(10, 4)  # å‡è®¾è¾“å…¥10ä¸ªæ ·æœ¬ï¼Œ4ç»´ç‰¹å¾
    targets = torch.randint(0, 3, (10,))  # å‡è®¾3åˆ†ç±»
    outputs = model(inputs)
    loss = loss_fn(outputs, targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### 6.2 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

#### 1ï¸âƒ£ æ¦‚è¿°


å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¤„ç†å…·æœ‰ç½‘æ ¼ç»“æ„æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚å®ƒé€šè¿‡å·ç§¯æ“ä½œè‡ªåŠ¨æå–è¾“å…¥æ•°æ®ä¸­çš„å±€éƒ¨ç‰¹å¾ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒè¯†åˆ«ã€è§†é¢‘åˆ†æã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚

CNN çš„ä¸»è¦ç‰¹ç‚¹ï¼š

- å±€éƒ¨è¿æ¥ï¼šå·ç§¯æ ¸åªä½œç”¨äºè¾“å…¥çš„å±€éƒ¨åŒºåŸŸ
- æƒé‡å…±äº«ï¼šåŒä¸€å·ç§¯æ ¸å‚æ•°åœ¨æ•´ä¸ªè¾“å…¥ä¸Šå…±äº«ï¼Œå‡å°‘å‚æ•°æ•°é‡
- ä¸‹é‡‡æ ·ï¼ˆPoolingï¼‰ï¼šç¼©å‡æ•°æ®ç»´åº¦ï¼Œæå–é‡è¦ç‰¹å¾ï¼Œå‡å°‘è®¡ç®—é‡

#### 2ï¸âƒ£ CNN ä¸»è¦ç»“æ„ç»„æˆ

- **å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰**  
  åˆ©ç”¨å·ç§¯æ ¸å¯¹è¾“å…¥åšæ»‘åŠ¨çª—å£å·ç§¯æ“ä½œï¼Œæå–å±€éƒ¨ç‰¹å¾ã€‚

- **æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰**  
  å¸¸ç”¨ ReLUï¼Œå¢åŠ ç½‘ç»œéçº¿æ€§ã€‚

- **æ± åŒ–å±‚ï¼ˆPooling Layerï¼‰**  
  å¯¹ç‰¹å¾å›¾é™é‡‡æ ·ï¼Œå¸¸ç”¨æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰ã€‚

- **å…¨è¿æ¥å±‚ï¼ˆFully Connected Layerï¼‰**  
  ç”¨äºå°†å·ç§¯æå–çš„ç‰¹å¾æ˜ å°„åˆ°æœ€ç»ˆè¾“å‡ºï¼ˆå¦‚åˆ†ç±»æ¦‚ç‡ï¼‰ã€‚

#### 3ï¸âƒ£ å·ç§¯æ“ä½œ

å‡è®¾è¾“å…¥çŸ©é˜µä¸º \(X\)ï¼Œå·ç§¯æ ¸ä¸º \(K\)ï¼Œè¾“å‡ºçŸ©é˜µä¸º \(Y\)ï¼Œå·ç§¯è®¡ç®—ä¸ºï¼š

$$
Y(i,j) = \sum_m \sum_n X(i+m, j+n) \times K(m,n)
$$

å…¶ä¸­ \(i,j\) ä¸ºè¾“å‡ºä½ç½®ç´¢å¼•ï¼Œ\(m,n\) éå†å·ç§¯æ ¸å¤§å°ã€‚

#### 4ï¸âƒ£ æ¿€æ´»å‡½æ•°

å¸¸ç”¨æ¿€æ´»å‡½æ•°ä¸º ReLUï¼š

$$
\mathrm{ReLU}(x) = \max(0, x)
$$


#### 5ï¸âƒ£ æ± åŒ–å±‚

æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰ä»¥çª—å£æ»‘åŠ¨ï¼Œé€‰å–çª—å£å†…æœ€å¤§å€¼ï¼Œå…¬å¼ç¤ºæ„ï¼š

$$
Y(i,j) = \max_{\substack{m=1,...,k \\ n=1,...,k}} X(s \cdot i + m, s \cdot j + n)
$$

å…¶ä¸­ \(k\) ä¸ºæ± åŒ–çª—å£å¤§å°ï¼Œ\(s\) ä¸ºæ­¥å¹…ã€‚

#### 6ï¸âƒ£ å‰å‘ä¼ æ’­æµç¨‹ç¤ºæ„


è¾“å…¥ â†’ å·ç§¯å±‚ â†’ æ¿€æ´»å‡½æ•° â†’ æ± åŒ–å±‚ â†’ å…¨è¿æ¥å±‚ â†’ è¾“å‡º

#### 7ï¸âƒ£ CNN åº”ç”¨åœºæ™¯

| åº”ç”¨é¢†åŸŸ   | è¯´æ˜                         |
|------------|------------------------------|
| å›¾åƒåˆ†ç±»   | è¯†åˆ«å›¾åƒæ‰€å±ç±»åˆ«             |
| ç›®æ ‡æ£€æµ‹   | å®šä½å›¾åƒä¸­ç›®æ ‡ç‰©ä½“           |
| å›¾åƒåˆ†å‰²   | åƒç´ çº§åˆ†ç±»                   |
| è§†é¢‘åˆ†æ   | åŠ¨ä½œè¯†åˆ«ã€äº‹ä»¶æ£€æµ‹           |
| è‡ªç„¶è¯­è¨€å¤„ç† | æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æï¼ˆç»“åˆ1Då·ç§¯ï¼‰ |


#### 8ï¸âƒ£ ç®€å• PyTorch å®ç°ç¤ºä¾‹

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 16 * 16, 10)  # å‡è®¾è¾“å…¥å›¾åƒå¤§å°ä¸º 32x32ï¼Œ3é€šé“ï¼Œè¾“å‡º10ç±»

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # å·ç§¯ + ReLU + æ± åŒ–
        x = torch.flatten(x, 1)               # å±•å¹³é™¤batchå¤–æ‰€æœ‰ç»´åº¦
        x = self.fc1(x)                       # å…¨è¿æ¥å±‚
        return x

model = SimpleCNN()
print(model)
```

### 6.3 å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNN/LSTMï¼‰

#### 1ï¸âƒ£ æ¦‚è¿°

å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ˜¯ä¸€ç±»é€‚åˆå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œèƒ½æ•æ‰æ—¶é—´æˆ–é¡ºåºä¸Šçš„ä¾èµ–å…³ç³»ã€‚å®ƒé€šè¿‡éšè—çŠ¶æ€ï¼ˆhidden stateï¼‰å°†è¿‡å»çš„ä¿¡æ¯ä¼ é€’åˆ°å½“å‰æ—¶åˆ»ã€‚

**é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰**æ˜¯RNNçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œè§£å†³äº†ä¼ ç»ŸRNNæ¢¯åº¦æ¶ˆå¤±å’Œé•¿æœŸä¾èµ–éš¾ä»¥æ•è·çš„é—®é¢˜ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è®°å¿†é•¿è·ç¦»ä¿¡æ¯ã€‚

#### 2ï¸âƒ£ RNN åŸºæœ¬ç»“æ„

RNN æ¯ä¸ªæ—¶é—´æ­¥ \(t\) æ¥æ”¶è¾“å…¥ \(x_t\) å’Œä¸Šä¸€æ—¶åˆ»éšè—çŠ¶æ€ \(h_{t-1}\)ï¼Œè®¡ç®—å½“å‰éšè—çŠ¶æ€ \(h_t\) å’Œè¾“å‡º \(y_t\)ï¼š

$$
h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
$$

$$
y_t = W_{hy} h_t + b_y
$$

#### 3ï¸âƒ£ LSTM ç»“æ„

LSTM ä½¿ç”¨é—¨æ§æœºåˆ¶æ§åˆ¶ä¿¡æ¯æµï¼ŒåŒ…å«ä¸‰ä¸ªé—¨ï¼š

- è¾“å…¥é—¨ \(i_t\)ï¼šæ§åˆ¶æ–°ä¿¡æ¯å†™å…¥å¤šå°‘
- é—å¿˜é—¨ \(f_t\)ï¼šæ§åˆ¶ä¿ç•™å¤šå°‘è¿‡å»çŠ¶æ€
- è¾“å‡ºé—¨ \(o_t\)ï¼šæ§åˆ¶è¾“å‡ºå¤šå°‘å½“å‰çŠ¶æ€

æ ¸å¿ƒå•å…ƒçŠ¶æ€ \(C_t\) å’Œéšè—çŠ¶æ€ \(h_t\) è®¡ç®—å¦‚ä¸‹ï¼š

$$
f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
$$

$$
i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
$$

$$
\tilde{C}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)
$$

$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$

$$
o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
$$

$$
h_t = o_t \odot \tanh(C_t)
$$

å…¶ä¸­ï¼š

- \(\sigma\)ï¼šSigmoidæ¿€æ´»å‡½æ•°
- \(\odot\)ï¼šé€å…ƒç´ ä¹˜ç§¯


#### 4ï¸âƒ£ RNN ä¸ LSTM çš„åŒºåˆ«

| ç‰¹ç‚¹           | RNN                    | LSTM                          |
|----------------|------------------------|-------------------------------|
| è®°å¿†èƒ½åŠ›       | å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±ï¼ŒçŸ­æœŸè®°å¿† | é—¨æ§æœºåˆ¶ï¼Œæœ‰æ•ˆé•¿çŸ­æœŸè®°å¿†       |
| ç»“æ„å¤æ‚åº¦     | ç®€å•                   | ç»“æ„å¤æ‚ï¼Œè®¡ç®—é‡å¤§             |
| åº”ç”¨åœºæ™¯       | ç®€å•åºåˆ—ä»»åŠ¡           | å¤æ‚é•¿åºåˆ—ä»»åŠ¡                 |


#### 5ï¸âƒ£ åº”ç”¨åœºæ™¯

| åº”ç”¨é¢†åŸŸ         | è¯´æ˜                          |
|------------------|-------------------------------|
| è¯­è¨€æ¨¡å‹         | é¢„æµ‹ä¸‹ä¸€ä¸ªè¯                    |
| æœºå™¨ç¿»è¯‘         | åºåˆ—åˆ°åºåˆ—ç¿»è¯‘                  |
| è¯­éŸ³è¯†åˆ«         | éŸ³é¢‘åºåˆ—è½¬æ–‡å­—                  |
| æ—¶é—´åºåˆ—é¢„æµ‹     | è‚¡ç¥¨ä»·æ ¼ã€æ°”è±¡æ•°æ®ç­‰             |

#### 6ï¸âƒ£ PyTorch ç®€å•ç¤ºä¾‹

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)      # out: (batch, seq_len, hidden_size)
        out = out[:, -1, :]       # å–æœ€åæ—¶é—´æ­¥è¾“å‡º
        out = self.fc(out)
        return out

class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)     # out: (batch, seq_len, hidden_size)
        out = out[:, -1, :]       # å–æœ€åæ—¶é—´æ­¥è¾“å‡º
        out = self.fc(out)
        return out

# ç¤ºä¾‹ä½¿ç”¨
rnn_model = SimpleRNN(input_size=10, hidden_size=20, output_size=1)
lstm_model = SimpleLSTM(input_size=10, hidden_size=20, output_size=1)

sample_input = torch.randn(5, 15, 10)  # batch=5, seq_len=15, feature=10
rnn_out = rnn_model(sample_input)
lstm_out = lstm_model(sample_input)
print(rnn_out.shape, lstm_out.shape)
```

