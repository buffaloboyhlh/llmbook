# 第七章：优化算法


---

### 1️⃣ 什么是优化算法？

优化算法是在机器学习模型训练中用于**最小化或最大化目标函数（通常是损失函数）**的算法。核心目标是：找到一组参数，使模型在训练数据上表现最好。

---

### 2️⃣ 常见术语

| 术语         | 含义说明                                                |
|--------------|---------------------------------------------------------|
| 损失函数     | 衡量模型预测与真实标签之间差异的函数（如MSE、交叉熵） |
| 梯度         | 目标函数在参数空间中的导数，用于指明最速下降方向       |
| 学习率       | 控制每次参数更新的步长                                 |
| 局部最小值   | 相对于邻域是最小的值，但可能不是整体最优               |
| 全局最小值   | 在整个函数定义域内最小的值                             |

---

### 3️⃣ 梯度下降法（Gradient Descent）

#### 基本思想：

根据损失函数的梯度，不断沿梯度下降方向更新参数，直到收敛。

#### 更新公式：

$$
\theta := \theta - \eta \cdot \nabla_\theta J(\theta)
$$

其中：

- \( \theta \)：模型参数  
- \( \eta \)：学习率  
- \( J(\theta) \)：损失函数  
- \( \nabla_\theta J(\theta) \)：损失函数对参数的梯度

---

### 4️⃣ 梯度下降的三种变体

| 类型              | 特点                                                   |
|-------------------|--------------------------------------------------------|
| 批量梯度下降（BGD） | 使用全部训练数据计算梯度，稳定但慢                    |
| 随机梯度下降（SGD） | 每次用一个样本更新梯度，噪声大但速度快                |
| 小批量梯度下降（Mini-Batch） | 每次用一小批数据更新梯度，结合了两者优点            |

---

### 5️⃣ 动量法（Momentum）

为了解决 SGD 的震荡问题，引入“速度”概念：

#### 更新公式：

$$
v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta) \\
\theta := \theta - v_t
$$

其中：

- \( \gamma \)：动量系数（如 0.9）  
- \( v_t \)：当前速度

---

### 6️⃣ AdaGrad

对每个参数自适应调整学习率，适合稀疏数据。

#### 更新公式：

$$
G_t = G_{t-1} + \nabla_\theta J(\theta)^2 \\
\theta := \theta - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot \nabla_\theta J(\theta)
$$

---

### 7️⃣ RMSProp

解决 AdaGrad 学习率快速衰减的问题：

#### 更新公式：

$$
E[g^2]_t = \rho E[g^2]_{t-1} + (1 - \rho)(\nabla_\theta J(\theta))^2 \\
\theta := \theta - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \cdot \nabla_\theta J(\theta)
$$

---

### 8️⃣ Adam（Adaptive Moment Estimation）

结合了动量和 RMSProp 的优点，最常用的优化器之一。

#### 更新公式：

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta) \\
v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_\theta J(\theta))^2 \\
\hat{m}_t = \frac{m_t}{1 - \beta_1^t},\quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \\
\theta := \theta - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

推荐超参数：\( \beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8} \)

---

### 9️⃣ 各优化算法对比

| 算法    | 是否适应学习率 | 是否使用动量 | 是否适合稀疏数据 | 收敛速度 | 稳定性 |
|---------|----------------|---------------|------------------|-----------|---------|
| SGD     | ❌             | ❌            | ❌               | 慢        | 差      |
| Momentum | ❌            | ✅            | ❌               | 中等      | 中等    |
| AdaGrad | ✅             | ❌            | ✅               | 慢        | 一般    |
| RMSProp | ✅             | ❌            | ✅               | 快        | 好      |
| Adam    | ✅             | ✅            | ✅               | 快        | 最好    |

---

### 🔟 总结

优化算法是机器学习训练过程中的核心部分，它决定了模型能否有效地从数据中学习到有用的模式。选择合适的优化器可以显著提高训练效果。