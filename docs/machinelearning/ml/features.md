# 第一章： 特征工程

特征工程（Feature Engineering）是指将原始数据转化为更适合机器学习模型使用的特征集的过程。它在机器学习中非常重要，因为好的特征往往比复杂的模型更能提升效果。

换句话说：特征工程就是“**从数据中提炼出最能描述问题的特征**”。

**特征工程**，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，特征工程旨在去除原始数据中的杂志和冗余，
设计更高效的特征以刻画求解的问题与预测模型之间的关系。

----

## 1.1 特征归一化

### 1️⃣ 什么是特征归一化？

特征归一化（Feature Normalization），也叫特征缩放（Feature Scaling），是把不同量纲（取值范围、分布）的特征，统一到相似的数值范围，以便模型更好地收敛和训练。

### 2️⃣ 为什么要归一化？

因为不同特征可能有不同的量纲、取值范围，直接放到模型里可能会导致：

+ 🚀 梯度更新时数值不平衡，影响模型收敛速度
+ 🚀 权重对不同特征的影响程度不同（偏向值大的特征）
+ 🚀 距离度量模型（如KNN、聚类）直接受数值大小影响

简单例子：

+ 特征 A：工资（单位：元），范围（2000, 10000）
+ 特征 B：年龄（单位：岁），范围（18, 60）
  如果不归一化，模型会偏向工资，因为数值大。

### 3️⃣ 常用的归一化方法

#### 📌 (1) Min-Max 归一化（最大最小缩放）

$$
x’ = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

将特征压缩到 [0, 1] 之间。

+ **适合**：数据分布接近均匀，且没有明显异常值。
+ **优点**：保留原始数据分布。
+ **缺点**：对异常值敏感。

#### 📌 (2) Z-Score 标准化（零均值单位方差）

$$
x’ = \frac{x - \mu}{\sigma}
$$

其中：

+ $\mu$ 是均值
+ $\sigma$ 是标准差

转换后的数据均值 0，标准差 1。

优点：适合大多数机器学习模型，对异常值有一定鲁棒性。

常用于：SVM、线性回归、神经网络等。

#### 📌 (3) L2 归一化（向量模归一化）

公式（对样本向量 $\mathbf{x}=[x_1, x_2, …, x_n]$）：

$$
\mathbf{x}’ = \frac{\mathbf{x}}{\|\mathbf{x}\|_2}
$$

将样本向量的欧氏模长缩放为 1，适合距离度量（如文本余弦相似度）。

#### 📌 (4) Robust Scaler（中位数缩放）

对中位数和 IQR（四分位数间距）归一化：

$$
x’ = \frac{x - \text{median}(x)}{\text{IQR}}
$$

优点：对异常值鲁棒性强。
适合数据有异常值的场景。

## 1.2 类别型特征 

### 1️⃣ 什么是类别型特征？

类别型特征是指 取值是离散的、通常代表类别而不是数值大小关系的特征。

🔹 例如：

+ 颜色（红、黄、蓝）
+  性别（男、女）
+ 地区（北京、上海、广州）

类别型特征通常是：

+ 无序类别（Nominal）：如颜色、性别，类别之间没有大小关系。
+ 有序类别（Ordinal）：如教育程度（小学、初中、高中、本科、硕士），类别有顺序，但仍是离散值。
