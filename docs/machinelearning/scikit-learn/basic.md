# scikit-learn æ‰‹å†Œ

## ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€

### 1. Scikit-learn ç®€ä»‹
Scikit-learn æ˜¯åŸºäº Python çš„ç§‘å­¦è®¡ç®—åº“ï¼ˆNumPy/SciPyï¼‰æ„å»ºçš„æœºå™¨å­¦ä¹ åº“ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- ç®€å•é«˜æ•ˆçš„æ•°æ®æŒ–æ˜å’Œæ•°æ®åˆ†æå·¥å…·
- å¼€æºã€å•†ä¸šå¯ç”¨ - BSD è®¸å¯è¯
- å»ºç«‹åœ¨ NumPyã€SciPy å’Œ matplotlib ä¹‹ä¸Š
- æ”¯æŒç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ç®—æ³•

### 2. å®‰è£…ä¸é…ç½®
```bash
# åŸºç¡€å®‰è£…
pip install scikit-learn

# å®Œæ•´å®‰è£…ï¼ˆåŒ…å«ç»˜å›¾åŠŸèƒ½ï¼‰
pip install scikit-learn[alldeps]

# éªŒè¯å®‰è£…
python -c "import sklearn; print(sklearn.__version__)"
```

### 3. åŸºæœ¬å·¥ä½œæµç¨‹


| æ­¥éª¤ | æ–¹æ³• |
|------|------|
| 1ï¸âƒ£ æ•°æ®é¢„å¤„ç† | `preprocessing` |
| 2ï¸âƒ£ åˆ’åˆ†è®­ç»ƒé›†æµ‹è¯•é›† | `train_test_split()` |
| 3ï¸âƒ£ é€‰æ‹©æ¨¡å‹ | `LogisticRegression()`ã€`SVC()` ç­‰ |
| 4ï¸âƒ£ æ¨¡å‹è®­ç»ƒ | `.fit(X_train, y_train)` |
| 5ï¸âƒ£ æ¨¡å‹é¢„æµ‹ | `.predict(X_test)` |
| 6ï¸âƒ£ æ¨¡å‹è¯„ä¼° | `accuracy_score`ã€`classification_report` ç­‰ |

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®
iris = load_iris()
X, y = iris.data, iris.target

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# æ¨¡å‹è®­ç»ƒ
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# æ¨¡å‹è¯„ä¼°
y_pred = clf.predict(X_test)
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.2f}")
```

## ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒæ¨¡å—

| æ¨¡å—å | è¯´æ˜ |
|--------|------|
| `sklearn.datasets` | æä¾›å†…ç½®æ•°æ®é›†ã€æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ã€å¤–éƒ¨æ•°æ®åŠ è½½å·¥å…· |
| `sklearn.model_selection` | æ•°æ®åˆ’åˆ†ã€äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢ç­‰æ¨¡å‹é€‰æ‹©å·¥å…· |
| `sklearn.preprocessing` | ç‰¹å¾ç¼©æ”¾ã€å½’ä¸€åŒ–ã€ç¼–ç ã€ç¼ºå¤±å€¼å¡«å……ç­‰é¢„å¤„ç†æ–¹æ³• |
| `sklearn.linear_model` | å„ç±»çº¿æ€§æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€Lassoã€Ridge |
| `sklearn.tree` | å†³ç­–æ ‘æ¨¡å‹ï¼ŒåŒ…æ‹¬åˆ†ç±»ä¸å›å½’æ ‘ |
| `sklearn.ensemble` | é›†æˆæ–¹æ³•ï¼Œå¦‚éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ã€æŠ•ç¥¨æ³•ç­‰ |
| `sklearn.svm` | æ”¯æŒå‘é‡æœºï¼Œç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ |
| `sklearn.naive_bayes` | æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼Œå¦‚é«˜æ–¯ã€ä¼¯åŠªåˆ©ã€å¤šé¡¹å¼è´å¶æ–¯ |
| `sklearn.neighbors` | Kè¿‘é‚»ç®—æ³•ï¼Œç”¨äºåˆ†ç±»ä¸å›å½’ |
| `sklearn.metrics` | è¯„ä¼°æŒ‡æ ‡æ¨¡å—ï¼Œæ”¯æŒåˆ†ç±»ã€å›å½’ã€èšç±»ç­‰è¯„ä¼° |
| `sklearn.pipeline` | æ„å»ºæ•°æ®å¤„ç† + æ¨¡å‹è®­ç»ƒçš„ä¸€ä½“åŒ–æµç¨‹ |

### 1. sklearn.datasets 

#### ğŸ§  ä¸€ã€datasets æ˜¯ä»€ä¹ˆï¼Ÿ

sklearn.datasets æä¾›äº†ï¼š

+ âœ… å†…ç½®ç»å…¸æ•°æ®é›†ï¼ˆå¦‚é¸¢å°¾èŠ±ã€æ³¢å£«é¡¿æˆ¿ä»·ï¼‰
+ âœ… ä¸‹è½½çœŸå®æ•°æ®é›†ï¼ˆå¦‚ 20 ç±»æ–°é—»æ–‡æœ¬ï¼‰
+ âœ… ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºåˆ†ç±»ã€å›å½’ã€èšç±»ç­‰ï¼‰

#### ğŸ“š äºŒã€å†…ç½®å°å‹æ•°æ®é›†ï¼ˆå¸¸ç”¨äºå…¥é—¨ç»ƒä¹ ï¼‰

| å‡½æ•°å | æè¿° |
|--------|------|
| `load_iris()` | é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆåˆ†ç±»ï¼‰ |
| `load_digits()` | æ‰‹å†™æ•°å­—æ•°æ®é›† |
| `load_diabetes()` | ç³–å°¿ç—…æ•°æ®é›†ï¼ˆå›å½’ï¼‰ |
| `load_wine()` | è‘¡è„é…’åˆ†ç±»æ•°æ® |
| `load_breast_cancer()` | ä¹³è…ºç™ŒäºŒåˆ†ç±»æ•°æ® |
| `load_linnerud()` | äººä½“æŒ‡æ ‡å›å½’æ•°æ® |


#### ğŸŒ ä¸‰ã€ä»å¤–éƒ¨ä¸‹è½½æ•°æ®é›†ï¼ˆçœŸå®ä¸–ç•Œï¼‰

| å‡½æ•°å | æè¿° |
|--------|------|
| `fetch_20newsgroups()` | 20 ç±»æ–°é—»æ–‡æœ¬ï¼ˆNLPï¼‰ |
| `fetch_california_housing()` | åŠ å·æˆ¿ä»·æ•°æ®ï¼ˆå›å½’ï¼‰ |
| `fetch_covtype()` | æ£®æ—è¦†ç›–ç±»å‹æ•°æ®é›† |
| `fetch_olivetti_faces()` | Olivetti äººè„¸è¯†åˆ«å›¾åƒæ•°æ® |
| `fetch_lfw_people()` | LFWï¼ˆLabeled Faces in the Wildï¼‰äººè„¸å›¾åƒ |
| `fetch_lfw_pairs()` | LFW è„¸éƒ¨é…å¯¹å›¾åƒï¼ˆäººè„¸éªŒè¯ï¼‰ |


#### ğŸ§ª å››ã€ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå»ºæ¨¡ä¸æµ‹è¯•ç”¨ï¼‰

| å‡½æ•° | ç”¨é€” |
|------|------|
| `make_classification()` | ç”Ÿæˆç”¨äºåˆ†ç±»çš„æ ·æœ¬æ•°æ® |
| `make_regression()` | ç”Ÿæˆç”¨äºå›å½’çš„æ ·æœ¬æ•°æ® |
| `make_blobs()` | ç”Ÿæˆç”¨äºèšç±»æµ‹è¯•çš„é«˜æ–¯åˆ†å¸ƒæ•°æ® |
| `make_moons()` | ç”ŸæˆåŒæœˆå½¢çŠ¶çš„éçº¿æ€§åˆ†ç±»æ•°æ® |
| `make_circles()` | ç”ŸæˆåŒå¿ƒåœ†å½¢çš„äºŒåˆ†ç±»æ•°æ® |
| `make_multilabel_classification()` | ç”Ÿæˆå¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜æ•°æ® |
| `make_sparse_spd_matrix()` | ç”Ÿæˆç¨€ç–å¯¹ç§°æ­£å®šçŸ©é˜µï¼ˆå¦‚å›¾æ¨¡å‹ç»“æ„ï¼‰ |


### 2. sklearn.model_selection

#### ğŸ“Œ ä¸€ã€model_selection æ˜¯ä»€ä¹ˆï¼Ÿ

è¿™æ˜¯ scikit-learn ä¸­è´Ÿè´£ï¼š

+	æ•°æ®åˆ’åˆ†ï¼ˆè®­ç»ƒé›†/æµ‹è¯•é›†ï¼‰
+	æ¨¡å‹è¯„ä¼°ï¼ˆäº¤å‰éªŒè¯ï¼‰
+	è¶…å‚æ•°æœç´¢ï¼ˆç½‘æ ¼æœç´¢ã€éšæœºæœç´¢ï¼‰
+	æ¨¡å‹é€‰æ‹©ä¸éªŒè¯ç­–ç•¥çš„æ ¸å¿ƒæ¨¡å—


#### ğŸ§© äºŒã€å¸¸ç”¨åŠŸèƒ½åˆ†ç±»ä¸ä½œç”¨

| åŠŸèƒ½ç±»åˆ« | å¸¸ç”¨å‡½æ•° | ä½œç”¨ |
|----------|----------|------|
| æ•°æ®é›†åˆ’åˆ† | `train_test_split` | è®­ç»ƒé›† / æµ‹è¯•é›†åˆ’åˆ† |
| äº¤å‰éªŒè¯ | `cross_val_score`, `cross_validate`, `KFold`, `StratifiedKFold` | å¤šæŠ˜è¯„ä¼°æ¨¡å‹ |
| è¶…å‚æ•°æœç´¢ | `GridSearchCV`, `RandomizedSearchCV` | ç½‘æ ¼/éšæœºæœç´¢å‚æ•° |
| å­¦ä¹ æ›²çº¿ | `learning_curve`, `validation_curve` | æ¨¡å‹å­¦ä¹ è¿‡ç¨‹å¯è§†åŒ– |
| é¢„å®šä¹‰éªŒè¯ | `ShuffleSplit`, `LeaveOneOut` ç­‰ | æ§åˆ¶éªŒè¯é›†åˆ’åˆ†æ–¹å¼ |


#### âœ‚ï¸ ä¸‰ã€æ•°æ®åˆ’åˆ†ï¼štrain_test_split()

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify=y)
```

+	test_sizeï¼šæµ‹è¯•é›†æ¯”ä¾‹ï¼ˆå¦‚ 0.2ï¼‰
+	stratify=yï¼šæŒ‰æ ‡ç­¾æ¯”ä¾‹åˆ†å±‚é‡‡æ ·ï¼ˆåˆ†ç±»å¸¸ç”¨ï¼‰


#### ğŸ” å››ã€äº¤å‰éªŒè¯

##### 1ï¸âƒ£ cross_val_score()

å¿«é€Ÿè¯„ä¼°æ¨¡å‹äº¤å‰éªŒè¯å¾—åˆ†ï¼š

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

scores = cross_val_score(LogisticRegression(), X, y, cv=5)
print("å¹³å‡å‡†ç¡®ç‡ï¼š", scores.mean())
```

##### 2ï¸âƒ£ cross_validate()ï¼ˆæ”¯æŒæ›´å¤šè¾“å‡ºï¼‰

```python
from sklearn.model_selection import cross_validate

result = cross_validate(LogisticRegression(), X, y,
                        scoring=['accuracy', 'f1_macro'],
                        return_train_score=True,
                        cv=5)
print(result)
```

#### ğŸ”€ äº”ã€äº¤å‰éªŒè¯ç­–ç•¥ç±»ï¼ˆKFold ç­‰ï¼‰

| ç±»å | è¯´æ˜ |
|------|------|
| `KFold` | ç®€å•å‡åŒ€åˆ’åˆ†ä¸º K æŠ˜ |
| `StratifiedKFold` | ä¿æŒç±»åˆ«åˆ†å¸ƒçš„ K æŠ˜ï¼ˆé€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼‰ |
| `ShuffleSplit` | å¤šæ¬¡éšæœºåˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›† |
| `LeaveOneOut` | ç•™ä¸€æ³•ï¼ˆæ¯æ¬¡ç•™ä¸€ä¸ªæ ·æœ¬åšæµ‹è¯•é›†ï¼‰ |
| `GroupKFold` | æŒ‰ç»„åˆ’åˆ†ï¼Œç¡®ä¿åŒä¸€ç»„æ•°æ®ä¸åœ¨è®­ç»ƒå’ŒéªŒè¯é›†ä¸­åŒæ—¶å‡ºç° |


```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5)
for train_idx, test_idx in skf.split(X, y):
    print(train_idx, test_idx)
```

#### ğŸ” å…­ã€è¶…å‚æ•°æœç´¢

##### âœ… GridSearchCVï¼ˆç½‘æ ¼æœç´¢ï¼‰

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid = GridSearchCV(SVC(), param_grid, cv=5)
grid.fit(X_train, y_train)

print("æœ€ä¼˜å‚æ•°ï¼š", grid.best_params_)
print("æœ€ä¼˜å¾—åˆ†ï¼š", grid.best_score_)
```

##### âœ… RandomizedSearchCVï¼ˆéšæœºæœç´¢ï¼‰

æ›´é€‚åˆå¤§æœç´¢ç©ºé—´ï¼š

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

param_dist = {'C': uniform(0.1, 10)}
rand_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=10, cv=5)
rand_search.fit(X_train, y_train)
```

#### ğŸ“ˆ ä¸ƒã€å­¦ä¹ æ›²çº¿ & éªŒè¯æ›²çº¿ï¼ˆå¯è§†åŒ–æ¨¡å‹å­¦ä¹ èƒ½åŠ›ï¼‰

##### å­¦ä¹ æ›²çº¿ï¼šéšç€æ ·æœ¬é‡å¢åŠ çš„æ€§èƒ½å˜åŒ–

```python
from sklearn.model_selection import learning_curve
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

train_sizes, train_scores, test_scores = learning_curve(LogisticRegression(), X, y, cv=5)

plt.plot(train_sizes, train_scores.mean(axis=1), label='train')
plt.plot(train_sizes, test_scores.mean(axis=1), label='test')
plt.legend()
plt.title("Learning Curve")
plt.show()
```

##### éªŒè¯æ›²çº¿ï¼šéšç€è¶…å‚æ•°å˜åŒ–æ¨¡å‹æ€§èƒ½å¦‚ä½•

```python
from sklearn.model_selection import validation_curve

param_range = [0.01, 0.1, 1, 10]
train_scores, test_scores = validation_curve(
    LogisticRegression(), X, y,
    param_name="C", param_range=param_range,
    cv=5)

plt.plot(param_range, train_scores.mean(axis=1), label="Train")
plt.plot(param_range, test_scores.mean(axis=1), label="Test")
plt.xscale('log')
plt.title("Validation Curve")
plt.legend()
plt.show()
```

### 3. sklearn.preprocessing

#### âœ… ä¸€ã€æ¨¡å—ä½œç”¨æ¦‚è¿°

`sklearn.preprocessing` æä¾›å„ç§æ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼š

| é¢„å¤„ç†ç±»åˆ« | å…¸å‹æ“ä½œ |
|------------|----------|
| ç‰¹å¾ç¼©æ”¾ | æ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ã€æœ€å¤§æœ€å°ç¼©æ”¾ |
| éæ•°å€¼ç‰¹å¾ç¼–ç  | LabelEncodingã€OneHotEncoding |
| éçº¿æ€§è½¬æ¢ | å¯¹æ•°ã€å¹‚æ¬¡ã€åˆ†ä½æ•°å˜æ¢ |
| ç¼ºå¤±å€¼å¤„ç† | æ’å€¼ã€å¡«å…… |

#### ğŸ“Š äºŒã€å¸¸ç”¨é¢„å¤„ç†å™¨æ€»è§ˆ

| åç§° | ç±»/å‡½æ•° | ç”¨é€” |
|------|---------|------|
| æ ‡å‡†åŒ– | `StandardScaler` | è½¬æ¢ä¸ºå‡å€¼ 0 æ–¹å·® 1 |
| å½’ä¸€åŒ– | `MinMaxScaler` | ç¼©æ”¾åˆ° [0, 1] åŒºé—´ |
| æœ€å¤§ç»å¯¹ç¼©æ”¾ | `MaxAbsScaler` | ç¼©æ”¾åˆ° [-1, 1] |
| ç¨€ç–ç¼©æ”¾ | `RobustScaler` | ç¼©æ”¾ä¸å—å¼‚å¸¸å€¼å½±å“ |
| å•ä½å‘é‡åŒ– | `Normalizer` | å°†æ¯è¡Œæ ·æœ¬ç¼©æ”¾ä¸ºå•ä½èŒƒæ•° |
| ç±»åˆ«ç¼–ç  | `LabelEncoder`, `OneHotEncoder` | æ ‡ç­¾ç¼–ç æˆ–ç‹¬çƒ­ç¼–ç  |
| ç¼ºå¤±å€¼å¡«å…… | `SimpleImputer` | ç”¨å‡å€¼/ä¸­ä½æ•°/ä¼—æ•°å¡«è¡¥ç¼ºå¤± |
| åŠŸèƒ½è½¬æ¢ | `FunctionTransformer`, `PowerTransformer`, `QuantileTransformer` | è‡ªå®šä¹‰å˜æ¢ã€Box-Cox/Yeo-Johnsonã€åˆ†ä½æ•°å˜æ¢ |


#### ğŸ“ ä¸‰ã€å¸¸ç”¨ç¼©æ”¾ç±»è¯¦è§£

##### 1ï¸âƒ£ StandardScalerï¼šæ ‡å‡†åŒ–

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

##### 2ï¸âƒ£ MinMaxScalerï¼šå½’ä¸€åŒ–

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```

##### 3ï¸âƒ£ RobustScalerï¼šæŠ—å¼‚å¸¸å€¼ç¼©æ”¾

```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
```

##### 4ï¸âƒ£ Normalizerï¼šå•ä½èŒƒæ•°ç¼©æ”¾ï¼ˆè¡Œæ–¹å‘ï¼‰

```python
from sklearn.preprocessing import Normalizer

scaler = Normalizer()
X_scaled = scaler.fit_transform(X)
```

#### ğŸ”¤ å››ã€ç±»åˆ«å˜é‡ç¼–ç å™¨

##### LabelEncoderï¼šæ ‡ç­¾ç¼–ç 

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_encoded = le.fit_transform(['yes', 'no', 'yes', 'no'])
```

##### OneHotEncoderï¼šç‹¬çƒ­ç¼–ç 

```python
from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(sparse=False)
X = [['Male'], ['Female'], ['Male']]
X_encoded = enc.fit_transform(X)
```

#### ğŸ§ª äº”ã€ç¼ºå¤±å€¼å¤„ç†

##### SimpleImputerï¼šå‡å€¼/ä¸­ä½æ•°å¡«è¡¥

```python
from sklearn.impute import SimpleImputer
import numpy as np

imp = SimpleImputer(strategy='mean')
X = np.array([[1, 2], [np.nan, 3], [7, 6]])
X_filled = imp.fit_transform(X)
```

#### ğŸ” å…­ã€éçº¿æ€§å˜æ¢

##### PowerTransformerï¼šBox-Cox / Yeo-Johnson

```python
from sklearn.preprocessing import PowerTransformer

pt = PowerTransformer(method='yeo-johnson')
X_transformed = pt.fit_transform(X)
```

##### QuantileTransformerï¼šåˆ†ä½æ•°å˜æ¢

```python
from sklearn.preprocessing import QuantileTransformer

qt = QuantileTransformer(output_distribution='normal')
X_transformed = qt.fit_transform(X)
```

#### ğŸ§¾ ä¸ƒã€å°ç»“ï¼šé¢„å¤„ç†å™¨é€‰æ‹©å»ºè®®è¡¨

| åœºæ™¯ | æ¨èæ–¹æ³• |
|------|-----------|
| æ•°æ®æ ‡å‡†åŒ– | `StandardScaler` |
| å¼‚å¸¸å€¼å­˜åœ¨ | `RobustScaler` |
| ç‰¹å¾åœ¨ä¸åŒé‡çº² | `MinMaxScaler` |
| æ ·æœ¬ä¸ºå‘é‡æ•°æ® | `Normalizer` |
| åˆ†ç±»æ ‡ç­¾è½¬æ¢ | `LabelEncoder`, `OneHotEncoder` |
| æ•°æ®ç¼ºå¤± | `SimpleImputer` |
| ç‰¹å¾éæ­£æ€åˆ†å¸ƒ | `PowerTransformer`, `QuantileTransformer` |


#### âœ… å…«ã€é…åˆ Pipeline ä½¿ç”¨


```python
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegression())
])
pipe.fit(X_train, y_train)
```

### 4. sklearn.metrics


#### âœ… ä¸€ã€æ¨¡å—ç”¨é€”

`sklearn.metrics` æ˜¯ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æ¨¡å—ï¼Œæ”¯æŒï¼š

- åˆ†ç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1 å€¼ï¼‰
- å›å½’ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡æ–¹è¯¯å·®ã€RÂ²ï¼‰
- èšç±»è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚è½®å»“ç³»æ•°ã€ARIï¼‰
- å¤šæ ‡ç­¾ã€å¤šè¾“å‡ºã€å¤šåˆ†ç±»ä»»åŠ¡æ”¯æŒ

#### ğŸ¯ äºŒã€åˆ†ç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡

| å‡½æ•° | å«ä¹‰ | ç”¨æ³•ä¸¾ä¾‹ |
|------|------|----------|
| `accuracy_score()` | å‡†ç¡®ç‡ | åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•° |
| `precision_score()` | ç²¾ç¡®ç‡ | æ­£ç±»é¢„æµ‹ä¸­æ­£ç¡®çš„æ¯”ä¾‹ |
| `recall_score()` | å¬å›ç‡ | å®é™…æ­£ç±»ä¸­è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹ |
| `f1_score()` | F1 å€¼ | ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ |
| `classification_report()` | åˆ†ç±»æ•´ä½“æŠ¥å‘Š | å¤šä¸ªæŒ‡æ ‡ç»¼åˆ |
| `confusion_matrix()` | æ··æ·†çŸ©é˜µ | TP/FP/FN/TN |
| `roc_auc_score()` | ROC æ›²çº¿ä¸‹çš„é¢ç§¯ | äºŒåˆ†ç±»æ€§èƒ½ |
| `log_loss()` | å¯¹æ•°æŸå¤± | æ¦‚ç‡é¢„æµ‹è¯¯å·® |


```python
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

y_true = [0, 1, 1, 1, 0, 0, 1]
y_pred = [0, 1, 0, 1, 0, 1, 1]

print("å‡†ç¡®ç‡ï¼š", accuracy_score(y_true, y_pred))
print("F1 å€¼ï¼š", f1_score(y_true, y_pred))
print("æŠ¥å‘Šï¼š\n", classification_report(y_true, y_pred))
print("æ··æ·†çŸ©é˜µï¼š\n", confusion_matrix(y_true, y_pred))
```

#### ğŸ“‰ ä¸‰ã€å›å½’ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡


| å‡½æ•° | å«ä¹‰ | ç”¨æ³• |
|------|------|------|
| `mean_squared_error()` | å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ | å‡æ–¹è¯¯å·®è¶Šå°è¶Šå¥½ |
| `mean_absolute_error()` | å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ | æ›´å°‘å—ç¦»ç¾¤ç‚¹å½±å“ |
| `r2_score()` | å†³å®šç³»æ•° RÂ² | è¶Šæ¥è¿‘ 1 è¶Šå¥½ |
| `mean_squared_log_error()` | å‡æ–¹å¯¹æ•°è¯¯å·® | é€‚åˆå¯¹æ•°å›å½’é—®é¢˜ |

```python
from sklearn.metrics import mean_squared_error, r2_score

y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]

print("MSE:", mean_squared_error(y_true, y_pred))
print("RÂ²:", r2_score(y_true, y_pred))
```

#### ğŸ§  å››ã€èšç±»ä»»åŠ¡è¯„ä¼°æŒ‡æ ‡

| å‡½æ•° | å«ä¹‰ | ç‰¹ç‚¹ |
|------|------|------|
| `adjusted_rand_score()` | è°ƒæ•´åçš„å…°å¾·æŒ‡æ•°ï¼ˆARIï¼‰ | æ— ç›‘ç£èšç±»æ ‡ç­¾ç›¸ä¼¼æ€§ |
| `normalized_mutual_info_score()` | å½’ä¸€åŒ–äº’ä¿¡æ¯ | èšç±»æ ‡ç­¾ä¸€è‡´æ€§ |
| `homogeneity_score()` | åŒè´¨æ€§è¯„åˆ† | æ¯ä¸ªèšç±»åªåŒ…å«ä¸€ç§æ ‡ç­¾ |
| `completeness_score()` | å®Œæ•´æ€§è¯„åˆ† | æ¯ç§æ ‡ç­¾åªåœ¨ä¸€ä¸ªèšç±»ä¸­ |
| `silhouette_score()` | è½®å»“ç³»æ•° | ç”¨äºåº¦é‡èšç±»åˆ†ç¦»ç¨‹åº¦ |


```python
from sklearn.metrics import adjusted_rand_score

labels_true = [0, 0, 1, 1, 2, 2]
labels_pred = [0, 0, 1, 2, 2, 2]

print("ARI:", adjusted_rand_score(labels_true, labels_pred))
```

#### ğŸ§¾ äº”ã€å¸¸ç”¨æŒ‡æ ‡é€ŸæŸ¥è¡¨

##### ğŸ“Š åˆ†ç±»æŒ‡æ ‡é€ŸæŸ¥è¡¨

| æŒ‡æ ‡ | å‡½æ•° | è¯´æ˜ |
|------|------|------|
| å‡†ç¡®ç‡ | `accuracy_score` | æ€»ä½“åˆ†ç±»æ­£ç¡®ç‡ |
| ç²¾ç¡®ç‡ | `precision_score` | æ­£ç±»ä¸­é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ |
| å¬å›ç‡ | `recall_score` | å®é™…æ­£ç±»ä¸­è¢«é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ |
| F1 å€¼ | `f1_score` | å¹³è¡¡ç²¾ç¡®ç‡ä¸å¬å›ç‡ |
| æ··æ·†çŸ©é˜µ | `confusion_matrix` | é¢„æµ‹ vs çœŸå®æ ‡ç­¾ |
| åˆ†ç±»æŠ¥å‘Š | `classification_report` | å¤šä¸ªæŒ‡æ ‡æ±‡æ€»å±•ç¤º |


##### ğŸ“ˆ å›å½’æŒ‡æ ‡é€ŸæŸ¥è¡¨

| æŒ‡æ ‡ | å‡½æ•° | è¯´æ˜ |
|------|------|------|
| MSE | `mean_squared_error` | å¯¹è¯¯å·®å¹³æ–¹çš„å¹³å‡å€¼ |
| MAE | `mean_absolute_error` | ç»å¯¹è¯¯å·®çš„å¹³å‡å€¼ |
| RÂ² | `r2_score` | å›å½’æ‹Ÿåˆä¼˜åº¦ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¥½ |

##### ğŸ§  èšç±»æŒ‡æ ‡é€ŸæŸ¥è¡¨

| æŒ‡æ ‡ | å‡½æ•° | è¯´æ˜ |
|------|------|------|
| ARI | `adjusted_rand_score` | èšç±»æ ‡ç­¾ä¸€è‡´æ€§ |
| NMI | `normalized_mutual_info_score` | æ ‡ç­¾ä¿¡æ¯å…±äº«ç¨‹åº¦ |
| Homogeneity | `homogeneity_score` | æ¯ä¸ªç°‡æ˜¯å¦åªåŒ…å«ä¸€ä¸ªç±» |
| Completeness | `completeness_score` | æ¯ä¸ªç±»æ˜¯å¦é›†ä¸­åœ¨ä¸€ä¸ªç°‡ |
| Silhouette | `silhouette_score` | èšç±»çš„åˆ†ç¦»æ€§å’Œç´§å¯†åº¦ |

#### ğŸ§  å…­ã€å»ºè®®ä½¿ç”¨åœºæ™¯æ€»ç»“

| ä»»åŠ¡ç±»å‹ | æ¨èæŒ‡æ ‡ |
|----------|----------|
| äºŒåˆ†ç±» | å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ã€AUC |
| å¤šåˆ†ç±» | åˆ†ç±»æŠ¥å‘Šã€æ··æ·†çŸ©é˜µ |
| å›å½’ | MSEã€MAEã€RÂ² |
| èšç±» | ARIã€NMIã€è½®å»“ç³»æ•° |

#### âœ… ä¸ƒã€è¡¥å……è¯´æ˜ï¼šå¤šæ ‡ç­¾/å¤šåˆ†ç±»è®¾ç½®å‚æ•°

- `average='binary'`ï¼šé»˜è®¤ï¼Œç”¨äºäºŒåˆ†ç±»
- `average='micro'`ï¼šå…¨å±€ç´¯è®¡ TP/FP/FN
- `average='macro'`ï¼šå„ç±»åˆ«æŒ‡æ ‡ç®€å•å¹³å‡
- `average='weighted'`ï¼šåŠ æƒå¹³å‡ï¼ˆæŒ‰æ”¯æŒåº¦ï¼‰
- `labels=`ï¼šæŒ‡å®šå‚ä¸è®¡ç®—çš„æ ‡ç­¾ç±»åˆ«ç´¢å¼•


### 5. sklearn.linear_model

