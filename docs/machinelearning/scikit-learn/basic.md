# scikit-learn åŸºç¡€æ•™ç¨‹

## ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€å…¥é—¨

### 1. Scikit-learn ç®€ä»‹
Scikit-learn æ˜¯åŸºäº Python çš„ç§‘å­¦è®¡ç®—åº“ï¼ˆNumPy/SciPyï¼‰æ„å»ºçš„æœºå™¨å­¦ä¹ åº“ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- ç®€å•é«˜æ•ˆçš„æ•°æ®æŒ–æ˜å’Œæ•°æ®åˆ†æå·¥å…·
- å¼€æºã€å•†ä¸šå¯ç”¨ - BSD è®¸å¯è¯
- å»ºç«‹åœ¨ NumPyã€SciPy å’Œ matplotlib ä¹‹ä¸Š
- æ”¯æŒç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ç®—æ³•

### 2. å®‰è£…ä¸é…ç½®
```bash
# åŸºç¡€å®‰è£…
pip install scikit-learn

# å®Œæ•´å®‰è£…ï¼ˆåŒ…å«ç»˜å›¾åŠŸèƒ½ï¼‰
pip install scikit-learn[alldeps]

# éªŒè¯å®‰è£…
python -c "import sklearn; print(sklearn.__version__)"
```

### 3. åŸºæœ¬å·¥ä½œæµç¨‹
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®
iris = load_iris()
X, y = iris.data, iris.target

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# æ¨¡å‹è®­ç»ƒ
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# æ¨¡å‹è¯„ä¼°
y_pred = clf.predict(X_test)
print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.2f}")
```

## ç¬¬äºŒéƒ¨åˆ†ï¼šä¸­çº§åº”ç”¨

### 1. æ•°æ®é¢„å¤„ç†è¿›é˜¶
#### ç®¡é“(Pipeline)ä½¿ç”¨
```python
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# æ•°å€¼å‹ç‰¹å¾å¤„ç†
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# ç±»åˆ«å‹ç‰¹å¾å¤„ç†
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# ç»„åˆå¤„ç†
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, ['age', 'income']),
        ('cat', categorical_transformer, ['gender', 'city'])
    ])
```

#### ç‰¹å¾é€‰æ‹©
```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=5)
X_new = selector.fit_transform(X, y)

# æŸ¥çœ‹é€‰æ‹©çš„ç‰¹å¾
selected_features = [iris.feature_names[i] for i in selector.get_support(indices=True)]
print("é€‰æ‹©çš„ç‰¹å¾:", selected_features)
```

### 2. æ¨¡å‹è°ƒä¼˜æŠ€æœ¯
#### ç½‘æ ¼æœç´¢ä¸éšæœºæœç´¢
```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC
from scipy.stats import uniform

# ç½‘æ ¼æœç´¢
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'linear']
}
grid_search = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# éšæœºæœç´¢
param_dist = {
    'C': uniform(loc=0, scale=4),
    'gamma': uniform(loc=0, scale=1),
    'kernel': ['rbf', 'linear']
}
random_search = RandomizedSearchCV(SVC(), param_dist, n_iter=100, cv=5, n_jobs=-1)
random_search.fit(X_train, y_train)
```

#### äº¤å‰éªŒè¯ç­–ç•¥
```python
from sklearn.model_selection import (KFold, StratifiedKFold, 
                                   TimeSeriesSplit, 
                                   cross_val_score)

# æ™®é€šKæŠ˜äº¤å‰éªŒè¯
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯ï¼ˆé€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼‰
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
tscv = TimeSeriesSplit(n_splits=5)

# ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹
scores = cross_val_score(RandomForestClassifier(), X, y, cv=skf, scoring='accuracy')
print(f"äº¤å‰éªŒè¯å¹³å‡å‡†ç¡®ç‡: {scores.mean():.2f} Â± {scores.std():.2f}")
```

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§ç²¾é€š

### 1. è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
```python
from sklearn.metrics import make_scorer

def custom_metric(y_true, y_pred):
    """è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ç¤ºä¾‹"""
    true_pos = sum((y_true == 1) & (y_pred == 1))
    false_pos = sum((y_true == 0) & (y_pred == 1))
    return true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0

custom_scorer = make_scorer(custom_metric)

# åœ¨ç½‘æ ¼æœç´¢ä¸­ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡
grid_search = GridSearchCV(
    SVC(), param_grid, cv=5, scoring=custom_scorer
)
```

### 2. è‡ªå®šä¹‰è½¬æ¢å™¨
```python
from sklearn.base import BaseEstimator, TransformerMixin

class LogTransformer(BaseEstimator, TransformerMixin):
    """å¯¹æ•°å˜æ¢è‡ªå®šä¹‰è½¬æ¢å™¨"""
    
    def __init__(self, add_one=True):
        self.add_one = add_one
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        if self.add_one:
            return np.log1p(X)
        else:
            return np.log(X)

# åœ¨ç®¡é“ä¸­ä½¿ç”¨
pipeline = Pipeline([
    ('log', LogTransformer()),
    ('scaler', StandardScaler()),
    ('clf', RandomForestClassifier())
])
```

### 3. é›†æˆå­¦ä¹ ä¸å †å 
```python
from sklearn.ensemble import (VotingClassifier, 
                            StackingClassifier)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

# æŠ•ç¥¨åˆ†ç±»å™¨
voting_clf = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression(max_iter=1000)),
        ('svm', SVC(probability=True)),
        ('dt', DecisionTreeClassifier())
    ],
    voting='soft'  # 'hard' æˆ– 'soft'
)

# å †å åˆ†ç±»å™¨
stacking_clf = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression()),
        ('svm', SVC(probability=True)),
        ('dt', DecisionTreeClassifier())
    ],
    final_estimator=LogisticRegression(),
    stack_method='auto',
    n_jobs=-1
)
```

### 4. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
```python
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline

# è®¡ç®—ç±»åˆ«æƒé‡
classes = np.unique(y)
weights = compute_class_weight('balanced', classes=classes, y=y)
class_weight = dict(zip(classes, weights))

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
model = RandomForestClassifier(class_weight=class_weight)

# ä½¿ç”¨SMOTEè¿‡é‡‡æ ·
smote_pipeline = make_pipeline(
    SMOTE(random_state=42),
    RandomForestClassifier()
)
```

## ç¬¬å››éƒ¨åˆ†ï¼šå®æˆ˜é¡¹ç›®

### é¡¹ç›®1ï¼šå®¢æˆ·æµå¤±é¢„æµ‹
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, roc_auc_score

# åŠ è½½æ•°æ®
data = pd.read_csv('customer_churn.csv')

# ç‰¹å¾å·¥ç¨‹
X = data.drop(['customerID', 'Churn'], axis=1)
y = data['Churn'].map({'Yes': 1, 'No': 0})

# æ•°æ®é¢„å¤„ç†
X = pd.get_dummies(X, drop_first=True)

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# è®­ç»ƒæ¨¡å‹
gbm = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    random_state=42
)
gbm.fit(X_train, y_train)

# è¯„ä¼°
y_pred = gbm.predict(X_test)
y_proba = gbm.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred))
print(f"AUCåˆ†æ•°: {roc_auc_score(y_test, y_proba):.2f}")

# ç‰¹å¾é‡è¦æ€§
pd.Series(gbm.feature_importances_, index=X.columns).sort_values().plot(kind='barh')
```

### é¡¹ç›®2ï¼šæ—¶é—´åºåˆ—é¢„æµ‹
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error

# åˆ›å»ºæ—¶é—´åºåˆ—ç‰¹å¾
def create_features(df, lags=5):
    for i in range(1, lags+1):
        df[f'lag_{i}'] = df['value'].shift(i)
    return df.dropna()

# å‡†å¤‡æ•°æ®
series = ...  # ä½ çš„æ—¶é—´åºåˆ—æ•°æ®
df = create_features(series)

X = df.drop('value', axis=1)
y = df['value']

# å¤šæ­¥é¢„æµ‹
X_train, X_test = X.iloc[:-100], X.iloc[-100:]
y_train, y_test = y.iloc[:-100], y.iloc[-100:]

# ä½¿ç”¨å¤šè¾“å‡ºå›å½’å™¨è¿›è¡Œå¤šæ­¥é¢„æµ‹
model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100))
model.fit(X_train, y_train)

# è¯„ä¼°
y_pred = model.predict(X_test)
print(f"MAE: {mean_absolute_error(y_test, y_pred)}")
```

## ç¬¬äº”éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–

### 1. å¹¶è¡Œå¤„ç†
```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier

# ä½¿ç”¨n_jobså‚æ•°
rf = RandomForestClassifier(n_estimators=200, n_jobs=-1)  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ

# ä½¿ç”¨HistGradientBoostingï¼ˆå†…å­˜æ•ˆç‡æ›´é«˜ï¼‰
hgb = HistGradientBoostingClassifier(
    max_iter=200,
    learning_rate=0.05,
    max_depth=5,
    random_state=42
)
```

### 2. å¢é‡å­¦ä¹ 
```python
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB

# é€‚ç”¨äºå¤§æ•°æ®é›†çš„å¢é‡å­¦ä¹ 
sgd = SGDClassifier(loss='log', max_iter=1000, tol=1e-3)

# åˆ†æ‰¹è®­ç»ƒ
for batch in pd.read_csv('large_data.csv', chunksize=1000):
    X_batch = batch.drop('target', axis=1)
    y_batch = batch['target']
    sgd.partial_fit(X_batch, y_batch, classes=np.unique(y))
```

### 3. æ¨¡å‹æŒä¹…åŒ–
```python
import joblib

# ä¿å­˜æ¨¡å‹
joblib.dump(model, 'model.joblib')

# åŠ è½½æ¨¡å‹
loaded_model = joblib.load('model.joblib')

# ä¿å­˜æ•´ä¸ªpipeline
joblib.dump(pipeline, 'pipeline.joblib')
```

## ç¬¬å…­éƒ¨åˆ†ï¼šæœ€ä½³å®è·µ

1. **ç‰¹å¾å·¥ç¨‹æ¯”ç®—æ³•é€‰æ‹©æ›´é‡è¦**ï¼šæ•°æ®è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™
2. **ä»ç®€å•æ¨¡å‹å¼€å§‹**ï¼šå…ˆå°è¯•çº¿æ€§æ¨¡å‹ï¼Œå†é€æ­¥å¤æ‚åŒ–
3. **ç†è§£ä¸šåŠ¡éœ€æ±‚é€‰æ‹©æŒ‡æ ‡**ï¼šå‡†ç¡®ç‡ä¸æ˜¯ä¸‡èƒ½çš„
4. **æ¨¡å‹å¯è§£é‡Šæ€§**ï¼šä½¿ç”¨SHAPæˆ–LIMEè§£é‡Šæ¨¡å‹å†³ç­–
5. **ç›‘æ§æ¨¡å‹æ€§èƒ½**ï¼šå»ºç«‹æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é¢„è­¦æœºåˆ¶


## ç¬¬ä¸ƒéƒ¨åˆ†ï¼š`sklearn.datasets` æ¨¡å—è¯¦è§£

`sklearn.datasets` æ˜¯ Scikit-learn ä¸­çš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒæä¾›äº†**åŠ è½½å¸¸ç”¨æ•°æ®é›†ã€ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†ã€ä»å¤–éƒ¨æ•°æ®æºå¯¼å…¥æ•°æ®é›†**çš„åŠŸèƒ½ï¼Œéå¸¸é€‚åˆç”¨äºå­¦ä¹ å’Œæµ‹è¯•æœºå™¨å­¦ä¹ ç®—æ³•ã€‚


### ä¸€ã€å¸¸ç”¨æ•°æ®é›†åŠ è½½å‡½æ•°ï¼ˆ`load_*`ï¼‰

è¿™äº›å‡½æ•°åŠ è½½çš„æ˜¯**å†…ç½®çš„æ ‡å‡†å°å‹æ•°æ®é›†**ï¼Œé€šå¸¸ä½œä¸ºå­¦ä¹ å’Œæµ‹è¯•ç”¨ï¼š

| å‡½æ•°å | æè¿° |
|--------|------|
| `load_iris()` | é¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆåˆ†ç±»ï¼‰ |
| `load_digits()` | æ‰‹å†™æ•°å­—æ•°æ®é›†ï¼ˆåˆ†ç±»ï¼‰ |
| `load_wine()` | è‘¡è„é…’åˆ†ç±»æ•°æ®é›† |
| `load_breast_cancer()` | ä¹³è…ºç™Œè¯Šæ–­æ•°æ®é›† |
| `load_diabetes()` | ç³–å°¿ç—…æ•°æ®é›†ï¼ˆå›å½’ï¼‰ |
| `load_linnerud()` | ä½“èƒ½è®­ç»ƒæ•°æ®é›†ï¼ˆå¤šè¾“å‡ºå›å½’ï¼‰ |

**è¿”å›ç±»å‹**ï¼š`Bunch` å¯¹è±¡ï¼Œç±»ä¼¼å­—å…¸ï¼ˆæœ‰ `.data`ã€`.target`ã€`.feature_names`ã€`.DESCR`ï¼‰

#### âœ… ç¤ºä¾‹ä»£ç 

```python
from sklearn.datasets import load_iris

iris = load_iris()
print(iris.data.shape)         # ç‰¹å¾æ•°æ®
print(iris.target.shape)       # æ ‡ç­¾æ•°æ®
print(iris.feature_names)      # ç‰¹å¾åç§°
print(iris.target_names)       # æ ‡ç­¾åç§°
```

### äºŒã€æ•°æ®é›†ç”Ÿæˆå‡½æ•°ï¼ˆmake_*ï¼‰


ç”¨äºç”Ÿæˆ**å¯æ§ç‰¹å¾çš„æ¨¡æ‹Ÿæ•°æ®é›†**ï¼Œå¸¸ç”¨äºæµ‹è¯•ç®—æ³•æ€§èƒ½å’Œå¯è§†åŒ–ã€‚


| å‡½æ•°å | æè¿° |
|--------|------|
| `make_classification()` | ç”Ÿæˆåˆ†ç±»é—®é¢˜æ•°æ® |
| `make_regression()` | ç”Ÿæˆå›å½’é—®é¢˜æ•°æ® |
| `make_blobs()` | ç”Ÿæˆèšç±»é—®é¢˜æ•°æ® |
| `make_moons()` | ç”Ÿæˆæœˆç‰™å‹åˆ†ç±»æ•°æ® |
| `make_circles()` | ç”Ÿæˆåœ†ç¯å‹åˆ†ç±»æ•°æ® |
| `make_multilabel_classification()` | å¤šæ ‡ç­¾åˆ†ç±»æ•°æ® |
| `make_sparse_coded_signal()` | ç¨€ç–ä¿¡å·æ•°æ®ï¼ˆç”¨äºç¨€ç–å­¦ä¹ ï¼‰ |

#### âœ… ç¤ºä¾‹ä»£ç 

```python
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# ç”Ÿæˆåˆ†ç±»æ•°æ®
X, y = make_classification(
    n_samples=100,     # æ ·æœ¬æ•°é‡
    n_features=2,      # ç‰¹å¾æ•°é‡
    n_redundant=0,     # å†—ä½™ç‰¹å¾æ•°é‡
    n_classes=2        # ç±»åˆ«æ•°é‡
)

# å¯è§†åŒ–åˆ†ç±»æ•°æ®
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.title("Classification Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

###  ä¸‰ã€å¤–éƒ¨å¤§å‹æ•°æ®é›†åŠ è½½ï¼ˆfetch_*ï¼‰

é€‚ç”¨äº**ä¸‹è½½å¤§å‹çœŸå®æ•°æ®é›†**ï¼ˆéœ€è”ç½‘ï¼Œé¦–æ¬¡ä¸‹è½½åç¼“å­˜æœ¬åœ°ï¼‰ã€‚

| å‡½æ•°å | æè¿° |
|--------|------|
| `fetch_20newsgroups()` | æ–°é—»æ–‡æœ¬åˆ†ç±»æ•°æ® |
| `fetch_olivetti_faces()` | äººè„¸è¯†åˆ«æ•°æ® |
| `fetch_lfw_people()` | LFWäººè„¸æ•°æ®é›† |
| `fetch_covtype()` | è¦†ç›–ç±»å‹æ•°æ®ï¼ˆåˆ†ç±»ï¼‰ |
| `fetch_california_housing()` | åŠ å·æˆ¿ä»·æ•°æ®ï¼ˆå›å½’ï¼‰ |

è¿™äº›å‡½æ•°é€šå¸¸ç”¨äºï¼š

- è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆå¦‚ `fetch_20newsgroups`ï¼‰
- å›¾åƒè¯†åˆ«ï¼ˆå¦‚äººè„¸è¯†åˆ«æ•°æ®é›†ï¼‰
- å®é™…å›å½’ä¸åˆ†ç±»å»ºæ¨¡é—®é¢˜

> âš ï¸ æ³¨æ„ï¼š`fetch_*` å‡½æ•°é¦–æ¬¡è¿è¡Œæ—¶ä¼šä¸‹è½½æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦è”ç½‘ï¼Œå¹¶ä¼šç¼“å­˜åˆ°æœ¬åœ°ã€‚

#### âœ… ç¤ºä¾‹ä»£ç ï¼šæ–°é—»æ–‡æœ¬åˆ†ç±»æ•°æ®

```python
from sklearn.datasets import fetch_20newsgroups

# åŠ è½½è®­ç»ƒé›†ï¼ˆä¹Ÿå¯ç”¨ subset='test'ï¼‰
data = fetch_20newsgroups(subset='train')

# æ•°æ®å†…å®¹ä¸æ ‡ç­¾
print(f"æ•°æ®æ€»æ•°ï¼š{len(data.data)}")
print(f"ç›®æ ‡ç±»åˆ«ï¼š{data.target_names}")
print("\nç¬¬ä¸€ç¯‡æ–‡ç« å†…å®¹ï¼š")
print(data.data[0])
```

#### âœ… ç¤ºä¾‹ä»£ç ï¼šåŠ å·æˆ¿ä»·æ•°æ®

```python
from sklearn.datasets import fetch_california_housing
import pandas as pd

housing = fetch_california_housing(as_frame=True)
df = housing.frame  # è½¬ä¸º DataFrame æ ¼å¼

print(df.head())     # æŸ¥çœ‹å‰å‡ è¡Œ
print(housing.DESCR) # æŸ¥çœ‹æ•°æ®é›†æè¿°
```

###  å››ã€Bunch å¯¹è±¡è¯´æ˜ï¼ˆç±»ä¼¼å­—å…¸ï¼‰

åŠ è½½çš„æ ‡å‡†æ•°æ®é›†å¦‚ load_iris() è¿”å›çš„æ˜¯ä¸€ä¸ª Bunch ç±»å‹å¯¹è±¡ï¼Œå®ƒçš„ä½¿ç”¨ç±»ä¼¼äºå­—å…¸ã€‚

```python
from sklearn.datasets import load_wine
data = load_wine()

print(data.keys())          # æŸ¥çœ‹æ‰€æœ‰é”®
print(data.data[:5])        # æŸ¥çœ‹å‰äº”ä¸ªæ ·æœ¬
print(data.target_names)    # æ ‡ç­¾çš„æ–‡å­—è¯´æ˜
```

###  äº”ã€ç»¼åˆç¤ºä¾‹ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«

```python
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

digits = load_digits()

plt.gray()
plt.matshow(digits.images[0])  # æ˜¾ç¤ºç¬¬ä¸€å¼ å›¾ç‰‡
plt.title(f'Label: {digits.target[0]}')
plt.show()
```

### ğŸ§¾ æ€»ç»“

| ç±»å‹ | å‡½æ•°å‰ç¼€ | ç”¨é€” |
|------|----------|------|
| å°å‹å†…ç½®æ•°æ®é›† | `load_` | å¿«é€Ÿæ¼”ç¤ºä¸æ•™å­¦ |
| æ¨¡æ‹Ÿæ•°æ®é›†ç”Ÿæˆ | `make_` | æµ‹è¯•ç®—æ³•å’Œå¯è§†åŒ– |
| å¤§å‹çœŸå®æ•°æ® | `fetch_` | å®é™…åº”ç”¨ä¸æ¨¡å‹è®­ç»ƒ |


## ç¬¬å…«éƒ¨åˆ†ï¼šmodel_selectionæ¨¡å—

`sklearn.model_selection` æ˜¯ Scikit-learn ä¸­ç”¨äº**æ¨¡å‹é€‰æ‹©ä¸è¯„ä¼°**çš„æ ¸å¿ƒæ¨¡å—ï¼Œæä¾›äº†æ•°æ®åˆ’åˆ†ã€äº¤å‰éªŒè¯ã€æ¨¡å‹è¯„ä¼°ä¸è¶…å‚æ•°æœç´¢ç­‰åŠŸèƒ½ã€‚


### 1. æ•°æ®åˆ’åˆ†ï¼š`train_test_split`

å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†ï¼ˆæˆ–éªŒè¯é›†ï¼‰ï¼Œå¸¸ç”¨äºæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚

#### âœ… å¸¸ç”¨å‚æ•°

- `test_size`ï¼šæµ‹è¯•é›†æ¯”ä¾‹ï¼ˆå¦‚ 0.2ï¼‰
- `train_size`ï¼šè®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤æ˜¯ 1 - test_sizeï¼‰
- `shuffle`ï¼šæ˜¯å¦æ‰“ä¹±æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
- `random_state`ï¼šéšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰
- `stratify`ï¼šæŒ‰æ ‡ç­¾åˆ†å±‚åˆ’åˆ†ï¼ˆåˆ†ç±»ä»»åŠ¡æ¨èï¼‰

#### âœ… ç¤ºä¾‹

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

### 2. äº¤å‰éªŒè¯ï¼šcross_val_score ä¸ cross_validate

#### âœ… cross_val_scoreï¼šå¿«é€Ÿè¯„ä¼°æ¨¡å‹æ€§èƒ½

```python
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
model = DecisionTreeClassifier()

scores = cross_val_score(model, X, y, cv=5)
print("äº¤å‰éªŒè¯å¾—åˆ†ï¼š", scores)
print("å¹³å‡å¾—åˆ†ï¼š", scores.mean())
```

#### âœ… cross_validateï¼šè·å–æ›´å¤šè¯„ä¼°æŒ‡æ ‡

```python
from sklearn.model_selection import cross_validate
from sklearn.svm import SVC

result = cross_validate(SVC(), X, y, cv=5, return_train_score=True)
print(result['train_score'])
print(result['test_score'])
```

### 3. K æŠ˜åˆ’åˆ†å™¨ï¼šKFoldã€StratifiedKFold ç­‰

ç”¨äºè‡ªå®šä¹‰äº¤å‰éªŒè¯çš„æ•°æ®åˆ’åˆ†ç­–ç•¥ï¼Œé€‚ç”¨äºæ›´ç²¾ç»†çš„æ§åˆ¶ã€‚

| åˆ’åˆ†å™¨ | è¯´æ˜ |
|--------|------|
| `KFold` | å°†æ•°æ®å‡åˆ†ä¸º K æŠ˜ï¼Œé€‚ç”¨äºå›å½’ä»»åŠ¡æˆ–ç±»åˆ«å‡è¡¡çš„æ•°æ® |
| `StratifiedKFold` | æ¯ä¸€æŠ˜ä¸­ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´ï¼ˆæ¨èç”¨äºåˆ†ç±»é—®é¢˜ï¼‰ |
| `GroupKFold` | åŒä¸€ç»„çš„æ ·æœ¬ä¸ä¼šè¢«åˆ’åˆ†åˆ°ä¸åŒæŠ˜ï¼ˆå¦‚æ‚£è€…ç¼–å·ï¼‰ |

#### âœ… `KFold` ç¤ºä¾‹

```python
from sklearn.model_selection import KFold
import numpy as np

X = np.arange(10).reshape((5, 2))  # 5ä¸ªæ ·æœ¬ï¼Œ2ä¸ªç‰¹å¾
kf = KFold(n_splits=3)

for train_index, test_index in kf.split(X):
    print("Train:", train_index, "Test:", test_index)
```

### 4. è¶…å‚æ•°æœç´¢ï¼šGridSearchCV ä¸ RandomizedSearchCV

#### âœ… GridSearchCVï¼šç½‘æ ¼æœç´¢ï¼ˆç©·ä¸¾æ³•ï¼‰

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid = GridSearchCV(SVC(), param_grid, cv=3)
grid.fit(X, y)

print("æœ€ä½³å‚æ•°ï¼š", grid.best_params_)
print("æœ€ä½³å¾—åˆ†ï¼š", grid.best_score_)
```

#### âœ… RandomizedSearchCVï¼šéšæœºæœç´¢ï¼ˆèŠ‚çœæ—¶é—´ï¼‰

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': randint(3, 10)
}

rand_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist, n_iter=10, cv=3)
rand_search.fit(X, y)

print("æœ€ä½³å‚æ•°ï¼š", rand_search.best_params_)
```

### 5. å­¦ä¹ æ›²çº¿ä¸éªŒè¯æ›²çº¿ï¼šlearning_curve ä¸ validation_curve

#### âœ… learning_curveï¼šæŸ¥çœ‹è®­ç»ƒé›†è§„æ¨¡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import numpy as np
from sklearn.svm import SVC

train_sizes, train_scores, test_scores = learning_curve(SVC(), X, y, cv=5)

plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Train')
plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Test')
plt.legend()
plt.title("Learning Curve")
plt.show()
```

#### âœ… validation_curveï¼šæŸ¥çœ‹æŸä¸ªè¶…å‚æ•°çš„å˜åŒ–å¯¹æ€§èƒ½å½±å“

```python
from sklearn.model_selection import validation_curve

param_range = [0.1, 1, 10]
train_scores, test_scores = validation_curve(
    SVC(), X, y, param_name="C", param_range=param_range, cv=5
)

plt.plot(param_range, train_scores.mean(axis=1), label='Train')
plt.plot(param_range, test_scores.mean(axis=1), label='Test')
plt.xscale("log")
plt.xlabel("C")
plt.title("Validation Curve")
plt.legend()
plt.show()
```

### ğŸ§¾ æ€»ç»“è¡¨

| åŠŸèƒ½ç±»åˆ«       | å¯¹åº”å‡½æ•° |
|----------------|----------|
| æ•°æ®åˆ’åˆ†       | `train_test_split` |
| äº¤å‰éªŒè¯è¯„ä¼°   | `cross_val_score`ï¼Œ`cross_validate` |
| è‡ªå®šä¹‰åˆ’åˆ†å™¨   | `KFold`, `StratifiedKFold`, `GroupKFold` |
| è¶…å‚æ•°æœç´¢     | `GridSearchCV`, `RandomizedSearchCV` |
| å­¦ä¹ æ›²çº¿åˆ†æ   | `learning_curve`, `validation_curve` |


## ç¬¬ä¹éƒ¨åˆ†ï¼š`sklearn.preprocessing` æ¨¡å—è¯¦è§£

`sklearn.preprocessing` æ¨¡å—åŒ…å«äº†ä¸€ç³»åˆ—ç”¨äºæ•°æ®é¢„å¤„ç†çš„å·¥å…·ï¼Œå¸®åŠ©æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œè½¬æ¢ã€å½’ä¸€åŒ–ã€æ ‡å‡†åŒ–ç­‰æ“ä½œï¼Œæå‡æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¡¨ç°å’Œæ”¶æ•›é€Ÿåº¦ã€‚


### 1. å¸¸ç”¨é¢„å¤„ç†æ–¹æ³•

#### 1.1 æ ‡å‡†åŒ–ï¼š`StandardScaler`

å°†ç‰¹å¾æ•°æ®è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒã€‚

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6]])
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

#### 1.2 å½’ä¸€åŒ–ï¼šMinMaxScaler

å°†æ•°æ®ç¼©æ”¾åˆ°æŒ‡å®šåŒºé—´ï¼ˆé»˜è®¤ [0,1]ï¼‰ã€‚

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```

#### 1.3 æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ï¼šMaxAbsScaler

å°†æ•°æ®æŒ‰æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾åˆ° [-1, 1]ï¼Œé€‚åˆç¨€ç–æ•°æ®ã€‚

```python
from sklearn.preprocessing import MaxAbsScaler

scaler = MaxAbsScaler()
X_scaled = scaler.fit_transform(X)
```

#### 1.4 å½’ä¸€åŒ–ï¼ˆèŒƒæ•°ç¼©æ”¾ï¼‰ï¼šNormalizer

æŒ‰è¡Œè¿›è¡Œç¼©æ”¾ï¼Œä½¿æ¯ä¸ªæ ·æœ¬çš„èŒƒæ•°ï¼ˆL1ã€L2ç­‰ï¼‰ä¸º1ã€‚

```python
from sklearn.preprocessing import Normalizer

normalizer = Normalizer(norm='l2')
X_normalized = normalizer.fit_transform(X)
```

### 2. ç¼–ç ç±»åˆ«æ•°æ®

#### 2.1 æ ‡ç­¾ç¼–ç ï¼šLabelEncoder

å°†ç±»åˆ«æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—ç¼–ç ã€‚

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y = ['cat', 'dog', 'cat', 'bird']
y_encoded = le.fit_transform(y)
```

#### 2.2 ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰ï¼šOneHotEncoder

å°†ç±»åˆ«å˜é‡è½¬æ¢æˆç‹¬çƒ­ç¼–ç çŸ©é˜µã€‚

```python
from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder(sparse=False)
X_cat = [['Male'], ['Female'], ['Female']]
X_encoded = enc.fit_transform(X_cat)
```

### 3. ç‰¹å¾ç”Ÿæˆä¸å¤šé¡¹å¼ç‰¹å¾

#### 3.1 å¤šé¡¹å¼ç‰¹å¾ï¼šPolynomialFeatures

ç”Ÿæˆå¤šé¡¹å¼åŠäº¤å‰ç‰¹å¾ï¼Œç”¨äºæ‰©å±•æ¨¡å‹èƒ½åŠ›ã€‚

```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)
```

### 4. è‡ªå®šä¹‰è½¬æ¢å™¨ï¼šFunctionTransformer

é€šè¿‡ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°å¯¹æ•°æ®è¿›è¡Œè½¬æ¢ã€‚

```python
from sklearn.preprocessing import FunctionTransformer
import numpy as np

def log_transform(x):
    return np.log1p(x)

transformer = FunctionTransformer(log_transform)
X_transformed = transformer.fit_transform(X)
```

### 5. ç¼ºå¤±å€¼å¤„ç†

è™½ç„¶ä¸»è¦ç¼ºå¤±å€¼å¤„ç†åœ¨ sklearn.impute æ¨¡å—ï¼Œä½† preprocessing ä¸­ä¹Ÿå¯ä»¥é…åˆä½¿ç”¨æ•°æ®è½¬æ¢ã€‚


### ğŸ§¾ æ€»ç»“

| åŠŸèƒ½          | å¸¸ç”¨ç±»/å‡½æ•°               | è¯´æ˜                         |
|---------------|--------------------------|------------------------------|
| æ ‡å‡†åŒ–        | `StandardScaler`         | é›¶å‡å€¼å•ä½æ–¹å·®               |
| å½’ä¸€åŒ–        | `MinMaxScaler`           | çº¿æ€§ç¼©æ”¾åˆ°æŒ‡å®šèŒƒå›´           |
| æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾| `MaxAbsScaler`           | ç¼©æ”¾åˆ°[-1,1]                 |
| æ ·æœ¬å½’ä¸€åŒ–    | `Normalizer`             | æŒ‰èŒƒæ•°ç¼©æ”¾æ ·æœ¬               |
| æ ‡ç­¾ç¼–ç       | `LabelEncoder`           | ç±»åˆ«æ ‡ç­¾è½¬æ•°å­—               |
| ç‹¬çƒ­ç¼–ç       | `OneHotEncoder`          | ç±»åˆ«å˜é‡è½¬ç‹¬çƒ­ç              |
| å¤šé¡¹å¼ç‰¹å¾    | `PolynomialFeatures`     | ç”Ÿæˆå¤šé¡¹å¼ä¸äº¤å‰ç‰¹å¾         |
| è‡ªå®šä¹‰è½¬æ¢    | `FunctionTransformer`    | é€šè¿‡å‡½æ•°è‡ªå®šä¹‰è½¬æ¢           |


## ç¬¬åéƒ¨åˆ†ï¼šsklearn.ensembleæ¨¡å—

`sklearn.ensemble` æ¨¡å—åŒ…å«äº†å¤šä¸ª**é›†æˆå­¦ä¹ æ–¹æ³•ï¼ˆEnsemble Methodsï¼‰**ï¼Œä¸»è¦åˆ†ä¸º **Baggingï¼ˆè£…è¢‹ï¼‰**ã€**Boostingï¼ˆæå‡ï¼‰** å’Œ **Stackingï¼ˆå †å ï¼‰** ä¸‰å¤§ç±»ã€‚å®ƒé€šè¿‡ç»“åˆå¤šä¸ªåŸºå­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚


### 1. Bagging æ–¹æ³•ï¼šå¹¶è¡Œè®­ç»ƒå¤šä¸ªæ¨¡å‹

#### âœ… `BaggingClassifier` ä¸ `BaggingRegressor`

ä½¿ç”¨åŒä¸€ä¸ªåŸºå­¦ä¹ å™¨åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šè®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œç„¶åè¿›è¡Œå¹³å‡ï¼ˆå›å½’ï¼‰æˆ–æŠ•ç¥¨ï¼ˆåˆ†ç±»ï¼‰ã€‚

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

bag = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=10,
    random_state=42
)
bag.fit(X_train, y_train)
print("Accuracy:", bag.score(X_test, y_test))
```

### 2. Boosting æ–¹æ³•ï¼šé€æ­¥æ”¹è¿›é”™è¯¯

#### âœ… AdaBoostClassifier ä¸ AdaBoostRegressor

é€‚ç”¨äºå¼±æ¨¡å‹ï¼ˆå¦‚å†³ç­–æ ‘æ¡©ï¼‰ï¼Œé€šè¿‡è°ƒæ•´æ ·æœ¬æƒé‡æ¥è¿­ä»£è®­ç»ƒã€‚

```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0,
    random_state=42
)
ada.fit(X_train, y_train)
print("Accuracy:", ada.score(X_test, y_test))
```

#### âœ… GradientBoostingClassifier ä¸ GradientBoostingRegressor

é€šè¿‡æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ä¼˜åŒ–æ®‹å·®ï¼Œæ•ˆæœå¥½ï¼Œå‚æ•°å¤šï¼Œé€‚åˆè°ƒä¼˜ã€‚

```python
from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)
gbc.fit(X_train, y_train)
print("Accuracy:", gbc.score(X_test, y_test))
```

### 3. éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰

#### âœ… RandomForestClassifier ä¸ RandomForestRegressor

é›†æˆå¤šæ£µå†³ç­–æ ‘ï¼Œé€šè¿‡ Bagging å’Œç‰¹å¾éšæœºå­é›†æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚

```python
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rfc.fit(X_train, y_train)
print("Accuracy:", rfc.score(X_test, y_test))
```

ç‰¹ç‚¹ï¼š

+ æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º
+ å¯è¯„ä¼°ç‰¹å¾é‡è¦æ€§ï¼ˆfeature_importances_ï¼‰
+ æ”¯æŒå¹¶è¡Œè®­ç»ƒ

### 4. Stacking æ–¹æ³•ï¼ˆå †å ï¼‰

#### âœ… StackingClassifier ä¸ StackingRegressor

å°†å¤šä¸ªæ¨¡å‹çš„è¾“å‡ºä½œä¸ºæ–°ç‰¹å¾ï¼Œå†è®­ç»ƒä¸€ä¸ªå…ƒæ¨¡å‹ï¼ˆmeta modelï¼‰è¿›è¡Œæœ€ç»ˆé¢„æµ‹ã€‚

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

estimators = [
    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
    ('svc', SVC(probability=True))
]
stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
stack.fit(X_train, y_train)
print("Accuracy:", stack.score(X_test, y_test))
```

### ğŸ§¾ æ€»ç»“è¡¨

| æ–¹æ³•ç±»åˆ«   | ä¸»è¦ç±»å                                     | è¯´æ˜                         |
|------------|----------------------------------------------|------------------------------|
| Bagging    | `BaggingClassifier`, `BaggingRegressor`      | å¹¶è¡Œè®­ç»ƒå¤šä¸ªåŸºå­¦ä¹ å™¨ï¼Œå‡å°‘æ–¹å·® |
| Boosting   | `AdaBoost*`, `GradientBoosting*`             | é¡ºåºè®­ç»ƒåŸºå­¦ä¹ å™¨ï¼Œä¼˜åŒ–æ®‹å·®     |
| éšæœºæ£®æ—   | `RandomForestClassifier`, `RandomForestRegressor` | Bagging çš„æ”¹è¿›ç‰ˆï¼ŒåŠ å…¥ç‰¹å¾éšæœºæ€§ |
| Stacking   | `StackingClassifier`, `StackingRegressor`    | å¤šæ¨¡å‹èåˆï¼Œç”¨å…ƒå­¦ä¹ å™¨ç»„åˆé¢„æµ‹ç»“æœ |

## ç¬¬åä¸€éƒ¨åˆ†ï¼šsklearn.metrics

`sklearn.metrics` æ¨¡å—æä¾›äº†ç”¨äºæ¨¡å‹**è¯„ä¼°å’Œåº¦é‡æŒ‡æ ‡**çš„ä¸°å¯Œå‡½æ•°ï¼Œæ”¯æŒåˆ†ç±»ã€å›å½’ã€èšç±»ã€æ’åºã€è·ç¦»ã€æ¦‚ç‡è¯„åˆ†ç­‰å¤šç§ä»»åŠ¡ã€‚

### 1ï¸âƒ£ åˆ†ç±»è¯„ä¼°æŒ‡æ ‡

#### âœ… å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰

```python
from sklearn.metrics import accuracy_score

y_true = [0, 1, 2, 2]
y_pred = [0, 2, 1, 2]
accuracy_score(y_true, y_pred)  # è¾“å‡º 0.5
```

#### âœ… ç²¾ç¡®ç‡ / å¬å›ç‡ / F1 åˆ†æ•°

```python
from sklearn.metrics import precision_score, recall_score, f1_score

precision_score(y_true, y_pred, average='macro')  # å¹³å‡ç²¾ç¡®ç‡
recall_score(y_true, y_pred, average='macro')      # å¹³å‡å¬å›ç‡
f1_score(y_true, y_pred, average='macro')          # å¹³å‡F1
```
+ average='macro'ï¼šå¯¹æ¯ä¸ªç±»è®¡ç®—å¾—åˆ†å†å¹³å‡ã€‚
+ average='micro'ï¼šç´¯è®¡å…¨å±€ TP/FP/FN å†è®¡ç®—ã€‚
+ average='weighted'ï¼šè€ƒè™‘ç±»åˆ«æ ·æœ¬æ•°é‡çš„åŠ æƒå¹³å‡ã€‚

#### âœ… æ··æ·†çŸ©é˜µï¼ˆConfusion Matrixï¼‰

```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_true, y_pred)
```

#### âœ… åˆ†ç±»æŠ¥å‘Šï¼ˆclassification_reportï¼‰

```python
from sklearn.metrics import classification_report

print(classification_report(y_true, y_pred))
```

#### âœ… ROC æ›²çº¿ & AUC

```python
from sklearn.metrics import roc_curve, roc_auc_score

y_true = [0, 0, 1, 1]
y_score = [0.1, 0.4, 0.35, 0.8]
fpr, tpr, thresholds = roc_curve(y_true, y_score)
roc_auc_score(y_true, y_score)  # è¾“å‡º AUC å€¼
```

### 2ï¸âƒ£ å›å½’è¯„ä¼°æŒ‡æ ‡

#### âœ… MSE / RMSE / MAE

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error
import numpy as np

y_true = np.array([3.0, -0.5, 2.0, 7.0])
y_pred = np.array([2.5, 0.0, 2.0, 8.0])

mean_squared_error(y_true, y_pred)  # MSE
np.sqrt(mean_squared_error(y_true, y_pred))  # RMSE
mean_absolute_error(y_true, y_pred)  # MAE
```

#### âœ… RÂ² åˆ†æ•°ï¼ˆå†³å®šç³»æ•°ï¼‰

```python
from sklearn.metrics import r2_score

r2_score(y_true, y_pred)  # è¶Šæ¥è¿‘1è¶Šå¥½
```

### 3ï¸âƒ£ èšç±»è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import adjusted_rand_score, silhouette_score

# å‡è®¾ labels_true å’Œ labels_pred æ˜¯èšç±»æ ‡ç­¾
adjusted_rand_score(labels_true, labels_pred)
silhouette_score(X, cluster_labels)  # è½®å»“ç³»æ•°
```

### 4ï¸âƒ£ è·ç¦»åº¦é‡

```python
from sklearn.metrics import pairwise_distances

pairwise_distances([[0, 1]], [[1, 0]], metric='euclidean')  # æ¬§æ°è·ç¦»
```

### 5ï¸âƒ£ è‡ªå®šä¹‰æ‰“åˆ†å‡½æ•°ï¼šmake_scorer

ç”¨äºåœ¨äº¤å‰éªŒè¯æˆ– GridSearchCV ä¸­è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ã€‚

```python
from sklearn.metrics import make_scorer

custom_scorer = make_scorer(mean_squared_error, greater_is_better=False)
```

### ğŸ§¾ æ€»ç»“è¡¨

| ä»»åŠ¡ç±»å‹ | å¸¸ç”¨æŒ‡æ ‡å‡½æ•°                                                                 | è¯´æ˜                           |
|----------|------------------------------------------------------------------------------|--------------------------------|
| åˆ†ç±»     | `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, `roc_auc_score`, `confusion_matrix`, `classification_report` | è¯„ä¼°åˆ†ç±»æ¨¡å‹æ€§èƒ½ï¼ˆç²¾åº¦ã€å¬å›ã€AUC ç­‰ï¼‰ |
| å›å½’     | `mean_squared_error`, `mean_absolute_error`, `r2_score`                     | è¯„ä¼°å›å½’æ¨¡å‹è¯¯å·®å’Œæ‹Ÿåˆç¨‹åº¦     |
| èšç±»     | `adjusted_rand_score`, `silhouette_score`                                   | æ— ç›‘ç£èšç±»ä¸çœŸå®æ ‡ç­¾ä¸€è‡´æ€§     |
| è·ç¦»     | `pairwise_distances`                                                        | è®¡ç®—æ ·æœ¬å¯¹ä¹‹é—´çš„è·ç¦»           |
| è‡ªå®šä¹‰è¯„åˆ† | `make_scorer`                                                               | è‡ªå®šä¹‰ç”¨äºäº¤å‰éªŒè¯å’Œæœç´¢çš„è¯„åˆ†å‡½æ•° |


## ç¬¬åäºŒéƒ¨åˆ†ï¼šsklearn å…¶ä»–æ¨¡å— 

é™¤äº†å¸¸ç”¨çš„æ¨¡å‹ä¸è¯„ä¼°æ¨¡å—ï¼Œ`scikit-learn` è¿˜åŒ…å«è®¸å¤šè¾…åŠ©æ¨¡å—ï¼Œæ”¯æŒæ•°æ®å¤„ç†ã€æ¨¡å‹æŒä¹…åŒ–ã€ç®¡é“ç»„åˆç­‰ä»»åŠ¡ã€‚

### 1ï¸âƒ£ sklearn.pipelineï¼šæ„å»ºæ•°æ®å¤„ç†ä¸æ¨¡å‹è®­ç»ƒæµç¨‹

å¯å°†æ•°æ®é¢„å¤„ç†ä¸æ¨¡å‹æ‰“åŒ…æˆæµæ°´çº¿ï¼Œä¾¿äºè‡ªåŠ¨åŒ–è®­ç»ƒä¸è°ƒå‚ã€‚

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression())
])
pipe.fit(X_train, y_train)
```

### 2ï¸âƒ£ sklearn.composeï¼šç»„åˆé¢„å¤„ç†å™¨

æ”¯æŒå¯¹ä¸åŒåˆ—åº”ç”¨ä¸åŒçš„è½¬æ¢ï¼ˆå¦‚åˆ†ç±»ç‰¹å¾ vs æ•°å€¼ç‰¹å¾ï¼‰ã€‚

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), [0, 1]),
        ('cat', OneHotEncoder(), [2])
    ]
)
```

### 3ï¸âƒ£ sklearn.feature_selectionï¼šç‰¹å¾é€‰æ‹©

ç”¨äºé€‰å‡ºæœ€ç›¸å…³çš„ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚

```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X, y)
```

### 4ï¸âƒ£ sklearn.feature_extractionï¼šç‰¹å¾æå–ï¼ˆæ–‡æœ¬/å›¾åƒï¼‰

+ æ–‡æœ¬ï¼šCountVectorizer, TfidfVectorizer
+ å›¾åƒï¼šimage.extract_patches_2d

```python
from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X_text = vec.fit_transform(["I love sklearn", "scikit-learn is powerful"])
```

### 5ï¸âƒ£ sklearn.imputeï¼šç¼ºå¤±å€¼å¤„ç†

```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_filled = imputer.fit_transform(X_missing)
```

### 6ï¸âƒ£ sklearn.decompositionï¼šé™ç»´æ–¹æ³•ï¼ˆå¦‚ PCAï¼‰

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)
```

### 7ï¸âƒ£ sklearn.manifoldï¼šæµå½¢å­¦ä¹ ï¼ˆå¦‚ t-SNEï¼‰

```python
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2)
X_embedded = tsne.fit_transform(X)
```

### 8ï¸âƒ£ sklearn.utilsï¼šå®ç”¨å·¥å…·å‡½æ•°

åŒ…æ‹¬æ‰“ä¹±æ•°æ®ã€è®¾ç½®éšæœºç§å­ã€ç¨€ç–çŸ©é˜µè½¬æ¢ç­‰ã€‚

```python
from sklearn.utils import shuffle

X_shuffled, y_shuffled = shuffle(X, y, random_state=42)
```

### ğŸ§¾ æ€»ç»“è¡¨

| æ¨¡å—                     | åŠŸèƒ½è¯´æ˜                                | å¸¸ç”¨ç±»/å‡½æ•°ç¤ºä¾‹                          |
|--------------------------|-----------------------------------------|------------------------------------------|
| `pipeline`               | æ„å»ºè®­ç»ƒæµæ°´çº¿                          | `Pipeline`, `make_pipeline`              |
| `compose`                | ä¸åŒåˆ—çš„é¢„å¤„ç†ç»„åˆ                      | `ColumnTransformer`                      |
| `feature_selection`      | ç‰¹å¾é€‰æ‹©                                | `SelectKBest`, `RFE`, `SelectFromModel`  |
| `feature_extraction`     | æ–‡æœ¬/å›¾åƒç‰¹å¾æå–                       | `CountVectorizer`, `TfidfVectorizer`     |
| `impute`                 | ç¼ºå¤±å€¼å¡«å……                              | `SimpleImputer`, `KNNImputer`            |
| `decomposition`          | é™ç»´                                    | `PCA`, `TruncatedSVD`                    |
| `manifold`               | éçº¿æ€§é™ç»´ / å¯è§†åŒ–                     | `TSNE`, `Isomap`                         |
| `utils`                  | å·¥å…·å‡½æ•°                                | `shuffle`, `resample`, `Bunch`           |





