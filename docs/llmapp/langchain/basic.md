# LangChain æ•™ç¨‹

## ä¸€ã€ ç®€ä»‹

![ç®€ä»‹](https://python.langchain.com/svg/langchain_stack_112024.svg)

LangChain å»ºç«‹åœ¨å‡ ä¸ªæ ¸å¿ƒæ¦‚å¿µä¹‹ä¸Šï¼š

+ æ¨¡å‹ï¼ˆModelsï¼‰ï¼šä¸å„ç§LLMï¼ˆå¦‚OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰ï¼‰è¿›è¡Œäº¤äº’çš„æ¥å£
+ æç¤ºï¼ˆPromptsï¼‰ï¼šæ„å»ºå’Œç®¡ç†å‘é€ç»™æ¨¡å‹çš„è¾“å…¥
+ è¾“å‡ºè§£æå™¨ï¼ˆOutput Parsersï¼‰ï¼šå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼
+ é“¾ï¼ˆChainsï¼‰ï¼šå°†å¤šä¸ªç»„ä»¶è¿æ¥æˆä¸€ä¸ªå·¥ä½œæµ
+ è®°å¿†ï¼ˆMemoryï¼‰ï¼šå­˜å‚¨å’Œæ£€ç´¢å¯¹è¯å†å²
+ å·¥å…·ï¼ˆToolsï¼‰ï¼šä½¿æ¨¡å‹èƒ½å¤Ÿä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’
+ ä»£ç†ï¼ˆAgentsï¼‰ï¼šè®©æ¨¡å‹å†³å®šä½¿ç”¨å“ªäº›å·¥å…·ä»¥åŠå¦‚ä½•ä½¿ç”¨

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®ç¯å¢ƒå¹¶å®‰è£… LangChain ï¼š

```bash
pip install langchain

pip install langchain-openai

pip install langchain-community

pip install langgraph

pip install "langserve[all]"

pip install langchain-cli

pip install langsmith

pip install langchain-huggingface
```

ä½¿ç”¨ç¤ºä¾‹ï¼š

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(model="deepseek-r1:1.5b", model_provider="ollama", base_url="http://192.168.3.5:11434/")

response = llm.invoke("ä½ æ˜¯è°ï¼Ÿ")
print(response.content)
```

!!! success "è¾“å‡ºç»“æœ"

    æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚

## äºŒã€æ¨¡å‹ä½¿ç”¨

### 2.1  ä½¿ç”¨æœ¬åœ°æ¨¡å‹

Ollama æ˜¯ä¸€ä¸ªæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿è¡Œå·¥å…·ï¼Œå®ƒè®©ä½ èƒ½åœ¨è‡ªå·±çš„ç”µè„‘ä¸Šå¿«é€Ÿè¿è¡Œå¤§æ¨¡å‹ï¼ˆåƒ LLaMAã€Mistralã€Gemmaã€Phi-3ã€Starling
ç­‰ï¼‰ï¼Œå¹¶ä¸”æä¾›ç±»ä¼¼äº ChatGPT çš„äº¤äº’ä½“éªŒã€‚

#### 1ã€æ–‡æœ¬æ¨¡å‹

```python
from langchain_ollama import OllamaLLM, ChatOllama

llm = OllamaLLM(
    model="deepseek-r1:1.5b",
    base_url="http://192.168.3.5:11434/"
)

llm.invoke("ä½ æ˜¯è°ï¼Ÿ")
```

!!! success "è¾“å‡ºç»“æœ"

    '<think>\n\n</think>\n\næ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚'

#### 2. èŠå¤©æ¨¡å‹

```python
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage

chat = ChatOllama(
    model="deepseek-r1:1.5b",
    base_url="http://192.168.3.5:11434/"
)

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€åç¿»è¯‘åŠ©æ‰‹ï¼Œå°†ç”¨æˆ·çš„è¾“å…¥ç¿»è¯‘æˆè‹±è¯­"),
    HumanMessage(content="å°æ˜å–œæ¬¢å»ä¸­å›½æ—…æ¸¸ï¼Œæœ‰ä»€ä¹ˆæ¨èçš„æ™¯ç‚¹?"),
]

response = chat.invoke(messages)
print(response.content)
```

!!! success "è¾“å‡ºç»“æœ"

    He likes traveling in China. What are your recommendations for the places to visit?

### 2.2 ä½¿ç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹ï¼ˆHugging Faceä¸ºä¾‹ï¼‰

ä½¿ç”¨Hugging Faceå¹³å°ä¸Šçš„æ¨¡å‹

```bash
pip install langchain-huggingface
```

langchain_huggingface åˆ†æˆä¸¤ç§ï¼Œä¸€ç§ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œåœ¨æœ¬åœ°è¿è¡Œæ¨¡å‹ï¼Œä¸€ç§ä½¿ç”¨hugging face æä¾›çš„æ¨¡å‹apiæ¥å£

#### ä½¿ç”¨æœ¬åœ°æ¨¡å‹

```python
from langchain_huggingface import HuggingFacePipeline

llm = HuggingFacePipeline.from_model_id(
    model_id="microsoft/Phi-3-mini-4k-instruct",
    task="text-generation",
    pipeline_kwargs={
        "max_new_tokens": 100,
        "top_k": 50,
        "temperature": 0.1,
    },
)
llm.invoke("Hugging Face is")
```

#### ä½¿ç”¨åœ¨çº¿æ¨¡å‹

```python
from langchain_huggingface import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
    repo_id="meta-llama/Meta-Llama-3-8B-Instruct",
    task="text-generation",
    max_new_tokens=100,
    do_sample=False,
)
llm.invoke("Hugging Face is")
```

### 2.3 è‡ªå®šä¹‰èŠå¤©æ¨¡å‹

```text
BaseLanguageModel
â”œâ”€â”€ BaseLLM
â””â”€â”€ BaseChatModel
```

+ **BaseLanguageModel**ï¼šæœ€é€šç”¨ï¼ˆæŠ½è±¡è¯­è¨€æ¨¡å‹ï¼‰
+ **BaseLLM**ï¼šå•è½® prompt â†’ æ–‡æœ¬
+ **BaseChatModel**ï¼šå¤šè½®å¯¹è¯æ¶ˆæ¯

#### è‡ªå®šä¹‰å¤šåŠŸèƒ½ Echo LLM

ä¸‹é¢æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½ Echo LLMï¼š

+ åŒæ­¥è°ƒç”¨ï¼šå›æ˜¾å¤§å†™
+ å¼‚æ­¥è°ƒç”¨ï¼šå›æ˜¾å°å†™
+ æ‰¹é‡è°ƒç”¨ï¼šæ¯ä¸ªå¥å­é¦–å­—æ¯å¤§å†™
+ æµå¼è°ƒç”¨ï¼šé€å­—æ¯è¾“å‡º
+ äº‹ä»¶æµå¼è°ƒç”¨ï¼šé€å•è¯è¾“å‡ºäº‹ä»¶

```python
from langchain_core.language_models import BaseLLM
from langchain_core.outputs import GenerationChunk, LLMResult
from langchain_core.runnables import Event
import asyncio


class MyMultiFunctionLLM(BaseLLM):
    def _call(self, prompt: str, stop=None):
        """
        åŒæ­¥æ¨ç†ï¼šå°†è¾“å…¥è½¬æˆå¤§å†™
        """
        return prompt.upper()

    async def _acall(self, prompt: str, stop=None):
        """
        å¼‚æ­¥æ¨ç†ï¼šå°†è¾“å…¥è½¬æˆå°å†™
        """
        return prompt.lower()

    def _batch(self, prompts, stop=None, **kwargs):
        """
        æ‰¹é‡æ¨ç†ï¼šæ¯å¥è¯é¦–å­—æ¯å¤§å†™
        """
        return [p.capitalize() for p in prompts]

    async def _abatch(self, prompts, stop=None, **kwargs):
        """
        å¼‚æ­¥æ‰¹é‡æ¨ç†ï¼šæ¯å¥è¯å€’åºè¾“å‡º
        """
        return [p[::-1] for p in prompts]

    def _stream(self, prompt: str, stop=None, **kwargs):
        """
        æµå¼æ¨ç†ï¼šé€å­—æ¯è¾“å‡º
        """
        for char in prompt:
            yield GenerationChunk(text=char)

    async def _astream(self, prompt: str, stop=None, **kwargs):
        """
        å¼‚æ­¥æµå¼æ¨ç†ï¼šé€å­—æ¯è¾“å‡ºï¼ˆå¼‚æ­¥ç‰ˆï¼‰
        """
        for char in prompt:
            await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿæ…¢é€Ÿç”Ÿæˆ
            yield GenerationChunk(text=char)

    async def _astream_events(self, prompt: str, stop=None, **kwargs):
        """
        äº‹ä»¶æµå¼æ¨ç†ï¼šé€å•è¯è¾“å‡ºäº‹ä»¶
        """
        words = prompt.split()
        for word in words:
            await asyncio.sleep(0.1)
            yield Event(type="word", data=word)
```

### 2.4 æµå¤±è¾“å‡º

#### 1. åŒæ­¥æµ

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    model="deepseek-r1:1.5b",
    model_provider="ollama",
    base_url="http://192.168.3.5:11434/"
)

for chunk in llm.stream("ç»™æˆ‘å†™ä¸€é¦–æœ‰å…³äºçˆ±æƒ…çš„æ­Œæ›²"):
    print(chunk.content)
```

#### 2. å¼‚æ­¥æµ

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    model="deepseek-r1:1.5b",
    model_provider="ollama",
    base_url="http://192.168.3.5:11434/"
)

async for chunk in llm.astream("ç»™æˆ‘å†™ä¸€é¦–æ ¡å›­æ­Œæ›²"):
    print(chunk.content, end="|", flush=True)
```

#### 3. å¼‚æ­¥äº‹ä»¶æµ

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    model="deepseek-r1:1.5b",
    model_provider="ollama",
    base_url="http://192.168.3.5:11434/"
)

async  for event in llm.astream_events("ä½ å¯ä»¥ä¸ºæˆ‘åšä»€ä¹ˆï¼Ÿ"):
    print(event)
```

### 2.5 å·¥å…·è°ƒç”¨

```python
from langchain.chat_models import init_chat_model


def add(a: int, b: int) -> int:
    return a + b


def mul(a: int, b: int) -> int:
    return a * b


llm = init_chat_model(
    model="deepseek-r1:1.5b",
    model_provider="ollama",
    base_url="http://192.168.3.5:11434/"
)

llm_with_tools = llm.bind_tools(tools=[add, mul])

llm_with_tools.invoke("What is 3 * 12?")
```

!!! failure "è¾“å‡ºç»“æœ"

    ResponseError: registry.ollama.ai/library/deepseek-r1:1.5b does not support tools (status code: 400)

## ä¸‰ã€æç¤ºï¼ˆPromptsï¼‰

### 3.1 æ¶ˆæ¯ï¼ˆmessagesï¼‰

`langchain_core.messages` æä¾›äº†å››ç§å¸¸ç”¨çš„æ¶ˆæ¯ç±»å‹ï¼Œåˆ†åˆ«æ˜¯ï¼š

| æ¶ˆæ¯ç±»å‹            | ç”¨é€”                 | è¯´æ˜                                          |
|-----------------|--------------------|---------------------------------------------|
| `SystemMessage` | ç³»ç»Ÿæ¶ˆæ¯ / ä¸Šä¸‹æ–‡çº¦æŸ       | é€šå¸¸ç”¨äºç»™æ¨¡å‹è®¾ç½®å…¨å±€è§’è‰²ã€è¡Œä¸ºè¯´æ˜ï¼Œç±»ä¼¼ OpenAI `system` role  |
| `HumanMessage`  | ç”¨æˆ·è¾“å…¥               | ç”¨æˆ·å‘é€çš„æ¶ˆæ¯ï¼Œç±»ä¼¼ OpenAI `user` role               |
| `AIMessage`     | AI çš„å›å¤             | æ¨¡å‹çš„å›ç­”ï¼Œç±»ä¼¼ OpenAI `assistant` role            |
| `ToolMessage`   | å·¥å…·è¿”å›çš„ç»“æœï¼ˆä¾›æ¨¡å‹åç»­å¯¹è¯ä½¿ç”¨ï¼‰ | å½“å·¥å…·è¢«è°ƒç”¨åè¿”å›ç»“æœï¼Œä½œä¸ºå·¥å…·çš„ã€Œåç»­ä¸Šä¸‹æ–‡æ¶ˆæ¯ã€ä¼ ç»™æ¨¡å‹ï¼ˆç±»ä¼¼å·¥å…·è°ƒç”¨çš„ä¸­è½¬æ¶ˆæ¯ï¼‰ |

---

#### ToolMessage

**ç”¨é€”**ï¼šå½“æ¨¡å‹è°ƒç”¨å·¥å…·ï¼ˆå¦‚æœç´¢ã€æ•°æ®åº“ï¼‰åï¼Œå·¥å…·è¿”å›çš„ç»“æœéœ€è¦ç”¨ ToolMessage ä¼ é€’ç»™æ¨¡å‹ï¼Œè®©æ¨¡å‹åŸºäºå·¥å…·ç»“æœè¿›è¡Œå›ç­”ã€‚

```python
from langchain_core.messages import ToolMessage

ToolMessage(
    tool_call_id="search_tool_123",
    content="æœç´¢åˆ°çš„ç»“æœæ˜¯ï¼šPython çš„æœ€æ–°ç‰ˆæœ¬æ˜¯ 3.12"
)
```

!!! info

    + tool_call_idï¼šå”¯ä¸€æ ‡è¯†å·¥å…·è°ƒç”¨ï¼Œå‘Šè¯‰æ¨¡å‹è¿™æ˜¯å“ªä¸ªå·¥å…·çš„è¿”å›ç»“æœ
    +  contentï¼šå·¥å…·çš„è¿”å›å†…å®¹

```python
from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage, HumanMessage

llm = init_chat_model(
    model="deepseek-r1:1.5b",
    model_provider="ollama",
    base_url="http://192.168.3.5:11434/"
)

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸­å›½çš„æ—…æ¸¸ä¸“å®¶ï¼Œå–„äºç»™å‡ºæ—…æ¸¸æ–¹æ¡ˆ"),
    HumanMessage(content="ç»™æˆ‘ä¸€ä¸ªåŒ—äº¬åˆ°æµ·å—çš„æ—…æ¸¸æ–¹æ¡ˆï¼Œè¦åŒ…å«ä¸»è¦çš„æ—…æ¸¸æ™¯ç‚¹")
]

llm.invoke(messages).content
```

### 3.2 æç¤ºæ¨¡ç‰ˆ

`langchain.prompts` ä¸­å¸¸ç”¨çš„æ ¸å¿ƒç±»æœ‰ï¼š

| ç±» / æ–¹æ³•                        | ä½œç”¨                                     |
|-------------------------------|----------------------------------------|
| `PromptTemplate`              | æœ€å¸¸ç”¨çš„æç¤ºæ¨¡æ¿ç±»ï¼Œæ”¯æŒåŠ¨æ€å˜é‡æ›¿æ¢                     |
| `ChatPromptTemplate`          | é¢å‘å¤šè½®å¯¹è¯æ¶ˆæ¯çš„æç¤ºæ¨¡æ¿ï¼ˆç”Ÿæˆå¯¹è¯æ ¼å¼ï¼‰                  |
| `SystemMessagePromptTemplate` | ç”¨äºç”Ÿæˆ System æ¶ˆæ¯ï¼ˆsystem roleï¼‰            |
| `HumanMessagePromptTemplate`  | ç”¨äºç”Ÿæˆ Human æ¶ˆæ¯ï¼ˆuser roleï¼‰               |
| `AIMessagePromptTemplate`     | ç”¨äºç”Ÿæˆ AI æ¶ˆæ¯ï¼ˆassistant roleï¼‰             |
| `MessagesPlaceholder`         | åœ¨å¤šè½®å¯¹è¯ä¸­æ’å…¥ã€Œå†å²ä¸Šä¸‹æ–‡ã€å ä½ç¬¦ï¼Œå¸¸ç”¨äº memory / RAG åœºæ™¯ |

#### PromptTemplate

æœ€å¸¸ç”¨çš„æç¤ºæ¨¡æ¿ï¼Œé€‚ç”¨äºå•æ¬¡ Prompt æˆ–ã€Œè¡¥å…¨æ¨¡å‹ã€çš„è°ƒç”¨ã€‚
å®ƒæ”¯æŒä½¿ç”¨èŠ±æ‹¬å· {} æ¥æ ‡è¯†å˜é‡ï¼Œå¹¶åœ¨è°ƒç”¨æ—¶è¿›è¡Œæ›¿æ¢ã€‚

```python
from langchain.prompts import PromptTemplate

template = "è¯·ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
prompt = PromptTemplate.from_template(template)

final_prompt = prompt.format(question="LangChain æ˜¯ä»€ä¹ˆï¼Ÿ")
print(final_prompt)
# è¾“å‡º: è¯·ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šLangChain æ˜¯ä»€ä¹ˆï¼Ÿ
```

#### ChatPromptTemplate

ç”¨äºå’Œã€ŒèŠå¤©æ¨¡å‹ï¼ˆChatModelï¼‰ã€é…åˆï¼Œç”Ÿæˆç¬¦åˆå¯¹è¯æ¶ˆæ¯æ ¼å¼çš„ Promptã€‚
é€šå¸¸åŒ…å«å¤šä¸ªã€Œæ¶ˆæ¯æ¨¡æ¿ï¼ˆMessagePromptTemplateï¼‰ã€ã€‚

```python
from langchain.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"),
    ("user", "ä½ å¥½ï¼Œå¸®æˆ‘ç”¨ä¸­æ–‡æ€»ç»“ä¸€ä¸‹ï¼š{text}")
])

final_prompt = chat_prompt.format_messages(text="LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚")
# è¿”å›ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å« systemã€user è§’è‰²æ¶ˆæ¯
for msg in final_prompt:
    print(msg)
```

!!! success "è¾“å‡ºç»“æœ"

    content='ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚' additional_kwargs={} response_metadata={}
    content='ä½ å¥½ï¼Œå¸®æˆ‘ç”¨ä¸­æ–‡æ€»ç»“ä¸€ä¸‹ï¼šLangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚' additional_kwargs={} response_metadata={}

#### SystemMessagePromptTemplate ã€ HumanMessagePromptTemplate ã€ AIMessagePromptTemplate

å®ƒä»¬æ˜¯ã€Œæ¶ˆæ¯æ¨¡æ¿ã€çš„å…·ä½“å®ç°ï¼Œä¸“é—¨ç”¨äºç”Ÿæˆå¯¹åº”è§’è‰²çš„æ¶ˆæ¯ï¼Œå¯ä¸ ChatPromptTemplate ç»„åˆä½¿ç”¨ã€‚

```python
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate
)

system_msg = SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘å®¶ã€‚")
human_msg = HumanMessagePromptTemplate.from_template("è¯·ç¿»è¯‘ï¼š{text}")

chat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])
final_prompt = chat_prompt.format_messages(text="Hello world!")

for msg in final_prompt:
    print(msg)
```

#### MessagesPlaceholder

åœ¨å¤šè½®å¯¹è¯ï¼ˆå¦‚æœ‰ Memoryï¼‰æˆ– RAG ä¸­ï¼Œä¿ç•™å†å²ä¸Šä¸‹æ–‡ çš„å ä½ç¬¦ã€‚

```python
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªè®°å¿†åŠ›å¾ˆå¼ºçš„åŠ©ç†ã€‚"),
    MessagesPlaceholder(variable_name="chat_history"),  # å ä½ç¬¦
    ("user", "{input}")
])
```

ç„¶ååœ¨è°ƒç”¨æ—¶ï¼Œå°†å¤šè½®ä¸Šä¸‹æ–‡ä¼ ç»™ chat_historyï¼š

```python
final_prompt = chat_prompt.format_messages(
    chat_history=[
        HumanMessage(content="ä½ å¥½ï¼"),
        AIMessage(content="ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ")
    ],
    input="è¯·å¸®æˆ‘å†™ä¸€ä¸ª Python è„šæœ¬"
)
```

#### FewShotPromptTemplate

åœ¨æç¤ºå·¥ç¨‹ä¸­ï¼ŒFew-Shot æç¤ºï¼ˆå°‘é‡ç¤ºä¾‹æç¤ºï¼‰ æ„å‘³ç€åœ¨æç¤ºä¸­æä¾›è‹¥å¹²ä¸ªç¤ºä¾‹ï¼Œå¸®åŠ©å¤§æ¨¡å‹ã€Œæ¨¡ä»¿ç¤ºä¾‹ã€æ›´å¥½åœ°ç”Ÿæˆç›®æ ‡ç­”æ¡ˆã€‚
FewShotPromptTemplate å°±æ˜¯ä¸ºæ­¤ä¸“é—¨å‡†å¤‡çš„æ¨¡æ¿ç±»ã€‚

```python
from langchain.prompts import PromptTemplate, FewShotPromptTemplate

# 1ï¸âƒ£ å®šä¹‰ç¤ºä¾‹
examples = [
    {"input": "2 + 2", "output": "4"},
    {"input": "3 + 5", "output": "8"}
]

# 2ï¸âƒ£ å®šä¹‰ç¤ºä¾‹çš„æ¨¡æ¿
example_prompt = PromptTemplate.from_template(
    "è¾“å…¥: {input}\nè¾“å‡º: {output}"
)

# 3ï¸âƒ£ åˆ›å»º FewShotPromptTemplate
prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="ä¸‹é¢æ˜¯ä¸€äº›æ•°å­¦é¢˜çš„ç¤ºä¾‹ï¼š",
    suffix="è¯·å›ç­”: {input}",
    input_variables=["input"]
)

# 4ï¸âƒ£ æ¸²æŸ“æç¤º
final_prompt = prompt.format(input="7 + 9")
print(final_prompt)
```
!!! success "è¾“å‡ºç»“æœ"
    
    ä¸‹é¢æ˜¯ä¸€äº›æ•°å­¦é¢˜çš„ç¤ºä¾‹ï¼š

        è¾“å…¥: 2 + 2
        è¾“å‡º: 4

        è¾“å…¥: 3 + 5
        è¾“å‡º: 8

        è¯·å›ç­”: 7 + 9
    

## å››ã€è¾“å‡ºè§£æå™¨

åœ¨ä½¿ç”¨å¤§æ¨¡å‹ï¼ˆLLM / Chat æ¨¡å‹ï¼‰æ—¶ï¼Œæ¨¡å‹è¾“å‡ºé€šå¸¸æ˜¯ã€Œè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€ã€‚
ç„¶è€Œï¼Œåœ¨çœŸå®åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€å¸Œæœ›æ¨¡å‹è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚ JSON / åˆ—è¡¨ / å…·ä½“å­—æ®µç­‰ï¼‰ï¼Œä¾¿äºåç»­å¤„ç†ã€‚

### 4.1  langchain.output_parsers æ ¸å¿ƒå†…å®¹æ¦‚è§ˆ

| ç±» / æ–¹æ³•                             | ä½œç”¨                                                         |
|-------------------------------------|------------------------------------------------------------|
| `StrOutputParser`                   | ç›´æ¥å°†è¾“å‡ºæ–‡æœ¬åŸæ ·è¿”å›ä¸ºå­—ç¬¦ä¸²                                    |
| `CommaSeparatedListOutputParser`    | è§£ææ¨¡å‹è¾“å‡ºä¸ºã€Œé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€                               |
| `ListOutputParser`                  | å°†æ¨¡å‹è¾“å‡ºçš„å¤šè¡Œæ–‡æœ¬ï¼Œè§£æä¸ºåˆ—è¡¨                                   |
| `JsonOutputParser`                  | è§£ææ¨¡å‹è¾“å‡ºçš„ JSON å­—ç¬¦ä¸²ï¼Œè¿”å›å­—å…¸ / åˆ—è¡¨                          |
| `PydanticOutputParser`              | å°†æ¨¡å‹è¾“å‡ºè§£æä¸ºç¬¦åˆ Pydantic æ¨¡å‹çš„ç»“æ„åŒ–æ•°æ®ï¼Œæ”¯æŒå­—æ®µæ ¡éªŒ             |
| `OutputFixingParser`                | å½“è¾“å‡ºæ ¼å¼ä¸ç¬¦åˆé¢„æœŸæ—¶ï¼Œè‡ªåŠ¨è¯·æ±‚ LLM è¿›è¡Œæ ¼å¼ä¿®æ­£ï¼ˆåŸºäº Retryï¼‰            |

### 4.2 ç¤ºä¾‹

#### 1ï¸âƒ£ StrOutputParser

æœ€ç®€å•çš„è§£æå™¨ï¼Œ**ç›´æ¥è¿”å›æ¨¡å‹è¾“å‡ºçš„åŸå§‹æ–‡æœ¬**ã€‚
é€‚åˆå¯¹æ¨¡å‹åŸå§‹å›å¤æ²¡æœ‰ç»“æ„åŒ–éœ€æ±‚çš„åœºæ™¯ã€‚
    
```python
from langchain.output_parsers import StrOutputParser
parser = StrOutputParser()

result = parser.parse("Hello, this is your answer.")
print(result)
# è¾“å‡º: "Hello, this is your answer."
```


#### 2ï¸âƒ£ CommaSeparatedListOutputParser

å°†æ¨¡å‹è¾“å‡ºçš„ã€Œé€—å·åˆ†éš”æ–‡æœ¬ã€è§£ææˆåˆ—è¡¨ã€‚
é€‚åˆåœºæ™¯ï¼šè®©æ¨¡å‹è¿”å›ã€ŒA, B, Cã€æ ¼å¼çš„ç­”æ¡ˆã€‚

```python
from langchain.output_parsers import CommaSeparatedListOutputParser
parser = CommaSeparatedListOutputParser()

output = "è‹¹æœ, é¦™è•‰, æ©™å­"
result = parser.parse(output)
print(result)
# è¾“å‡º: ["è‹¹æœ", "é¦™è•‰", "æ©™å­"]
```

#### 3ï¸âƒ£ ListOutputParser

å°†æ¨¡å‹è¾“å‡ºçš„ã€Œå¤šè¡Œæ–‡æœ¬ã€è§£ææˆåˆ—è¡¨ã€‚
é€‚åˆåœºæ™¯ï¼šè®©æ¨¡å‹é€è¡Œåˆ—å‡ºå†…å®¹ï¼ˆæ¯è¡Œä¸€ä¸ªé¡¹ç›®ï¼‰ã€‚

```python
from langchain.output_parsers import ListOutputParser
parser = ListOutputParser()

output = "ç¬¬ä¸€é¡¹\nç¬¬äºŒé¡¹\nç¬¬ä¸‰é¡¹"
result = parser.parse(output)
print(result)
# è¾“å‡º: ["ç¬¬ä¸€é¡¹", "ç¬¬äºŒé¡¹", "ç¬¬ä¸‰é¡¹"]
```

#### 4ï¸âƒ£ JsonOutputParser

å½“ä½ å¸Œæœ›æ¨¡å‹è¿”å› JSON æ ¼å¼æ—¶ï¼Œä½¿ç”¨å®ƒæ¥è§£æ JSON å¹¶è¿”å›å­—å…¸ / åˆ—è¡¨ã€‚

```python
from langchain.output_parsers import JsonOutputParser
parser = JsonOutputParser()

output = '{"name": "Tom", "age": 20}'
result = parser.parse(output)
print(result)
# è¾“å‡º: {'name': 'Tom', 'age': 20}
```

####  5ï¸âƒ£ PydanticOutputParser

âš¡ï¸ é«˜çº§è§£æå™¨ï¼š
å°†æ¨¡å‹è¾“å‡ºç›´æ¥è§£æä¸ºPydantic æ¨¡å‹å¯¹è±¡ï¼Œæ”¯æŒä¸¥æ ¼å­—æ®µæ ¡éªŒã€‚
é€‚åˆåœºæ™¯ï¼šè¾“å‡ºæœ‰å›ºå®šå­—æ®µï¼ˆå¦‚ name, ageï¼‰ã€‚

```python
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int

parser = PydanticOutputParser(pydantic_object=Person)

output = '{"name": "Alice", "age": 30}'
result = parser.parse(output)
print(result)
# è¾“å‡º: Person(name='Alice', age=30)
```

#### 6ï¸âƒ£ OutputFixingParser

å¦‚æœä½ ç”¨ PydanticOutputParser æˆ– JsonOutputParserï¼Œæœ‰æ—¶å€™æ¨¡å‹è¿”å›çš„æ ¼å¼å¹¶ä¸å®Œå…¨æ­£ç¡®ï¼ˆæ¯”å¦‚ç¼ºå°‘å­—æ®µï¼‰ã€‚
OutputFixingParser ä¼šè‡ªåŠ¨è®©æ¨¡å‹é‡æ–°æ ¼å¼åŒ–è¾“å‡ºï¼Œç›´åˆ°ç¬¦åˆé¢„æœŸã€‚

```python
from langchain.output_parsers import OutputFixingParser

parser = OutputFixingParser.from_llm(parser=PydanticOutputParser(pydantic_object=Person), llm=llm)
# llm: ChatOpenAI ç­‰å¤§æ¨¡å‹å¯¹è±¡
```

## äº”ã€è®°å¿†ï¼ˆMemoryï¼‰

åœ¨ å¤šè½®å¯¹è¯ æˆ–è€… Agent åœºæ™¯ä¸­ï¼Œå¤§æ¨¡å‹å¾€å¾€éœ€è¦è®°ä½ç”¨æˆ·ä¹‹å‰è¯´è¿‡ä»€ä¹ˆï¼Œæ‰èƒ½æ›´è‡ªç„¶ã€è¿è´¯åœ°å›ç­”ã€‚
LangChain çš„ Memoryï¼Œå°±æ˜¯ä¸“é—¨ç”¨æ¥ä¿å­˜ä¸Šä¸‹æ–‡å¯¹è¯è®°å½•ï¼Œå¹¶åœ¨æ¯æ¬¡è°ƒç”¨æ—¶åŠ¨æ€è¡¥å……ä¸Šä¸‹æ–‡ï¼Œè®©æ¨¡å‹å…·å¤‡ã€Œè®°å¿†åŠ›ã€ã€‚


| ç±» / ç±»å‹                          | è¯´æ˜                                                        |
|---------------------------------|-----------------------------------------------------------|
| `ConversationBufferMemory`      | æœ€ç®€å•çš„ã€Œä¸Šä¸‹æ–‡ç¼“å†²ã€å†…å­˜ï¼ŒæŒ‰é¡ºåºä¿å­˜æ‰€æœ‰æ¶ˆæ¯                          |
| `ConversationSummaryMemory`     | å¯¹å†å²å¯¹è¯åšã€Œæ€»ç»“ã€ä¿å­˜ï¼ˆé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰                           |
| `ConversationBufferWindowMemory`| åªä¿å­˜æœ€è¿‘ N è½®å¯¹è¯ï¼ŒèŠ‚çœä¸Šä¸‹æ–‡é•¿åº¦                                  |
| `ConversationKGMemory`          | ä½¿ç”¨ã€ŒçŸ¥è¯†å›¾è°±ã€æ–¹å¼å­˜å‚¨è®°å¿†ï¼Œæ›´ç»“æ„åŒ–                                   |
| `VectorStoreRetrieverMemory`    | å°†è®°å¿†åµŒå…¥å‘é‡æ•°æ®åº“ï¼Œæ£€ç´¢ç›¸ä¼¼ä¸Šä¸‹æ–‡ï¼ˆæ›´å…ˆè¿›ï¼‰                             |
    

```python
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

# å®šä¹‰ LLM
llm = ChatOpenAI(model="gpt-4o")

# å®šä¹‰è®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾ï¼Œè‡ªåŠ¨æºå¸¦è®°å¿†
conversation = ConversationChain(
    llm=llm,
    memory=memory
)

# å¤šè½®å¯¹è¯
print(conversation.invoke({"input": "ä½ å¥½ï¼"}))
print(conversation.invoke({"input": "ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ"}))
print(conversation.invoke({"input": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}))
```
æ¨¡å‹åœ¨æ¯æ¬¡ç”Ÿæˆæ—¶ï¼Œä¼šè‡ªåŠ¨æ‹¼æ¥ä¸Šä¸‹æ–‡å†å²ï¼ˆMemory ä¸­çš„æ¶ˆæ¯ï¼‰ï¼Œå›ç­”æ›´è‡ªç„¶ï¼

----

#### ğŸ”¶ Memory å†…éƒ¨ç»“æ„

æ‰€æœ‰ Memory éƒ½æ˜¯åŸºäº `BaseMemory` æŠ½è±¡ç±»ï¼Œæ ¸å¿ƒæ–¹æ³•æœ‰ï¼š

| æ–¹æ³•                              | è¯´æ˜                                                   |
|---------------------------------|------------------------------------------------------|
| `load_memory_variables(inputs)` | åŠ è½½è®°å¿†å˜é‡ï¼Œè¿”å›ä¸€ä¸ª dictï¼Œé€šå¸¸æ˜¯å†å²å¯¹è¯æ–‡æœ¬ï¼ˆ`history`ï¼‰      |
| `save_context(inputs, outputs)` | åœ¨æ¨¡å‹è°ƒç”¨åï¼Œä¿å­˜ã€Œç”¨æˆ·è¾“å…¥ & æ¨¡å‹è¾“å‡ºã€åˆ° Memory            |
| `clear()`                        | æ¸…ç©ºè®°å¿†                                                 |
    
    
## å…­ã€å·¥å…·ï¼ˆToolsï¼‰

### å·¥å…·çš„æ ¸å¿ƒæŠ½è±¡ï¼šBaseTool

langchain_core.tools ä¸­çš„æ ¸å¿ƒåŸºç±»æ˜¯ï¼š

```python
from langchain_core.tools import BaseTool
```

ğŸ”¹ BaseTool å®šä¹‰äº†ã€Œå·¥å…·ã€çš„æ ‡å‡†æ¥å£ï¼š
+ name: å·¥å…·çš„åç§°ï¼ˆæ¨¡å‹éœ€è¦ç”¨å®ƒæ¥è°ƒç”¨ï¼‰
+ description: å·¥å…·çš„æè¿°ï¼ˆå¸®åŠ©æ¨¡å‹ç†è§£å®ƒèƒ½åšä»€ä¹ˆï¼‰
+ args_schema: å·¥å…·å‚æ•°çš„ Schemaï¼ˆå‚æ•°æ ¡éªŒï¼Œé€šå¸¸ç”¨ Pydantic BaseModelï¼‰
+ invoke() / run(): å·¥å…·çœŸæ­£è¢«è°ƒç”¨æ—¶çš„é€»è¾‘

### å®šä¹‰ä¸€ä¸ªå·¥å…·ç¤ºä¾‹

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åŸºäº BaseTool è‡ªå®šä¹‰ä¸€ä¸ªå·¥å…·ï¼š

```python
from langchain_core.tools import BaseTool
from pydantic import BaseModel, Field

# å®šä¹‰å‚æ•° Schema
class CalculatorArgs(BaseModel):
    a: int = Field(..., description="ç¬¬ä¸€ä¸ªæ•°å­—")
    b: int = Field(..., description="ç¬¬äºŒä¸ªæ•°å­—")

# è‡ªå®šä¹‰å·¥å…·
class AddTool(BaseTool):
    name = "add_numbers"
    description = "è®¡ç®—ä¸¤ä¸ªæ•°å­—ä¹‹å’Œ"
    args_schema = CalculatorArgs

    def _run(self, a: int, b: int) -> int:
        return a + b

    async def _arun(self, a: int, b: int) -> int:
        # è¿™é‡Œä¹Ÿå¯ä»¥å†™å¼‚æ­¥é€»è¾‘
        return a + b
```

### å†…ç½®å·¥å…·ï¼štool

é™¤äº†æ‰‹åŠ¨ç»§æ‰¿ BaseToolï¼ŒLangChain ä¹Ÿæä¾›äº†å¿«é€Ÿæ³¨å†Œå·¥å…·çš„è£…é¥°å™¨ @toolï¼Œè®©å®šä¹‰æ›´ç®€å•ï¼

```python
from langchain_core.tools import tool

@tool
def add_numbers(a: int, b: int) -> int:
    """è®¡ç®—ä¸¤ä¸ªæ•°å­—ä¹‹å’Œ"""
    return a + b
```


    
    
    
    
    

    
    

    
    

    
    
    
    
    
    
    

    
    
    
    

