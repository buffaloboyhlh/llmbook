# æ³¨æ„åŠ›æœºåˆ¶

---

## ä¸€ã€ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰

2014 å¹´ã€ŠNeural Machine Translation by Jointly Learning to Align and Translateã€‹æå‡ºçš„å•å¤´æ³¨æ„åŠ›ï¼Œè¾“å…¥çš„ Queryã€Key å’Œ Value
çŸ©é˜µéƒ½æ˜¯å®Œæ•´çš„å¼ é‡ã€‚

ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æ—©äº Transformer è¢«æå‡ºï¼Œå—åˆ°çš„å…³æ³¨å¹¶ä¸å¤šï¼Œå…¶å†…éƒ¨åªå®ç°äº† $q,k,v$ çš„æ³¨æ„åŠ›è®¡ç®—ã€‚

+ è¾“å…¥æ˜¯ query å’Œ key-valueï¼Œæ³¨æ„åŠ›æœºåˆ¶é¦–å…ˆè®¡ç®— query ä¸æ¯ä¸ª key çš„å…³è”æ€§
+ æ¯ä¸ªå…³è”æ€§ä½œä¸ºæ¯ä¸ª value çš„æƒé‡ (weight)ï¼Œå„ä¸ªæƒé‡ä¸ value çš„ä¹˜ç§¯ç›¸åŠ å¾—åˆ°è¾“å‡ºã€‚
+ **SDPA å¯ä»¥è¢«è®¤ä¸ºæ˜¯ MHA çš„ä¸­é—´æ­¥éª¤ï¼**

#### 1.1 æ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬æ•°å­¦å½¢å¼

æœ€åŸºç¡€çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Scaled Dot-Product Attentionï¼ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)V
$$

+ Qï¼šQueryï¼ˆæŸ¥è¯¢ï¼‰
+ Kï¼šKeyï¼ˆé”®ï¼‰
+ Vï¼šValueï¼ˆå€¼ï¼‰
+ d_kï¼šKey çš„ç»´åº¦ï¼ˆç”¨äºç¼©æ”¾ï¼‰

#### 1.2 æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰

åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œ**æ©ç ï¼ˆMaskï¼‰ç”¨äºæ§åˆ¶æ³¨æ„åŠ›çš„è®¡ç®—èŒƒå›´**ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿå¿½ç•¥æŸäº› tokenï¼Œæˆ–é¿å…è®¿é—®æœªæ¥çš„ä¿¡æ¯ã€‚

##### ğŸ§© æ ¸å¿ƒä½œç”¨ï¼š

åœ¨æ³¨æ„åŠ›å…¬å¼ä¸­åŠ å…¥ maskï¼Œä½¿å¾—æŸäº›ä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°å˜ä¸ºæå°å€¼ï¼ˆè´Ÿæ— ç©·ï¼‰ï¼Œä»è€Œåœ¨ softmax åå˜æˆ 0ï¼Œä¸å‚ä¸åŠ æƒæ±‚å’Œï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} + \text{mask} \right) V
$$

##### ğŸ“š å¸¸è§ç±»å‹

| Mask åç§°                | ä½œç”¨                            | åº”ç”¨åœºæ™¯ï¼ˆæ¨¡å‹ï¼‰                  |
|------------------------|-------------------------------|---------------------------|
| `padding_mask`         | å¿½ç•¥å¡«å……ç¬¦$ï¼ˆ<PAD>ï¼‰$çš„ä½ç½®             | æ‰€æœ‰ Encoder/Decoder æ¨¡å‹     |
| `causal_mask`          | é˜²æ­¢æ³¨æ„åŠ›â€œå·çœ‹â€æœªæ¥çš„ä¿¡æ¯                | GPT / Transformer Decoder |
| `combined_mask`        | åŒæ—¶è€ƒè™‘ padding å’Œ causal         | è§£ç å™¨ä¸­çš„è‡ªæ³¨æ„åŠ›å±‚                |
| `cross_attention_mask` | æ§åˆ¶ cross-attention ä¸­ key çš„æœ‰æ•ˆæ€§ | Transformer è§£ç å™¨äº¤å‰æ³¨æ„åŠ›      |

###### ğŸ” ä¸€ã€Padding Maskï¼ˆå¡«å……æ©ç ï¼‰

åœ¨ NLP ä¸­ï¼Œè¾“å…¥é•¿åº¦å¸¸ä¸åŒï¼Œéœ€è¦ç»Ÿä¸€æˆå®šé•¿ï¼Œç”¨ <PAD> è¿›è¡Œå¡«å……ã€‚ä¸ºäº†ä¸è®©æ¨¡å‹å…³æ³¨è¿™äº›å¡«å……å€¼ï¼Œéœ€è¦å±è”½æ‰å®ƒä»¬ã€‚

åŸå§‹è¾“å…¥ï¼š

```python
input_1 = [æˆ‘, çˆ±, ä½ ]
input_2 = [ä½ , å¥½]
```

å¡«å……åï¼ˆé•¿åº¦ç»Ÿä¸€ä¸º 4ï¼‰ï¼š

```python
input_1 = [æˆ‘, çˆ±, ä½ , < PAD >]
input_2 = [ä½ , å¥½, < PAD >, < PAD >]
```

å¯¹åº”çš„ padding_maskï¼š

```python
mask_1 = [1, 1, 1, 0]
mask_2 = [1, 1, 0, 0]
```

###### â³ äºŒã€Causal Maskï¼ˆå› æœæ©ç ï¼‰

åœ¨è‡ªå›å½’ç”Ÿæˆä»»åŠ¡ä¸­ï¼ˆå¦‚ GPTï¼‰ï¼Œå½“å‰å•è¯ä¸èƒ½çœ‹åˆ°æœªæ¥å•è¯ã€‚Causal Mask ä¿è¯äº†ä¿¡æ¯åªä»è¿‡å»æµå‘ç°åœ¨ï¼Œé¿å…â€œæœªæ¥æ³„æ¼â€ã€‚

ç”Ÿæˆä¸€ä¸ª ä¸‹ä¸‰è§’çŸ©é˜µï¼ˆLower Triangular Maskï¼‰ï¼š

```python
[[1, 0, 0, 0],
 [1, 1, 0, 0],
 [1, 1, 1, 0],
 [1, 1, 1, 1]]
```

æ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä¼šå°†ä¸Šä¸‰è§’åŒºåŸŸçš„ attention scores è®¾ç½®ä¸º -âˆã€‚

```python
seq_len = 4
causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool()
```

###### ğŸ”€ ä¸‰ã€Combined Maskï¼ˆè”åˆæ©ç ï¼‰

åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œå¸¸å¸¸éœ€è¦åŒæ—¶ä½¿ç”¨ padding mask å’Œ causal maskï¼Œå°¤å…¶æ˜¯åœ¨ Transformer çš„è§£ç å™¨ä¸­ã€‚

å‡è®¾æœ‰ä¸€ä¸ªå¥å­ï¼š

```python
input = [æˆ‘, çˆ±, < PAD >]
padding_mask = [1, 1, 0]

causal_mask =
[[1, 0, 0],
 [1, 1, 0],
 [1, 1, 1]]
```

åˆå¹¶åï¼š

```python
combined_mask = causal_mask & padding_mask_broadcast
```

å®ç°æ–¹å¼å¦‚ä¸‹ï¼š

```python
combined_mask = causal_mask & padding_mask.unsqueeze(1).expand(-1, seq_len, -1)
```

#### 1.3 å®ç°ä»£ç 

```python
import torch
import torch.nn as nn


class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super(ScaledDotProductAttention, self).__init__()

    def forward(self, query, key, value, casual_mask=None, padding_mask=None):
        '''
        query, key, value å½¢çŠ¶: (batch_size, seq_len, hidden_size)
        :param query:
        :param key:
        :param value:
        :param casual_mask: å› æœæ©ç 
        :param padding_mask: å¡«å……æ©ç 
        :return:
        '''

        d_k = query.size(-1)  # è·å– hidden_size

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        # key.transpose(-1, -2) å°†æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡Œè½¬ç½®ï¼Œä»¥è¿›è¡Œç‚¹ç§¯
        # attention_scores å½¢çŠ¶: (batch_size, seq_len, seq_len)
        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(
            torch.tensor(d_k, dtype=torch.float32))

        # æ·»åŠ æ³¨æ„åŠ›æ©ç ï¼ˆseq_len, seq_lenï¼‰ï¼Œæ©ç ä½ç½®ï¼ˆ1ï¼‰çš„å€¼ä¸ºè´Ÿæ— ç©·
        if casual_mask is not None:
            attention_scores += casual_mask * -1e9  # -1e9 ä»£è¡¨è´Ÿæ— ç©·

        # æ·»åŠ å¡«å……ä½ç½®çš„æ©ç ï¼Œæ¯ä¸ªå¥å­ä¸ä¸€æ ·ï¼ˆbatch_size, seq_len)
        if padding_mask is not None:
            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)  # æ‰©å±•æˆä¸æ³¨æ„åŠ›æƒé‡çŸ©é˜µå¯å¹¿æ’­çš„å½¢çŠ¶
            attention_scores += padding_mask * -1e9

        # å¯¹æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æ¦‚ç‡
        attention_probs = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)
        # è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºï¼Œé€šè¿‡æ³¨æ„åŠ›æ¦‚ç‡åŠ æƒå€¼

        attention_output = torch.matmul(attention_probs, value)  # (batch_size, num_heads, seq_len, hidden_size)

        return attention_output
```

**éªŒè¯attention**

```python
def test_attention():
    batch_size = 16
    seq_length = 32
    hidden_size = 512

    query = torch.randn(batch_size, seq_length, hidden_size)
    key = torch.randn(batch_size, seq_length, hidden_size)
    value = torch.randn(batch_size, seq_length, hidden_size)

    attention = ScaledDotProductAttention()
    output = attention(query, key, value, casual_mask=None, padding_mask=None)

    print("Query shape:", query.shape)
    print("Key shape:", key.shape)
    print("Value shape:", value.shape)
    print("Output shape:", output.shape)


test_attention()
```

**è¾“å‡º**

```text
Query shape: torch.Size([16, 32, 512])
Key shape: torch.Size([16, 32, 512])
Value shape: torch.Size([16, 32, 512])
Output shape: torch.Size([16, 32, 512])
```

## äºŒã€å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰

å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer æ¨¡å‹ä¸­çš„æ ¸å¿ƒç»„ä»¶ã€‚åœ¨å…¶è®¾è®¡ä¸­ï¼Œã€Œå¤šå¤´ã€æ„å‘³ç€è¯¥æœºåˆ¶å¹¶ä¸åªè®¡ç®—ä¸€ç§æ³¨æ„åŠ›æƒé‡ï¼Œè€Œæ˜¯å¹¶è¡Œè®¡ç®—å¤šç§æƒé‡ï¼Œæ¯ç§æƒé‡éƒ½ä»ä¸åŒçš„ã€Œè§†è§’ã€æ•è·è¾“å…¥çš„ä¸åŒä¿¡æ¯ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1ã€ä¸ºè¾“å…¥åºåˆ—ä¸­è®¡ç®— Q, K, V ï¼Œè¿™æ˜¯é€šè¿‡å°†è¾“å…¥è¯å‘é‡ä¸ä¸‰ä¸ªæƒé‡çŸ©é˜µç›¸ä¹˜å®ç°çš„:

$$
\begin{aligned} & Q = X W_q \\ & K = X W_k \\ & V = X W_v \end{aligned}
$$

2ã€è®¡ç®— Q, K æ³¨æ„åŠ›å¾—åˆ†ï¼Œå…¶ä¸­ï¼Œ $d_k$ æ˜¯$k$çš„ç»´åº¦ï¼š

$$
\operatorname{score}(Q, K) = \frac{Q \cdot K^T}{\sqrt{d_k}}
$$

3ã€ä½¿ç”¨ Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼š

$$
\operatorname{Attention}(Q, K) = \operatorname{softmax}(\operatorname{score}(Q, K))=\operatorname{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right)
$$

4ã€ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å’ŒV ï¼Œè®¡ç®—è¾“å‡ºï¼š

$$
\text{Output} = \operatorname{Attention}(Q, K) \cdot V = \operatorname{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d_k}}\right) \cdot V
$$

5ã€ æ‹¼æ¥å¤šå¤´è¾“å‡ºï¼Œä¹˜ä»¥ $W_O$ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºï¼š

$$
\text{MultiHeadOutput} = \text{Concat} (\text{Output}^1, \text{Output}^2, \ldots, \text{Output}^H) W_O
$$


```python
import torch
from torch import nn

class MultiHeadAttention(torch.nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦ï¼ŒäºŒè€…å¿…é¡»æ•´é™¤
        
        # åˆå§‹åŒ– Qã€Kã€V çš„æŠ•å½±çŸ©é˜µï¼Œå°†è¾“å…¥è¯å‘é‡çº¿æ€§å˜æ¢ä¸º Qã€Kã€Vï¼Œç»´åº¦ä¿æŒä¸€è‡´
        self.q_linear = nn.Linear(hidden_size, hidden_size) 
        self.k_linear = nn.Linear(hidden_size, hidden_size)
        self.v_linear = nn.Linear(hidden_size, hidden_size)
        
        # è¾“å‡ºçº¿æ€§å±‚ï¼Œå°†æ‹¼æ¥åçš„å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºå˜æ¢ä¸ºæ‰€éœ€çš„è¾“å‡ºç»´åº¦ï¼Œè¿™é‡Œç»´åº¦ä¿æŒä¸€è‡´
        self.o_linear = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, hidden_state, causal_mask=None, padding_mask=None):
        # hidden_state å½¢çŠ¶: (batch_size, seq_len, hidden_size)
        batch_size = hidden_state.size(0)  # è·å–æ‰¹é‡å¤§å°

        # è®¡ç®— Qã€Kã€Vï¼Œçº¿æ€§å˜æ¢ï¼Œå¾—åˆ°å½¢çŠ¶ï¼š(batch_size, seq_len, hidden_size)
        query = self.q_linear(hidden_state)
        key = self.k_linear(hidden_state)
        value = self.v_linear(hidden_state)
        
        # å°†æ¯ä¸ªå¤´çš„ç»´åº¦æ‹†åˆ†å‡ºæ¥ï¼Œå¾—åˆ°å½¢çŠ¶ï¼š(batch_size, num_heads, seq_len, head_dim)
        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œattention_scores å½¢çŠ¶: (batch_size, num_heads, seq_len, seq_len)
        attention_scores = torch.matmul(query, key.transpose(-1, -2)) \
        / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        
        # æ·»åŠ å› æœæ³¨æ„åŠ›æ©ç ï¼ˆseq_len, seq_lenï¼‰ï¼Œæ©ç ä½ç½®ï¼ˆ1ï¼‰çš„å€¼ä¸ºè´Ÿæ— ç©·ï¼Œè‡ªåŠ¨å¹¿æ’­
        if causal_mask is not None:
            attention_scores += causal_mask * -1e9
        
        # æ·»åŠ å¡«å……ä½ç½®çš„æ©ç ï¼Œæ¯ä¸ªå¥å­ä¸ä¸€æ ·ï¼ˆbatch_size, seq_len)
        if padding_mask is not None:
            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, seq_len)
            attention_scores += padding_mask * -1e9
            
        # å¯¹æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æ¦‚ç‡
        attention_probs = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)
        # å¦‚æœæœ‰ dropout æ“ä½œå°±åŠ åœ¨è¿™ï¼Œself.dropout(attention_probs)ï¼Œä¹Ÿå¯ä»¥åœ¨å‡½æ•°å¤–é¢åŠ 

        # è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºï¼Œé€šè¿‡æ³¨æ„åŠ›æ¦‚ç‡åŠ æƒå€¼
        output = torch.matmul(attention_probs, value)  # (batch_size, num_heads, seq_len, head_dim)
        
        # å¯¹å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œæ‹¼æ¥ï¼Œå°†å½¢çŠ¶è°ƒæ•´ä¸º (batch_size, seq_len, hidden_size)
        # å…ˆ output.transpose(1, 2) å°† num_heads å’Œ seq_len ç»´åº¦è½¬ç½®
        output = output.transpose(1, 2).view(batch_size, -1, self.head_dim * self.num_heads)
        
        # é€šè¿‡çº¿æ€§å±‚å°†æ‹¼æ¥åçš„è¾“å‡ºå˜æ¢ä¸ºæ‰€éœ€çš„è¾“å‡ºç»´åº¦
        output = self.o_linear(output)  # (batch_size, seq_len, hidden_size)
        return output

def test_MHA():
    batch_size = 128
    seq_len = 512
    hidden_size = 1024
    num_heads = 8
    
    # éšæœºç”Ÿæˆè¾“å…¥æ•°æ®
    hidden_state = torch.randn(batch_size, seq_len, hidden_size)  # (batch_size, seq_len, hidden_size)
    
    # ç”Ÿæˆå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰ï¼Œè¿™é‡Œå°±ä¸åˆ»æ„ç”Ÿæˆ padding_mask
    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    
    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—
    mha = MultiHeadAttention(hidden_size, num_heads)
    
    # è®¡ç®—å¤šå¤´æ³¨æ„åŠ›è¾“å‡º
    output = mha(hidden_state, causal_mask=causal_mask)
    
    print("Input shape:", hidden_state.shape)
    print("Output shape:", output.shape)
    
if __name__ == "__main__":
	test_MHA()
```

## ä¸‰ã€MHA with KV Cache

é”®å€¼ç¼“å­˜ï¼ˆKV Cacheï¼‰ä¸»è¦ç”¨äº Decoder çš„ Next Token Prediction æ—¶å‡å°‘é‡å¤è®¡ç®—ï¼Œé€šè¿‡ç¼“å­˜å¹¶é€æ­¥æ›´æ–°é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ï¼Œæ¥ç”¨ç©ºé—´æ¢æ—¶é—´ã€‚

ä½†è¦æ³¨æ„å³ä½¿æ˜¯ Decoder-only çš„æ¨¡å‹ï¼Œåœ¨é¢„å¤„ç†è¾“å…¥ï¼ˆPrefillï¼‰çš„æ—¶å€™ä¹Ÿä¸éœ€è¦åˆ©ç”¨ KV Cacheï¼ˆP/D åˆ†ç¦»ï¼‰ï¼Œæœ¬ä»£ç åªä½œä¸ºç¤ºä¾‹ï¼š

```python
import torch
from torch import nn

class MultiHeadAttention(torch.nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.q_linear = nn.Linear(hidden_size, hidden_size) 
        self.k_linear = nn.Linear(hidden_size, hidden_size)
        self.v_linear = nn.Linear(hidden_size, hidden_size)
        self.o_linear = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, hidden_state, causal_mask=None, past_key_value=None, use_cache=False):
        batch_size = hidden_state.size(0)
        
        # è®¡ç®— Qã€Kã€Vï¼Œæ³¨æ„æ­¤æ—¶åªæœ‰ä¸€ä¸ª Token
        query = self.q_linear(hidden_state)  # (batch_size, 1, hidden_size)
        key = self.k_linear(hidden_state)
        value = self.v_linear(hidden_state)
        
        # åˆ†å‰²å¤šå¤´ï¼Œå¾—åˆ°å½¢çŠ¶ï¼š(batch_size, num_heads, 1, head_dim)
        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è‹¥å­˜åœ¨ç¼“å­˜ï¼Œæ‹¼æ¥å½“å‰ Kã€V
        if past_key_value is not None:
            past_key, past_value = past_key_value
            key = torch.cat([past_key, key], dim=2)	  # (batch_size, num_heads, seq_len, head_dim)
            value = torch.cat([past_value, value], dim=2)
        
        # ä¿å­˜æ–°çš„ç¼“å­˜
        new_past_key_value = (key, value) if use_cache else None
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œattention_scores å½¢çŠ¶: (batch_size, num_heads, 1, seq_len)
        attention_scores = torch.matmul(query, key.transpose(-1, -2)) \
        / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        
        # åº”ç”¨å› æœæ©ç ï¼ˆè‹¥éœ€è¦ï¼‰
        if causal_mask is not None:
            attention_scores += causal_mask * -1e9
            
        # è®¡ç®—æ³¨æ„åŠ›è¾“å‡º
        attention_probs = torch.softmax(attention_scores, dim=-1)
        output = torch.matmul(attention_probs, value)
        
        # åˆå¹¶å¤šå¤´å¹¶çº¿æ€§å˜æ¢
        output = output.transpose(1, 2).view(batch_size, -1, self.num_heads * self.head_dim)
        output = self.o_linear(output)
        
        return (output, new_past_key_value) if use_cache else output

def test_MHA_with_cache():
    batch_size = 2
    seq_len = 5
    hidden_size = 64
    num_heads = 4
    
    # æ„é€ è¾“å…¥ï¼Œæ¨¡æ‹Ÿæ•´ä¸ªåºåˆ—
    hidden_state = torch.randn(batch_size, seq_len, hidden_size)
    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    
    # åˆ»æ„åˆ†æ­¥å¤„ç†ï¼Œä½¿ç”¨ KV ç¼“å­˜
    past_key_value = None
    outputs = []
    for i in range(seq_len):
        # å½“å‰æ­¥éª¤çš„è¾“å…¥ï¼ˆå•ä¸ª tokenï¼‰
        current_input = hidden_state[:, i:i+1, :]
        # ç”Ÿæˆå½“å‰æ­¥éª¤çš„å› æœæ©ç ï¼ˆä»…å…è®¸å…³æ³¨åˆ°å½“å‰ä½ç½®åŠä¹‹å‰çš„ï¼‰
        current_causal_mask = causal_mask[i:i+1, :i+1]  # (1, i+1)
        # å‰å‘ä¼ æ’­
        output_step, past_key_value = mha(
            current_input,
            causal_mask=current_causal_mask,
            past_key_value=past_key_value,
            use_cache=True
        )
        outputs.append(output_step)
    
    # åˆå¹¶åˆ†å¸ƒè¾“å‡º
    output = torch.cat(outputs, dim=1)
    
    print("Input shape:", hidden_state.shape)
    print("Output shape:", output.shape)

if __name__ == "__main__":
    test_MHA_with_cache()
```

## å››ã€å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQA)

å›´ç»•ã€Œå¦‚ä½•å‡å°‘ KV Cache åŒæ—¶å°½å¯èƒ½åœ°ä¿è¯æ•ˆæœã€è¿™ä¸ªä¸»é¢˜å‘å±•è€Œæ¥çš„äº§ç‰©ã€‚åªæœ‰ä¸€ç»„ key-value å¯¹ï¼Œç”±ã€ŠFast Transformer Decoding: One Write-Head is All You Needã€‹åœ¨ 2019 å¹´æå‡ºã€‚

ä¸ MHA ä¸åŒçš„æ˜¯ï¼ŒMQA è®©æ‰€æœ‰çš„å¤´ä¹‹é—´å…±äº«åŒä¸€ä»½ Key å’Œ Value çŸ©é˜µï¼Œæ¯ä¸ªå¤´åªå•ç‹¬ä¿ç•™äº†ä¸€ä»½ Query å‚æ•°ï¼Œä»è€Œå¤§å¤§å‡å°‘ Key å’Œ Value çŸ©é˜µçš„å‚æ•°é‡ã€‚ä½¿ç”¨ MQA çš„æ¨¡å‹åŒ…æ‹¬ PaLMã€StarCoderã€Gemini ç­‰ã€‚

```python
import torch
from torch import nn

class MultiQueryAttention(torch.nn.Module):
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        # åˆå§‹åŒ– Qã€Kã€V æŠ•å½±çŸ©é˜µï¼Œæ³¨æ„è¿™é‡Œçš„ K V æ¯”åŸæ¥æ›´å°
        self.q_linear = nn.Linear(hidden_size, hidden_size)
        self.k_linear = nn.Linear(hidden_size, self.head_dim)
        self.v_linear = nn.Linear(hidden_size, self.head_dim)
        
        self.o_linear = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, hidden_state, causal_mask=None, padding_mask=None):
        batch_size = hidden_state.size(0)
        
        query = self.q_linear(hidden_state)  # (batch_size, seq_len, hidden_size)
        key = self.k_linear(hidden_state)    # (batch_size, seq_len, head_dim)
        value = self.v_linear(hidden_state)  # (batch_size, seq_len, head_dim)
        
        # åˆ†å‰²å¤´éƒ¨ï¼ŒK V çŸ©é˜µä¹Ÿè¦åŠ ä¸Šä¸€ä¸ªç»´åº¦
        query = self.split_head(query)  # (batch_size, num_heads, seq_len, head_dim)
        key = self.split_head(key, 1)   # (batch_size, 1, seq_len, head_dim)
        value = self.split_head(value, 1) # (batch_size, 1, seq_len, head_dim)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œè‡ªåŠ¨å¹¿æ’­ï¼Œ(batch_size, num_heads, seq_len, seq_len)
        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        
        if causal_mask is not None:
            attention_scores += causal_mask * -1e9  

        if padding_mask is not None:
            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)
            attention_scores += padding_mask * -1e9
            
        attention_probs = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)

        output = torch.matmul(attention_probs, value)  # (batch_size, num_heads, seq_len, head_dim)
        
        # å¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œæ‹¼æ¥ï¼Œ(batch_size, seq_len, hidden_size)
        output = output.transpose(1, 2).view(batch_size, -1, self.head_dim * self.num_heads)
        
        output = self.o_linear(output)  # (batch_size, seq_len, hidden_size)
        
        return output

    def split_head(self, x, head_num=None):
        batch_size = x.size(0)  # è·å–æ‰¹é‡å¤§å°
        if head_num is None:
            head_num = self.num_heads  # é»˜è®¤ä½¿ç”¨ç±»ä¸­çš„ num_heads
        
        # è¿”å›å½¢çŠ¶: (batch_size, head_num, seq_len, head_dim)
        return x.reshape(batch_size, -1, head_num, self.head_dim).transpose(1, 2)
```

## äº”ã€åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰

æœ‰äººæ‹…å¿ƒ MQA å¯¹ KV Cache çš„å‹ç¼©å¤ªä¸¥é‡ï¼Œäºæ˜¯æå‡ºäº†ä¸€ä¸ªæŠ˜ä¸­ç‰ˆæœ¬ï¼Œå‡ºè‡ª 2023 å¹´è®ºæ–‡ã€ŠGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpointsã€‹ã€‚




```python
import torch
from torch import nn

class GroupQueryAttention(torch.nn.Module):
    def __init__(self, hidden_size, num_heads, group_num):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.group_num = group_num  # ç»„çš„æ•°é‡
        
        # åˆå§‹åŒ– Qã€Kã€V æŠ•å½±çŸ©é˜µï¼Œæ³¨æ„è¿™é‡Œçš„ K V åšäº†æŠ˜è¡·
        self.q_linear = nn.Linear(hidden_size, hidden_size)  # (hidden_size, hidden_size)
        self.k_linear = nn.Linear(hidden_size, self.group_num * self.head_dim)  # (hidden_size, group_num * head_dim)
        self.v_linear = nn.Linear(hidden_size, self.group_num * self.head_dim)  # (hidden_size, group_num * head_dim)
        
        self.o_linear = nn.Linear(hidden_size, hidden_size)  # (hidden_size, hidden_size)
        
    def forward(self, hidden_state, causal_mask=None, padding_mask=None):
        batch_size = hidden_state.size(0)
        
        query = self.q_linear(hidden_state)  # (batch_size, seq_len, hidden_size)
        key = self.k_linear(hidden_state)    # (batch_size, seq_len, group_num * head_dim)
        value = self.v_linear(hidden_state)  # (batch_size, seq_len, group_num * head_dim)
        
        # åˆ†å‰²å¤´éƒ¨ï¼Œå°†æ¯ä¸ªå¤´çš„ç»´åº¦æ‹†åˆ†å‡ºæ¥
        query = self.split_head(query)  # (batch_size, num_heads, seq_len, head_dim)
        key = self.split_head(key, self.group_num)  # (batch_size, num_heads, seq_len, head_dim)
        value = self.split_head(value, self.group_num)  # (batch_size, num_heads, seq_len, head_dim)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œè‡ªåŠ¨å¹¿æ’­ï¼Œ(batch_size, num_heads, seq_len, seq_len)
        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        
        if causal_mask is not None:
            attention_scores += causal_mask * -1e9  

        if padding_mask is not None:
            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)
            attention_scores += padding_mask * -1e9
        
        attention_probs = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)
        
        output = torch.matmul(attention_probs, value)  # (batch_size, num_heads, seq_len, head_dim)
        
        # å¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œæ‹¼æ¥ï¼Œå½¢çŠ¶: (batch_size, seq_len, hidden_size)
        output = output.transpose(1, 2).view(batch_size, -1, self.head_dim * self.num_heads)
        
        # é€šè¿‡çº¿æ€§å±‚å°†æ‹¼æ¥åçš„è¾“å‡ºå˜æ¢ä¸ºæ‰€éœ€çš„è¾“å‡ºç»´åº¦
        output = self.o_linear(output)  # (batch_size, seq_len, hidden_size)
        
        return output

    def split_head(self, x, group_num=None):
        batch_size, seq_len = x.size()[:2]  # è·å–æ‰¹é‡å¤§å°å’Œåºåˆ—é•¿åº¦
        
        if group_num is None:
            return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        else:
            # å°† hidden_size åˆ†å‰²ä¸º group_num å’Œ head_dim
            x = x.view(batch_size, -1, group_num, self.head_dim).transpose(1, 2)
            # å†å°†å…¶æ‰‹åŠ¨ expand åˆ°ç›¸åŒå¤§å°
            x = x[:, :, None, :, :].expand(batch_size, group_num, self.num_heads // group_num, seq_len, self.head_dim).view(batch_size, self.num_heads, seq_len, self.head_dim)
            return x 	# å½¢çŠ¶: (batch_size, num_heads, seq_len, head_dim)
```

