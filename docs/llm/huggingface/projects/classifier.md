# æ–‡æœ¬åˆ†ç±»

æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤—
Transformersä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä»»åŠ¡æ¥æºäº[GLUE Benchmark](https://gluebenchmark.com/).

![text_classification.png](../../../imgs/llm/huggingface/text_classification.png)

å¯¹äºä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç®€å•çš„Datasetåº“åŠ è½½æ•°æ®é›†ï¼ŒåŒæ—¶ä½¿ç”¨transformerä¸­çš„Traineræ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

GLUEæ¦œå•åŒ…å«äº†9ä¸ªå¥å­çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯ï¼š

- [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) é‰´åˆ«ä¸€ä¸ªå¥å­æ˜¯å¦è¯­æ³•æ­£ç¡®.
- [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) ç»™å®šä¸€ä¸ªå‡è®¾ï¼Œåˆ¤æ–­å¦ä¸€ä¸ªå¥å­ä¸è¯¥å‡è®¾çš„å…³ç³»ï¼šentails,
  contradicts æˆ–è€… unrelatedã€‚
- [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus)
  åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦äº’ä¸ºparaphrases.
- [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) åˆ¤æ–­ç¬¬2å¥æ˜¯å¦åŒ…å«ç¬¬1å¥é—®é¢˜çš„ç­”æ¡ˆã€‚
- [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) åˆ¤æ–­ä¸¤ä¸ªé—®å¥æ˜¯å¦è¯­ä¹‰ç›¸åŒã€‚
- [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment)åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦ä¸å‡è®¾æˆentailå…³ç³»ã€‚
- [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) åˆ¤æ–­ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿæ­£è´Ÿå‘.
- [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark)
  åˆ¤æ–­ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼ˆåˆ†æ•°ä¸º1-5åˆ†ï¼‰ã€‚
- [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference)
  Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not.

å¯¹äºä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç®€å•çš„Datasetåº“åŠ è½½æ•°æ®é›†ï¼ŒåŒæ—¶ä½¿ç”¨transformerä¸­çš„`Trainer`æ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

```python
GLUE_TASKS = ["cola", "mnli", "mnli-mm", "mrpc", "qnli", "qqp", "rte", "sst2", "stsb", "wnli"]
```


### Step1 å¯¼å…¥åº“

```python
import torch
import numpy as np
from transformers import AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer
from datasets import load_dataset
import evaluate

task = "sst2"
model_checkpoint = "distilbert-base-uncased"
```

### Step2 æ•°æ®é¢„å¤„ç†

ä½¿ç”¨GLEUä¸­çš„SST-2æ•°æ®é›†ï¼Œ åˆ¤æ–­ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿæ­£è´Ÿå‘ã€‚
```python
# åŠ è½½æ•°æ®é›† å’Œ é¢„å¤„ç†
dataset = load_dataset(path="glue", name=task)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,use_fast=True)

def preprocess_fn(examples):
    return tokenizer(examples["sentence"], truncation=True, max_length=512)

tokenized_dataset = dataset.map(preprocess_fn,batched=True)
print(tokenized_dataset)
labels =  tokenized_dataset['train'].features['label'].names
print("åˆ†ç±»æ ‡ç­¾:",labels)
```

### Step3 åŠ è½½æ¨¡å‹

```python
# åŠ è½½æ¨¡å‹

model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(labels))  # åªæœ‰ä¿©ä¸ªåˆ†ç±»


# æ¨¡å‹é¢„æµ‹å‡½æ•°
def model_predict(sentences):
    results = []
    inputs = tokenizer(sentences, truncation=True, max_length=512, return_tensors="pt", padding=True).to("cpu")
    model.to("cpu")
    model.eval()

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.argmax(torch.softmax(logits, dim=-1), dim=-1)
        for prob in probs:
            results.append(labels[int(prob.item())])

    return results


sentences = ["I really not like this movie", "this song is great"]
print("è®­ç»ƒå‰çš„æ¨¡å‹ï¼š", model_predict(sentences))
```

### Step4 åˆ¶å®šè¯„ä¼°æŒ‡æ ‡

```python
# åŠ è½½è¯„ä¼°æŒ‡æ ‡

metirc = evaluate.load(path="glue", config_name=task)

def compute_metric(eval_pred):
    predictions,labels = eval_pred
    predictions = np.argmax(predictions,axis=1)
    return metirc.compute(predictions=predictions, references=labels)
```

### Step5 è®¾ç½®è®­ç»ƒå‚æ•°

```python
# è®¾ç½®è®­ç»ƒå‚æ•°

train_args = TrainingArguments(
    output_dir="outputs",
    logging_dir="logs",
    logging_strategy="epoch",
    logging_steps=10,
    report_to="tensorboard",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=4,
    weight_decay=0.01,
    load_best_model_at_end=True,
    learning_rate=2e-5,
    save_strategy="epoch",
    save_total_limit=1,
    eval_strategy="epoch",
)
```

### Step6 è®­ç»ƒæ¨¡å‹

```python
# è®­ç»ƒæ¨¡å‹
model.train()
trainer = Trainer(
    model=model,
    args=train_args,
    compute_metrics=compute_metric,
    train_dataset=tokenized_dataset["train"].select(range(10000)),
    eval_dataset=tokenized_dataset['validation'],
    tokenizer=tokenizer,
)

trainer.train() # æ¨¡å‹è®­ç»ƒ

trainer.evaluate() # è¯„ä¼°æ¨¡å‹

text = ["I like that song","I really hate this movie. It's disaster!","I really not like this movie", "this song is great"]
results = model_predict(text)
print("è®­ç»ƒåçš„æ¨¡å‹ï¼š:",results)
```

### Step7 ä¸Šä¼ æ¨¡å‹åˆ°hub

[ä¸Šä¼ æ¨¡å‹åˆ°hugging face model hub](https://huggingface.co/docs/transformers/model_sharing)


